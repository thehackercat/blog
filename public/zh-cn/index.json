[{"categories":["Coding"],"content":"背景 上个月，Facebook 发生了一次由 BGP 引起的大型故障，从故障报告 中我确实看得云里雾里。所以近期阅读一些文章，期望深入学习。 这篇博文将会分享一些我如何学习 BGP 的过程及用来查询 BGP 信息的相关工具使用。因为我对 BGP 不是那么了解，故文章难免有不精确的部分，希望多多交流。 ","date":"2021-11-16","objectID":"/zh-cn/tools-to-explore-bgp/:1:0","tags":["Network","Linux"],"title":"从 Facebook 故障开始探索 BGP 工具使用","uri":"/zh-cn/tools-to-explore-bgp/"},{"categories":["Coding"],"content":"BGP 探索之路 如何发布BGP路由 之前我从没有开始接触 BGP 的原因之一是–我不知道如何在我的服务器上通过 BGP 发布路由。 对于大多数网络协议，如果你愿意，都可以折腾，自己实现协议。例如，你可以。 发行你自己的TLS证书 编写你自己的HTTP服务器 编写你自己的TCP实现 为你的域名编写你自己的权威DNS服务器 建立你自己的证书颁发机构 但对于BGP，我认为除非你拥有自己的 ASN，否则不能自己发布路由，尽管可能可以在你的家庭网络上实现BGP，但我希望它们运行在真正的互联网上。 于是我开始着手于如何在我的服务器上发布 BGP 路由。 首先我们来谈谈 BGP 的一些术语。这部分我会简单的介绍，因为我对工具更感兴趣，而且网上有很多关于 BGP 的高水平资料（比如 cloudflare 的这篇帖子）。 什么是 AS（autonomous system）？ 我们需要了解的第一件术语是 AS。每一个 AS 都是什么呢？ 参考这篇维基百科, 他们有以下特征， 由一个组织拥有的（通常是一个大型组织，如你的ISP，政府，大学，Facebook，等等） 他们能控制一组特定的 IP 地址（例如，我的 ISP 的 AS 包括 247,808 个IP地址）。 他们有一个固定标识的数字码（如1403）, 我们称为 ASN。 下面是我简单做一些 AS 相关的实验观察。 一些大型的科技公司其实并没有自己的 AS。例如，我在 BGPView 上查看了 Patreon，就我所知，他们没有自己的 AS – 他们的主站（patreon.com，104.16.6.49）在 Cloudflare 的 AS 中。 一个 AS 可以包括许多国家的 IP。Facebook 的 AS（AS32934）肯定有新加坡、加拿大、中国、美国和其他国家的IP地址。 似乎 IP 地址可以在一个以上的 AS 中。例如，如果我查找 209.216.230.240，它有 2 个 ASN 与之相关– AS6130 和 AS21581。显然，当这种情况发生时，可能会有路由规则就会有对应的优先算法–所以到该IP的数据包会被路由到 AS21581。 什么是 BGP 路由呢 互联网上有很多的路由设备。例如，我的 ISP 有所在云厂商提供的路由策略。 当我向我的 ISP 发送一个数据包时（例如通过运行 ping 129.134.30.0），我的 ISP 的路由器需要弄清楚如何将我的数据包实际发送到 IP 地址 129.134.30.0 路由器计算的方法是它有一个路由表 – 它有一堆IP范围的列表（比如129.134.30.0/23），以及它知道的通往该子网的路由。 下面是一个 129.134.30.0/23 的真实路由的例子： 11670 32934 206.108.35.2 from 206.108.35.254 (206.108.35.254) Origin IGP, metric 0, valid, external Community: 3856:55000 Last update: Mon Oct 4 21:17:33 2021 这条路由说明了通往 129.134.30.0 的一条路径是通过机器 206.108.35.2 ，这是在其本地网络上。所以路由器接下来可能会把我的 ping 包发送到 206.108.35.2，然后 206.108.35.2 会知道如何把它送到目标地址。 其中开头的两个数字（11670 32934）是ASN。 所以什么是 BGP 呢 我对 BGP 的理解比较模糊，但它是一个公司用来公布 BGP 路由的协议。 如上个月发生在 Facebook 的故障是，他们发布了 BGP 广播，撤销了他们所有的 BGP 路由，所以世界上每一个路由器都删除了所有与 Facebook 有关的路由，所以没有流量可以到达那里。 好了，现在我们已经涵盖了一些基本的术语，让我们来谈谈可以用来查看自治系统和 BGP 的工具吧! 工具1：用 BGPView 查看你的 ISP 的 AS 为了使 AS 这个东西不那么抽象，我通常使用一个叫 BGPView 的工具来查看一个真实的 AS。 我的 ISP 拥有 AS4808。这里是我的 ISP 拥有的 IP 地址。 如果我查找我的计算机的公网 IPv4 地址，可以发现它是我的 ISP 拥有的 IP 地址之一 – 它在 111.192.0.0/12 块中。 BGPView 还提供了我的 ISP 与其他 AS 的连接情况的图表 工具2: traceroute -A 和 mtr -z 既然我们对 AS 有所了解。那么让我们看看我的网络请求会从哪些 AS 中穿过 traceroute 和 mtr 都能可以告诉你每个 IP 的 ASN 。 具体用法是 traceroute -A 和 mtr -z，分别。 让我们看看我用 mtr 在访问 baidu.com 的路上经过了哪些 AS $ sudo mtr -z baidu.com My traceroute [v0.94] Bong (10.255.2.107) -\u003e baidu.com 2021-11-16T13:12:47+0800 Keys: Help Display mode Restart statistics Order of fields quit Packets Pings Host Loss% Snt Last Avg Best Wrst StDev 1. AS??? 10.255.0.1 0.0% 86 2.3 50.2 1.7 925.0 169.2 2. AS4808 121.69.9.97 1.2% 86 3.8 47.7 3.4 876.4 158.8 3. AS4808 218.241.128.173 1.2% 86 3.3 54.0 3.0 996.9 180.1 4. AS4808 218.241.254.185 0.0% 86 5.0 59.5 2.5 998.4 196.0 5. AS4808 218.241.254.197 0.0% 86 3.6 55.7 3.4 974.3 184.4 6. AS4808 218.241.254.249 0.0% 86 9.0 51.7 4.4 921.5 171.7 7. AS4808 218.241.244.17 58.1% 86 14.2 45.7 4.9 874.7 151.3 8. AS4808 61.149.212.201 62.4% 86 1003. 105.0 5.4 1003. 244.9 9. AS4808 202.96.13.249 4.7% 86 5.6 66.5 5.1 1010. 201.5 10. AS4808 124.65.194.77 12.9% 85 11.0 33.7 5.7 647.0 105.9 11. AS4837 219.158.13.78 0.0% 85 6.1 58.5 6.1 916.7 172.3 12. AS4837 219.158.44.114 89.3% 85 28.4 43.1 6.3 310.7 100.6 13. AS4134 202.97.18.125 72.6% 85 8.4 53.8 6.3 1026. 212.0 14. AS23724 36.110.246.126 82.1% 85 9.4 87.2 7.3 982.4 252.9 15. (waiting for reply) 16. (waiting for reply) 17. (waiting for reply) 18. (waiting for reply) 19. (waiting for reply) 20. (waiting for reply) 21. AS23724 220.181.38.148 0.0% 85 9.6 59.0 7.9 983.3 182.6 虽然看得眼花缭乱，但看起来网络数据包直接从我的 ISP 的AS（4808）到 Baidu 的 AS（23724），中间有一个 “互联网交换”。 我不确定互联网交换是什么，但我知道它是互联网的一个极其重要的部分。不过这部分暂时不在文章中介绍了。我最好的猜测是，它是互联网中实现 “对等 “的部分– 就像 IX 是一个有巨大的交换机的机房，里面有无限的带宽，一堆不同的公司把他们的计算机放在那里，这样他们就可以互相发送数据包。 工具3：PCH looking glass PCH（“packet clearing house”）是运行大量互联网交换点的组织。“looking glass” 似乎是一个通用术语，指的是让你从另一个人的电脑上运行网络命令的终端。有一些 looking glass 不支持 BGP，而我只对那些能显示 BGP 路由信息的 looking glass 工具感兴趣，本文只重点介绍这一部分。 这里是常用的 PCH looking glass 地址：https://www.pch.net/tools/looking_glass/。 我按下图随便选了一个选项进行 bgp 查询 Step1：“show ip bgp summary” 下面是输出结果。由于太长我删减了一些不重要内容。 From cache (number of seconds old (max 600)): 339 IPv4 Unicast Summary: BGP router identifier 74.80.118.4, local AS number 3856 vrf-id 0 BGP t","date":"2021-11-16","objectID":"/zh-cn/tools-to-explore-bgp/:1:1","tags":["Network","Linux"],"title":"从 Facebook 故障开始探索 BGP 工具使用","uri":"/zh-cn/tools-to-explore-bgp/"},{"categories":["Coding"],"content":"总结 至此，所有工具的介绍基本结束。也许你会看到一堆 BGP 相关术语和知识，或者一些工具的参考，如果你想（作为一个业余爱好者）实际发布 BGP 路由，这里有一些链接提供参考 a guide to getting your own ASN dn42 seems to have a playground for BGP (it’s not on the public internet, but it does have other people on it which seems more fun than just doing BGP by yourself at home) 我想还有很多BGP工具（比如PCH有一堆路由数据的每日快照，都是十分有趣的工具），但这篇文章已经很长了，而且我确实也没有逐个了解过这些工具。 我对我作为一个没有专业网络知识的工程师可以得到这么多关于BGP的信息感到惊讶，我一直认为它是一个 “秘密的网络向导 “的东西，但显然有各种公共机器，任何人都可以直接telnet到那里，用来查看路由表! 还有各种资料！ 希望以后能更多的接触到 BGP 相关知识，再和你们做分享！ ","date":"2021-11-16","objectID":"/zh-cn/tools-to-explore-bgp/:1:2","tags":["Network","Linux"],"title":"从 Facebook 故障开始探索 BGP 工具使用","uri":"/zh-cn/tools-to-explore-bgp/"},{"categories":["Coding"],"content":"背景 今天主要分享下我在美团 Hulk 团队调度系统组期间学习到的 k8s scheduler 调度模型及美团对于调度器相关改造 ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:1:0","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"Hulk 团队介绍 Hulk 是美团负责容器云平台的团队, 我所在的组是调度系统组下面的 k8s 小组. 主要负责所有 k8s 相关组件开发维护. ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:1:1","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"Kube-scheduler 调度模型介绍 (本次介绍基于 k8s 1.16 版本) pod 是 k8s 工作负载的最小实体，也是 scheduler 调度的主要对象。 k8s 调度器所做的工作，即将 pod 绑定到指定节点上，为 pod 选择合适的节点。 Scheduler 会先 watch apiserver 获取其中待调度的 pod, 即 pod.spec.nodeName 字段为空的 pod 对象, 通过调用 pod informer 的 handler 并将该 pod 加入调度队列中。 默认情况下, k8s 调度队列是一个 PriorityQueue, 并且当某些集群信息发生改变时, 调度器会对调度队列的内容进行一些特殊操作. 在这儿不论述. 一次调度过程在判断一个 Node是否可作为目标机器时，主要分为三个阶段： Predicates 预选阶段：硬性条件，过滤掉不满足条件的节点，这个过程称为 Predicates。这是固定先后顺序的一系列过滤条件，任何一个 Predicate 不符合则放弃该 Node。 Prioritites 优选阶段：软性条件，对通过的节点按照优先级排序，称之为 Priorities。每一个 Priority 都是一个影响因素，都有一定的权重。 Select 选定阶段：从优选列表中选择优先级最高的节点，称为 Select。选择的 Node 即为最终部署 Pod 的机器。 ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:1:2","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"预选阶段 Scheduler 起 n 个 goroutine 并发对整个集群的所有 nodes 进行 predicates 串行运算过滤规则，直到最终过滤出符合条件的 nodes 列表 ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:1:3","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"打分(优选)阶段 优选阶段，也叫打分阶段，采用 0-10 分加权平均制, 每个优选算法有一定的权重, 优选算法*权重后的积分则为优选阶段最终评分, 0 分为最低分, 10 分为最高分. 最终会选取打分最高的节点作为调度分配节点. 打分规则 最常用为 LeastRequestedPriority 打分规则. 可以简单总结为: 即, 选择出宿主机空闲资源(cpu/memory)最多的节点。 同时还有如 BalancedResourceAllocation 打分规则, 其标准是判断调度完成后，所有节点里各种资源分配最均衡的那个节点. 随着社区 scheduler 不断演进, 也不断增加更细粒度的打分算法. 节点打分算法我们可以抽象为以下公式: ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:1:4","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"美团优化 由于早期 kube-scheduler 版本在大规模集群下存在一定的调度时延问题, 未解决该问题, 美团也做了定制化改造. ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:2:0","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"预选中断机制 通过深入分析调度过程可以发现，调度器在预选阶段即使已经知道当前 Node不符合某个过滤条件仍然会继续判断后续的过滤条件是否符合。试想如果有上万台 Node节点，这些判断逻辑会浪费很多计算时间，这也是调度器性能低下的一个重要因素。 为此，我们提出了“预选失败中断机制”，即一旦某个预选条件不满足，那么该 Node即被立即放弃，后面的预选条件不再做判断计算，从而大大减少了计算量，调度性能也大大提升。如下图所示： ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:2:1","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"局部最优解 当前调度策略中，每次调度调度器都会遍历集群中所有的Node，以便找出最优的节点，这在调度领域称之为BestFit算法。但是在生产环境中，我们是选取最优Node还是次优Node，其实并没有特别大的区别和影响，有时候我们还是会避免选取最优的Node（例如我们集群为了解决新上线机器后频繁在该机器上创建应用的问题，就将最优解随机化）。换句话说，找出局部最优解就能满足需求。 假设集群一共1000个Node，一次调度过程PodA，这其中有700个Node都能通过Predicates（预选阶段），那么我们就会把所有的Node遍历并找出这700个Node，然后经过得分排序找出最优的Node节点NodeX。但是采用局部最优算法，即我们认为只要能找出N个Node，并在这N个Node中选择得分最高的Node即能满足需求，比如默认找出100个可以通过Predicates（预选阶段）的Node即可，最优解就在这100个Node中选择。当然全局最优解NodeX也可能不在这100个Node中，但是我们在这100个Node中选择最优的NodeY也能满足要求。最好的情况是遍历100个Node就找出这100个Node，也可能遍历了200个或者300个Node等等，这样我们可以大大减少计算时间，同时也不会对我们的调度结果产生太大的影响。 方案1： 每次调度只对 K 个 node 方案2： 每次调度对全量 node, 一旦某一个打分超过水位线, 则终止其他 node 打分 ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:2:2","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"PriorityQueue 改造 PriorityQueue 其实内部细分为多个 Queue 数据结构. ActiveQ 待调度队列 BackOffQ Bind失败队列 UnscheduledQ 调度失败队列 其余与本次改造无关的队列(此处不一一列举)… 当某一时刻出现大量调度失败的高优先级 bad pods 时，由于重调度组件和 priorityQueue 机制，会导致这些 bad pods 重新回到 activeQ 队列头, 导致低优先级的正常 pod 无法调度. 降级方案: 当 ActiveQ 出现大量积压时, 且存在 Unscheduled pods 时 PriorityQueue 降级为 FIFO Queue, 确保 bad pods 在下一个调度周期进入到队列尾, 保证 good pod 能被正常调度 ","date":"2021-10-09","objectID":"/zh-cn/kubernetes-scheduler-modified-in-meituan/:2:3","tags":["Kubernetes","Linux"],"title":"Kube-scheduler 调度模型与美团本地化改造","uri":"/zh-cn/kubernetes-scheduler-modified-in-meituan/"},{"categories":["Coding"],"content":"Lexus Lee ","date":"2021-10-02","objectID":"/zh-cn/talk-about-cloud-based-release-platform/:0:0","tags":["Docker","Kubernetes"],"title":"云原生下发布平台建设思考","uri":"/zh-cn/talk-about-cloud-based-release-platform/"},{"categories":["Coding"],"content":"背景 从过去几个月, 我的工作精力主要集中在企业级发布平台的建设上. 同时也聚焦在容器云 PaaS 层建设。 在当下云原生时代, 发布平台通常起到一个联结 PaaS 和 IaaS 的作用. 通过在 PaaS 层抽象，将 IaaS 层基建细节屏蔽，同时通过 api 和各个服务平台(CI/CD 服务, 监控告警服务, 网关服务, 工单系统, 服务治理系统等) 交互, 在云平台提供了一个统一的快捷入口，从逻辑上进行汇聚，便于开发者进行一站式应用发布、服务生命周期管理、服务治理、服务运维等操作. 这也是发布平台的职能，在一个完整的 PaaS 云平台体系中，其能力应是集成的，而非离散的－－系统需要提供很好的集成能力，让系统得到收敛，避免系统被割裂成一个一个的执行单元，那么用户常常会为此痛苦不堪；同时平台也是场景化的，而非基于功能需求的－－场景能够串联工具的能力，跨越短平快的需求，让平台的各个工具能够串联的运转，其数据能力能在多个子系统间流动起来。 ","date":"2021-10-02","objectID":"/zh-cn/talk-about-cloud-based-release-platform/:1:0","tags":["Docker","Kubernetes"],"title":"云原生下发布平台建设思考","uri":"/zh-cn/talk-about-cloud-based-release-platform/"},{"categories":["Coding"],"content":"那么什么是 PaaS 平台? PaaS 是在 IaaS 层之上的基础平台，一个典型的特征是拥有 App 概念，一切围绕 App 开发的生命周期展开，也会对 App 的架构做一定的约束. 由于直接对接业务侧应用甚至应用架构, 且 PaaS 平台通常是业务方最直接所见的基础平台. 对 PaaS 平台的建设要求就有所提高，通常需要在建设通用性能力的基础上，提供一定的贴切公司使用场景的定制化能力。 故通常云平台也在承接着各个业务线开发者提来的各式各样的需求. 起到对业务部门垂直支撑的作用. 一个常见的云平台生态支撑场景如下图所示 ","date":"2021-10-02","objectID":"/zh-cn/talk-about-cloud-based-release-platform/:1:1","tags":["Docker","Kubernetes"],"title":"云原生下发布平台建设思考","uri":"/zh-cn/talk-about-cloud-based-release-platform/"},{"categories":["Coding"],"content":"云原生时代发布平台思考 随着近年来云原生概念和云原生架构设计的流行，越来越多的开发团队开始使用 DevOps 模式进行系统开发，并把大型系统拆解成一个个微小的服务模块，以便系统能更好地进行容器化部署。 基于 DevOps、微服务、容器化等云原生的能力，可以帮助业务团队快速、持续、可靠和规模化地交付系统，同时也使得系统的复杂度成倍提升，由此带来了前所未有的运维挑战，比如： 模块之间的调用从进程内的函数调用变为容器进程间的调用，而网络总是不可靠的。 微服务拆分多样, 难以从全局视角观测各个微服务健康信息。 服务的调用路径变长，使得流量的走向变得不可控，故障排查的难度增大。 引入 Kubernetes、Docker、Service Mesh 等云原生系统，基础设施层对业务开发团队来说变得更加黑盒。 无论是微服务拆分, 容器化, 云原生化等, 都对服务的发布过程带来巨大挑战. 如何做到无感地支持容器/k8s发布, 同时提供云原生时代的可观察性可以说的云原生时代发布平台的主要目标。 而从前司的平台建设经验来看, 我们在平台建设初中期也遇到了以下问题 如发布过程中的金丝雀/预发布环境/持续测试支持, 属于对于服务自身发布过程的稳定性建设, 帮助业务更搞笑稳定的进行版本发布. 发布后的应用观测中也涉及到日志查询, 容器 debug 能力, 实时诊断能力等支持. 基于以上遇到的问题，大致抽象了以下几个长线来探讨发布平台建设 服务发布稳定性 发布系统可扩展性 服务可观察性 发布系统易用性 根于上述4条长线, 列出了我个人理解的云原生时代的发布平台建设全景图 ","date":"2021-10-02","objectID":"/zh-cn/talk-about-cloud-based-release-platform/:1:2","tags":["Docker","Kubernetes"],"title":"云原生下发布平台建设思考","uri":"/zh-cn/talk-about-cloud-based-release-platform/"},{"categories":["Coding"],"content":"发布平台全景图 ","date":"2021-10-02","objectID":"/zh-cn/talk-about-cloud-based-release-platform/:1:3","tags":["Docker","Kubernetes"],"title":"云原生下发布平台建设思考","uri":"/zh-cn/talk-about-cloud-based-release-platform/"},{"categories":["Coding"],"content":"Lexus Lee ","date":"2021-09-28","objectID":"/zh-cn/service-portrait-vpa-design/:0:0","tags":["Docker","Kubernetes","Linux"],"title":"基于服务画像的垂直伸缩系统建设思考","uri":"/zh-cn/service-portrait-vpa-design/"},{"categories":["Coding"],"content":"背景 近期公司开始做成本治理, 降本增效. 通过观察以往各业务线的资源规格配置, 不难发现存在大量服务资源超配情况. 即服务实际资源开销远小于其声明的资源开销. 而这部分的资源超配直接导致了集群的资源使用率较低. 通常形如下图 从过去的集群资源使用实践过程中，我们也遇到了如下问题： 服务本身资源使用率较低, 但申请资源规格时使用了较高的规格 服务仅在特定时段(流量波峰)有较高资源使用, 其余时段资源使用率较低，没有申请合理的资源规格 开发者不知道如何评估及配置自身服务规格 在这篇文章的过程中，我们核心介绍的是： - 如何建设合理的服务资源画像, 对外提供画像数据查询能力 - 依据资源画像数据在调度系统提供一定资源超售能力, 从而提高集群资源使用率 ","date":"2021-09-28","objectID":"/zh-cn/service-portrait-vpa-design/:1:0","tags":["Docker","Kubernetes","Linux"],"title":"基于服务画像的垂直伸缩系统建设思考","uri":"/zh-cn/service-portrait-vpa-design/"},{"categories":["Coding"],"content":"目标 如背景中提到的资源分配值与资源使用值之间存在的差值是带来整体资源利用率较低的直接原因, 故有效缩短这部分差值能带来资源利用率的直接提升. 故我们抽象了以下公式: gap=abs(resourcerequest-resourceusage) 基于上述背景，我们细化出以下目标： 降低服务实际使用资源和服务声明使用资源的差异, 即尽可能减少上述公式中提到的 gap 根据在 PaaS 平台配置的服务类型提供梯度的资源超卖机制 对于指标中周期性抓取的资源特征数据进行持久化, 目前只考虑 cpu, memory 对于开发者在 PaaS 平台配置规格时给出推荐值 本次方案将降低用户配置服务资源规格成本，在集群维度避免过度超卖导致的节点内存掉底导致的宕机现象或频繁换页带来的性能影响. 本次讨论不关注 k8s 节点资源超卖, 仅关注服务粒度资源超卖 ","date":"2021-09-28","objectID":"/zh-cn/service-portrait-vpa-design/:1:1","tags":["Docker","Kubernetes","Linux"],"title":"基于服务画像的垂直伸缩系统建设思考","uri":"/zh-cn/service-portrait-vpa-design/"},{"categories":["Coding"],"content":"系统架构 名词解释: console: 服务平台, 存储服务元信息 plume: 调度系统, 包装 k8s api, 提供 k8s workload 操作能力 portrait data service: 服务画像服务, 提供服务维度画像数据 VPA 结构上主要分为三个组件： Recommender： 定期(Interval 默认为1天)从 Console 获取全量部署到 kubernetes 上的服务列表 向 Prometheus 查询服务过去 N 天的资源使用情况 (默认 N=1 以覆盖每日业务峰值) 对于 CPU, 获取 TP50/90/95/99 的 cpu 利用率作为业务容器平均 cpu 利用率 (即从小到大排序有第 50/90/95/99 位的容器 cpu 使用率) 对于 Memory, 获取特定时间窗口的峰值 根据推荐算法⑴计算服务资源的推荐值, 并根据 Recommender 策略进行服务元信息配置更新 策略1 – Auto: 将自动将推荐值刷入 console 服务元数据数据库中 (默认 Auto, 使用项目维度黑名单机制开启 Off) 策略2 – Off: 将推荐值记录到 db 中用于 console 界面展示规格推荐值 Recommender-Updater: Recommender 的子模块, 用于周期性更新 k8s 资源, 存储特征数据 根据配置策略更新 k8s deployment/pod 策略1 – Auto: 将自动更新 k8s pod spec 中的资源 requests/limit 值, 且服务将发生重调度 (业务有感知) 策略2 – Off: 不进行自动更新, 靠服务重新在 console 部署触发资源更新 (默认 Off, 使用项目维度白名单机制开启 Auto) 往 db 中存储资源特征数据 Portrait Data 画像服务: 接收 Recommender 请求，对采集到的服务画像数据进行持久化. 提高服务维度画像数据查询能力, 对外提供 http 查询接口. Admission Controller: (非必须组件) 拦截 pod 提交请求, 自动获取 Recommender 中的资源推荐值, 并替换 pod spec 中的资源 requests/limit 值 该组件用于提供以下能力 解决确保弹性伸缩时扩容出的 Pod 为新资源配比值 提供 Updater kill pod 时可以自动生成带有新资源配比值的 Pod 能力 综合 b. 中两点考虑，目前我们暂无复杂用法, 先降低系统复杂度. 不做该组件, 转为和 k8s api 打交道 ","date":"2021-09-28","objectID":"/zh-cn/service-portrait-vpa-design/:1:2","tags":["Docker","Kubernetes","Linux"],"title":"基于服务画像的垂直伸缩系统建设思考","uri":"/zh-cn/service-portrait-vpa-design/"},{"categories":["Coding"],"content":"数据库表结构设计 数据库选用 Hbase, 像画像中的资源数据类似指标打点, 会有 TP50/90/95/99 需要较灵活的分位分割, 对表的扩展性需要高.是 MySQL 不易于处理的场景，而列存储天然适合. 同时画像数据通常有较强的实时性要求, 即可能一段时间周期(比如6个月)前的画像数据的参考性不高. 同时为防止表数据膨胀, 也需要定期对画像表进行清理. 基于此可以利用 Hbase TTL 特性进行定期清理. 资源画像表 资源画像表记录服务粒度的资源的静态信息，通过 resource_type 区分资源类型，目前有 cpu, memory, disk io。 类似时序数据库用法，在表中存储类型，特征值, 采集时间字段 表名：resources_portrait 列名 类型 备注信息 data varchar(255) 采集时间 id bigint 主键 resource_suffix varchar(255) 特征后缀, 如 cpu_busy_TP90, cpu_busy_TP99 resource_type int(11) 资源类型：cpu、memory等 value varchar(255) 资源值 ","date":"2021-09-28","objectID":"/zh-cn/service-portrait-vpa-design/:1:3","tags":["Docker","Kubernetes","Linux"],"title":"基于服务画像的垂直伸缩系统建设思考","uri":"/zh-cn/service-portrait-vpa-design/"},{"categories":["Coding"],"content":"设计细节 以下设计均建立在不考虑 Admission Controller 的场景下. 一、一次资源画像更新流程是怎样的？ Recommender 每周一早上 10 点调用服务平台 api 获取全量服务列表, 发起资源画像更新流程 向 Prometheus 查询 avg(rate(container_cpu_usage_seconds_total{container=\"${container}\", pod=~\"$service.+\"} Recommender-Updater 将采集到的特征数据更新到 db 中 Recommender-Updater 根据自身策略更新 k8s deployment spec 二、调度流程有啥修改？ 调用流程与原来一致 Plume 调度系统接收到调度请求, 根据服务元信息中的资源规格配置生成 k8s pod spec 中相应资源字段 调用 k8s api 进行 apply workload 三、推荐算法是什么？ 根据 PaaS 平台的服务类型 给予以下梯度 CPU 超售比 在线无状态服务: 1.5 (核心服务不超售) 离线服务 : 3.0 对于 CPU 按区间(单位为核数)拆分为 （0, 0.5], (0.5, 1], (1, 2], (2,4], (4,6], (6, 8], (8, 10], (10, 12], (12, 14], (14, 16], (16, *]. 按以下公式计算 CPU request 值 for interval in （0, 0.5], (0.5, 1], (1, 2], (2,4], (4,6], (6, 8], (8, 10], (10, 12], (12, 14], (14, 16]: if 应用_cpu_usage in interval: 应用_cpu_request = ceil(interval) * 1/超售比 # 当前区间向上取整. break if 应用_cpu_usage \u003e 16: 应用_cpu_request = 应用_cpu_usage * 1/超售比 对于 Memory 按区间(单位为 GB)拆分为 （0, 0.5], (0.5, 1], (1, 2], (2,4], (4,6], (6, 8], (8, 10], (10, 12], (12, 14], (14, 16], (16, *]. 按以下公式计算 Memory request 值 for interval in （0, 0.5], (0.5, 1], (1, 2], (2,4], (4,6], (6, 8], (8, 10], (10, 12], (12, 14], (14, 16]: if 应用_mem_usage in interval: 应用_mem_request = ceil(interval) * 1/超售比 # 当前区间向上取整. break if 应用_mem_usage \u003e 16: 应用_mem_request = 应用_mem_usage * 1/超售比 # 内存当前不超售, 未来可能超售 四、会自动修改资源的 limit 吗？ 本次模型中 Recommender Auto 模式下只会修改 Request, Request 对于用户无感知. 对于 limit 在 console 上会与服务规格概念相关, 故只会给出推荐值, 不会帮用户修改. 五、Recommender-Updater 自动修改 k8s 模板后触发重调度业务可接受吗？ 默认使用项目白名单机制，只有能接受这部分场景的应用开启，其余默认关闭 Updater. 对于二期链路上再尝试优化不进行重调度的 k8s 资源垂直扩缩. 六、Admission Controller 的作用是啥，可以去掉吗？ 期望提供通用 patch pod spec 能力, 同时对于后续 Recommender-Updater 更新 pod spec 时可以提供通过逐步 kill pods 机制实现无感知上线 在目前的链路上可以去掉，也不计划在这一期支持 七、对于过度超卖的情况下如何矫正？ 必然会存在部分项目出现计算出的 cpu requests/mem requests 不合理导致过度超卖的情况, 对于这类服务使用两种方式解决 在 Recommender 中白名单固定超卖比. 在 Recommender 中使用白名单关闭接入. 八、是否会对不同服务给予特定的超售比？ 会计划, 但需要根据具体采集到的画像数据进行统一分析才能得到有依据的结论或超售比计算公式. 常见问题 Q: 对于 CPU, 获取 TP50/90/95/99 的 cpu 利用率作为业务容器平均 cpu 利用率 (即从小到大排序有第 50/90/95/99 位的容器 cpu 使用率) 是按哪个利用率作为阀值来决策呢? A: cpu 默认以 P95 为决策，对于个别有明显波峰波谷的服务需要根据画像数据做具体调整. mem 默认以时间窗口内的峰值为决策. Q: 内存超卖会影响业务稳定性，建议不进行。如果内存真的不合理，应推动业务主动更改配置更新服务 在资源调度层面不建议做内存和CPU超卖，会导致调度系统过于复杂，k8s pod层面request和limit可以不一样适当超卖 A: 内存不会进行超卖. 本次会对 cpu/mem 给出规格推荐值(界面展示). 会以邮件或其他通知方式推动配置更新. 用户主动在 PaaS 平台界面上按推荐值修改的资源规格实际影响的是 k8s limit. 对于方案中 VPA 调整实际是 k8s request, 对于用户无感. 同时对于 k8s resources limit 不会在调度链路中动态修改 Q: Recommender 虽然做了项目白名单可以选择性开启该功能，但自动调整风险较高。如果是一次性的调整，完全可以由平台给出参考建议，督促业务检查确认是否合理，之后由业务做出调整 如果是针对波峰波谷流量下资源的调整，可以给出利用率等曲线，业务需要综合服务延时等 确定按时段扩缩容规则，平台按业务规则进行操作。需要留一定的利用率buffer，要保证能够快速拉起服务 平台需要全局考虑所有业务在某个时间点扩容后需要的资源是否大于集群物理资源 因为在线任务原则上都是不能kill的，是高优，在线任务需要扩容时需要能保证资源，都是在线就无法完全保证了，可以结合离线任务来填补波峰波谷的资源； 在离线混部需要考虑离线任务对在线任务的干扰，需要一定的资源隔离机制保证。初期可以混部少量离线任务逐步打磨 Q: 在线资源利用率优化需要在考虑业务稳定性和服务延时的情况下去做，建议分步骤逐步实施，主要分为纵向扩缩容和横向扩缩容 1、纵向扩缩容，在满足业务延时情况下，尽力提升qps的情况下，资源仍然有富裕的，这部分应该通过缩减实例配置来优化 2、横向扩容容，在实例配置合理的情况下，因为业务流量下降（固定的）或者波峰波谷，可以调整实例数量来调整单实例的资源利用率 初期可以考虑在业务峰值压力下 如何优化单实例配置 和 实例数量，波峰波谷下的实例调整可以在一期积累更多经验后再进行。 A：合理的. 非常同意，目前计划是在 recommender 组件的机制上先留出这样一个策略的口子. 但不计划对生产业务线接入. 后续有能力建设对业务无感的资源调整机制后再通过这个口子开放业务接入. ","date":"2021-09-28","objectID":"/zh-cn/service-portrait-vpa-design/:1:4","tags":["Docker","Kubernetes","Linux"],"title":"基于服务画像的垂直伸缩系统建设思考","uri":"/zh-cn/service-portrait-vpa-design/"},{"categories":["Coding"],"content":"背景 最近看一些社区 issue 中正好接触到 containerd 源码, 早年大家基本都使用 docker/podman 这类容器上层包装, 包括我也是, 故对真正容器运行时的结构部分了解得不够深入, 最近 k8s 社区在 1.21 规划上计划从 kubelet 中移除 docker-shim 交互的逻辑, 大势上组件走上 containerd-shim, 故需要着手对容器运行时更深入了解. 带着问题作为引入, 容器底层还是对 Linux LXC 的交互, 那究竟 docker daemon 中是哪个组件来完成这一步的呢？ ","date":"2021-04-10","objectID":"/zh-cn/talk-about-containerd/:1:0","tags":["Docker","Kubernetes"],"title":"Talk about Containerd","uri":"/zh-cn/talk-about-containerd/"},{"categories":["Coding"],"content":"1、背景知识 首先我们需要了解 Docker Daemon 生产出容器的过程. 当前整个 Docker 调用链架构可以用下图来简单概括 从 Docker 1.11 之后，Docker Daemon 被分成了多个模块以适应 OCI 标准。拆分之后，结构分成了以下几个部分： 16年12月 Docker公司宣布将 containerd 从Docker Engine中分离，并捐赠到开源社区独立发展和运营。 一个工业标准的容器运行时，注重简单、 健壮性、可移植性 Docker本身其实已经被剥离干净了，只剩下 Docker 自身作为 cli 的一些特色功能了，真正容器的管控都在 containerd 里实现。 在 17 年 docker重命名为moby, 从命名上逐渐脱离和 container 的关系, 而 Moby 更像是一个“乐高积木”，它包含了容器化的组件库 底层的构建、日志处理、卷管理、网络、镜像管理、containerd、SwarmKit等模块, 是个大杂烩. 19年2月containerd 正式从cncf社区毕业，成为继 Kubernetes, Prometheus, Envoy, and CoreDNS 之后第五个毕业项目。 ","date":"2021-04-10","objectID":"/zh-cn/talk-about-containerd/:2:0","tags":["Docker","Kubernetes"],"title":"Talk about Containerd","uri":"/zh-cn/talk-about-containerd/"},{"categories":["Coding"],"content":"2、生态架构 单看 docker 生态圈是非常庞大的, 故今天我们聚焦在运行时 containerd 生态架构上, 周边生态如下: OCI CRI kubelet dockerd docker.sock (/var/run/docker.sock) dockershim dockershim.sock (/var/run/dockershim.sock) containerd-cri containerd.sock (/var/run/docker/containerd/containerd.sock|/run/containerd/containerd.sock) containerd-shim Runc RunV Kata gVisor containerd 在容器生态中的角色, 作为容器运行时的扛把子 ","date":"2021-04-10","objectID":"/zh-cn/talk-about-containerd/:3:0","tags":["Docker","Kubernetes"],"title":"Talk about Containerd","uri":"/zh-cn/talk-about-containerd/"},{"categories":["Coding"],"content":"3、containerd 架构 简单来说分为: containerd-shim runC LXC 调用封装 更细化为 grpc 模块向上层提供服务接口，metrics 则提供监控数据(cgroup 相关数据)，两者均向上层提供服务。containerd 包含一个守护进程，该进程通过本地 UNIX 套接字暴露 grpc 接口。 storage 部分负责镜像的存储、管理、拉取等metadata 管理容器及镜像的元数据，通过bootio存储在磁盘上task – 管理容器的逻辑结构，与 low-level 交互event – 对容器操作的事件，上层通过订阅可以知道发生了什么事情Runtimes – low-level runtime（对接 runC） containerd-shim containerd-shim 是 containerd 的一个组件，主要是用于剥离 containerd 守护进程与容器进程。containerd 通过 shim 调用 runc 的包函数来启动容器. 各个组件以插件的形式注册 cri /run/containerd/containerd.sock containers/tasks/event/snapshots/namespace/tasks/image /run/containerd/containerd.sock 低内存环境 /run/containerd/containerd.sock.ttrpc debug /run/containerd/debug.sock /debug/pprof metrics metrics.sock /v1/metrics bolt 元数据存储 和etcd底层使用相同结构 直接启动containerd和moby启动方式 检测 /run/containerd/containerd.sock是否存在，判断是否启动containerd 用supervisor启动containerd，直接二进制调用 容器的 namespace 容器 ns 主要目的用于在 Linux 层面做 namespace 划分, 故拆分为一下3种主流 namespace. 目前最常见的 namespace type 主要分下面两种: io.kubernetes.cri.container-type io.kubernetes.docker.type containerd-shim 容器生命周期管理 允许 runc 在创建\u0026运行容器之后退出 用 shim 作为容器的父进程，而不是直接用 containerd 作为容器的父进程，是为了防止这种情况：当 containerd 挂掉的时候，shim 还在，因此可以保证容器打开的文件描述符不会被关掉 依靠 shim 来收集\u0026报告容器的退出状态，这样就不需要 containerd 来wait 子进程 因此，使用 shim 的主要作用，就是将 containerd 和真实的容器（里的进程）解耦。 而第一点，为什么要允许 runc 退出呢？ 因为，Go 编译出来的二进制文件，默认是静态链接，因此，如果一个机器上起N个容器，那么就会占用M*N的内存，其中M是一个 runc 所消耗的内存。 但是出于上面描述的原因又不想直接让 containerd 来做容器的父进程，因此，就需要一个比 runc 占内存更小的东西来作父进程，也就是 shim。 shimv2 启动一个服务，复用 与OCI组件的交互 runC 是标准化的产物，为了防止一家商业公司主导容器化标准，因此又成立了opencontainers 组织 二进制直接调用 更新 config.v2.json, hostconfig.json 文件 k8s runtime class docker-init UNIX系统中，1号进程是init进程，也是所有孤儿进程的父进程。而使用 docker 时，如果不加 –init 参数，容器中的1号进程 就是所给的ENTRYPOINT，例如下面例子中的 sh。而加上 –init 之后，1号进程就会是 tini： docker-init 作用 避免僵尸进程 默认信号处理 ","date":"2021-04-10","objectID":"/zh-cn/talk-about-containerd/:4:0","tags":["Docker","Kubernetes"],"title":"Talk about Containerd","uri":"/zh-cn/talk-about-containerd/"},{"categories":["Coding"],"content":"Scuba: Diving into Data at Facebook 原文链接: https://www.semanticscholar.org/paper/Scuba%3A-Diving-into-Data-at-Facebook-Abraham-Allen/0bce0735ef3ea01e02b73c98338727d519a71be4 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:0:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"概要 Facebook很重视性能监控。一些性能问题可能会影响超过10亿用户，所以我们跟踪了成千上万的服务器，每天上百PB的网络流量，每天上百个代码变更，以及许多其他指标。我们对时延性的要求是，在事件发生后的一分钟内，如一个客户电话请求，一个提交了bug报告，一份新的代码变更, 能及时更新到图表，并在监控系统中展示出来。 Scuba 是Facebook的数据管理系统，用于大部分的实时分析。Scuba 是一个快速的、可扩展的、分布式的、内存式的 Facebook 自研数据库。它目前吞吐率为每秒数百万行(事件)流入，并以同样的速度过期数据进行流出。Scuba 存储后端在数百台服务器上，每台服务器上的数据完全在内存中，每台服务器内存规格为 144 GB RAM。为了处理每个查询，Scuba 会从所有的数据中聚合服务器以处理每天处理近百万次查询。 在 Facebook, Scuba 广泛应用于交互式、临时性的分析查询，这些查询运行在在一秒内完成对实时数据的处理。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:1:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"1. 介绍 在Facebook，无论是诊断性能回归还是衡量基础设施变化带来的影响，我们都希望用有效数据来衡量。Facebook基础设施团队依靠的是在实时数据监控以确保网站始终运行顺利。我们的需求包括能在非常短的延迟（通常在一分钟以内）从运行Facebook的网络服务器上查询到发生的事件。查询数据的灵活性和速度对于诊断出问题根因带来不少收益。识别问题的根本原因往往是由于各子系统之间复杂的依赖关系，很难在短时间内完成。然而，一旦出现了没有在几分钟或几小时未解决的问题。那 Facebook 的10亿用户就会变得不快乐，从而对 Facebook 整体发展不利。 为了解决上述问题， 起初，我们依靠的是预先汇总的图表和一套精心管理、手工编码的脚本，通过MySQL数据库的形式管理数据。到了2011年，这个解决方案变得过于僵化和缓慢。它无法跟上不断增长的数据吞吐率和查询效率。而 Facebook内部的其他查询系统，比如Hive[20]和Peregrine[13]，在数据被提供给查询之前，查询数据被写入HDFS，有很长的（典型的一天）延迟，而查询本身需要几分钟的时间来运行。 因此，我们建立了 Scuba，一个快速、可扩展、内存数据库。Scuba 是我们收集和分析来自各种系统的数据的方式的一个重大演变，正是这些系统保证了网站每天的正常运行。我们现在使用 Scuba 能对大多数实时的、结构化的人工数据进行分析。我们将在本文后面将 Scuba 与其他数据管理系统进行比较，但据我们了解，没有任何其他系统能像 Scuba 一样快速地吞吐数据并进行复杂的查询。 如今，Scuba 运行在数百台服务器上，每台服务器有144 GB内存，在一个无共享架构的集群中。它在内存中为1000多个表存储了大约70TB的压缩数据，通过在所有服务器上随机分配每个表来分配内存。Scuba 每秒可处理数百万行。由于 Scuba 是内存绑定的，它能以同样的速度过期掉老数据。为了限制数据量，Scuba 允许行指定一个可选的采样率，这表明 Scuba 只包含原始事件的一小部分（通常是100分之一到100万分之一）。这种采样对于像 Facebook 客户端请求这样每秒发生数百万次的事件是十分必要的。采样可以是统一的，也可以是基于一些关键数据，比如用户ID等。Scuba在计算汇总时，会对采样率进行聚合补偿。 除了提供一个 SQL 查询接口（对于一个 SQL 子集，包括分组和聚合，但不包括连接表），Scuba 提供了一个 GUI，可以生成时间序列图、饼图、列值分布，以及除文本选项之外的其他十几种数据的可视化。图1显示了一个时间序列图，在Scuba GUI中对页面流量进行了一周的比较。在后台，一个聚合树将每个查询分发到每个服务器，然后收集结果发回客户端。 虽然 Scuba 是为了支持性能分析而建立的，但它很快就成为了在其他时间敏感数据上执行探索性查询的首选系统。Facebook的许多团队都在使用Scuba。 移动开发团队使用 Scuba 来跟踪哪些用户在运行不同的移动设备、操作系统和Facebook应用的版本。 广告开发团队利用 Scuba 来监控人们对广告的印象、点击和收入的变化。当出现下降时，他们可以迅速将其缩小到特定的国家、广告类型或服务器集群，并确定根本问题。 SRE 团队过使用 Scuba观察服务器错误。当发生峰值时，他们可以精确地指出是否是由于特定端点中的错误、特定数据中心或服务器集群中的服务，或数据中心的部分物理问题。 错误报告监测每小时运行数千次查询，以寻找Facebook用户报告的错误数量高峰，按几十个人口统计维度（位置、年龄、好友数等）进行分组。 总的来说，用户会先提出高级别的汇总查询，以识别数据中的有趣现象，然后再深入挖掘（因此被称为Scuba），以找到感兴趣的基础数据点。在上述所有情况下，能够沿着多个维度对数据进行临时分解是至关重要的。 同时 Scuba 也是 Facebook 代码回归分析工具、bug 报告监控工具、实时帖子监控工具（例如，有多少 Facebook 帖子提到了电影 “Argo”?）以及许多其他工具的基础引擎。 Scuba 的主要特点是，即使在扫描数百GB的数据时，查询的执行时间也不到一秒钟，而且结果通常是实时的，超过一分钟前发生的事件。 在第 2 节中，我们描述了 Scuba 在 Facebook 具体落地的一些用例，包括性能监控、趋势发现和模式挖掘。第3节详细描述了 Scuba 的架构、存储和查询功能。在第 4 节中，我们提出了 Scuba 查询执行的简单分析模型。在第 5 节中，我们通过实验对 Scuba 进行了具体评估。我们用真实的数据和查询来研究它的加速和扩展特性。在第 6 节中，我们将 Scuba 与业界相关工作进行比较。在第 7 节中，我们以列出 Scuba 与其他大多数数据库系统的不同之处作为结论。通过上面这些差异使得 Scuba 更适合我们在Facebook的使用。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:2:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"2. Scuba 使用场景 Scuba 目前存储了1000多个表。在本节中，我们将描述 Scuba 的几个代表性用例。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:3:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"2.1 性能分析 Scuba 最原始也是最常见的用途是用于实时性能监控。Julie 负责监控 facebook.com 的性能。她会首先查看 Scuba 仪表板上的几十个图表，这些图表显示了服务器上的CPU负载；缓存请求、缓存命中和缓存丢失的次数；网络吞吐量；以及许多其他指标。这些图表比较了一周与一周前的性能差异，如图-1。每当她发现一个显著的性能差异时，她就会通过不同的列（通常包括堆栈痕迹）进行深入研究，细化查询，直到她能将差异锁定在一个特定的代码块上，并填写一份紧急错误报告。 Julie的仪表盘可以在不超过几秒钟的数据上运行预制查询。性能错误往往在引入后的几分钟到几小时内就能被及时发现（并修复！）– 而且这些错误还在网站的一小部分上进行测试。这些性能图表中的峰值警报可以使她的初始监控自动化。在Facebook的所有服务器上实时记录和导入数据会太过昂贵；所以 Julie 的表格中包含了大约万分之一的事件样本（但不同的表格和事件类型会有所不同）。Scuba 会记录采样率并进行补偿。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:3:1","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"2.2 趋势分析 Scuba 的另一个时间敏感型的使用场景是趋势发现。Eric 想寻找数据内容的趋势。他从用户帖子中提取词语集，并寻找词语频率随时间和许多维度的峰值：国家、年龄、性别等。和 Julie一样，Eric也在分析秒速时时彩数据。他为Facebook的传播热点建立了一个工具团队，以图表方式显示有多少帖子提到了当前的短语。例如，在奥斯卡颁奖典礼之前，这个工具就被用来实时查看过去一小时内有多少帖子包含了竞争电影的名字。与Julie不同的是，Eric通常会编写新的自定义查询，因为他在尝试新的趋势分析思路。他还会编写自定义的Javascript函数来计算数据的统计数据，比如整数列之间的共变性。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:3:2","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"2.3 模式挖掘 产品分析和模式挖掘是 Scuba 的第三个用例。与Julie和Eric不同，Bob不是软件工程师。他是一名产品专家，他需要分析不同的 Facebook 用户如何应对网站或移动应用的变化。他根据不同的维度，如用户的位置和年龄、产品（设备、操作系统和构建）和错误报告中的关键词，寻找特定的模式，而不知道哪些维度可能很重要。鲍勃在不知道数据是如何被记录的，也不知道哪些列可能存在的情况下，查看许多表中的数据。他寻找他能找到的任何模式。他使用 Scuba 是为了以毫秒为单位运行滚动查询，而不是在 Hive 中需要几分钟。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:3:3","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"3. Scuba 总览 在本节中，我们将提供 Scuba 的架构概述。如图2所示，Scuba 的存储引擎由许多独立的服务器组成，每个服务器被划分为称为 “叶子节点”（或叶子）的逻辑存储单元。cpu核心的数量决定了叶子的数量，目前每台服务器有8个。每个叶子包含了大部分表的数据分区，所有的查询都会进入所有的叶子节点，如下所述。现在我们介绍一下Scuba的数据模型。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:4:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"3.1 数据模型 Scuba 为其用户提供了一个标准的数据模型。每个表都有包含四种可能类型的数据列的行。 整数。整数用于聚合、比较和分组。时间戳也存储为整数。 字符串：字符串用于比较和分组。字符串用于比较和分组。 字符集：字符串集用于表示，比如说，Facebook帖子中的单词或特征集，Facebook帖子中的某个单词，或者是某一用户真实的功能集（如图搜索、新闻源重新设计等）。都使用字符串集来表示。 字符串的矢量。字符串的Vectors是有序的，多用于堆栈痕迹，顺序对应堆栈级别。 请注意，不支持浮点数；在许多叶子上的浮点数上的聚合可能会导致太多准确度上的错误。作为替代，Scuba 建议用户选择他们关心的小数点位数，例如 5，并将 trunc(X ∗ 105) 存储为一个整数，而不是浮点 X。 由于 Scuba 捕获的数据是关于时间变化的现象，所以每一行都有一个强制性的时间戳。这些时间戳代表实际事件（客户端请求、错误报告、帖子等）的时间。任何表都可以有任意数量的一种或多种类型的列。所有的表都包含整数和字符串；只有一些表包含字符串的集合或向量。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:4:1","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"3.2 数据结构 图3显示了 Scuba 对每种数据类型所使用的压缩方法。可以用N个字节-1位自然表示的整数，直接用N个字节编码。字典编码是指每个字符串在字典中存储一次，其索引在字典里（一个整数）存储在行中。字符串列可以被压缩或不压缩存储，这取决于有多少不同的值。对于压缩后的字符串列，每个索引使用表示最大索引所需的位数来存储。未压缩的列存储原始字符串及其长度。对于字符串集，索引被排序和delta编码，然后每个索引被Fibonacci编码。(斐波那契编码使用的是可变位数。)编码后的索引在行中连续存储。对于向量，有一个2字节的字符串计数和一个1字节大小的最大字典索引的位数。然后，每个索引使用大小位数连续存储在行中。所有的字典都是每个叶子的局部，每个列的字典都是独立的。与在每列中存储8个字节的整数和原始字符串相比，压缩数据使其体积减少了6倍以上（每个表的情况不同）。 Scuba 目前以行的顺序存储表，因为它最初的用例在每次查询中都会访问表的大部分列。鉴于 Scuba 的用例从那时起变得更加通用，我们现在正在探索面向列的存储布局。其他人[18，8]已经表明，列存储一般可以得到更好的压缩和更好的缓存定位。 Scuba 的数据模型有两个关键点与标准关系模型不同。首先，没有创建表声明类型；每当叶子第一次接收到数据时，就会在每个叶子节点上创建一个表。由于叶子在不同的时间接收数据，表可能只存在于一些叶子上，并且可能在每个叶子上有不同的模式。然而，尽管模式不同，Scuba 通过将任何缺失的列视为空值，向用户展示了一个单一的表映像。其次，表的行中的列可能被自动地填充；在一个表中有 2 或 3 个不同的行模式或一个列随着时间的推移改变其类型（通常是为了实现更好的压缩）是很常见的。这两种差异加在一起，让 Scuba 无需任何复杂的模式演化命令或工作流，就能让表适应用户的需求。这种适应性是 Scuba 的优势之一。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:4:2","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"3.3 数据提取, 分发与生命周期 图2显示了数据导入 Scuba 的吞吐路径。Facebook 的代码库其中包含日志调用，用于将数据导入 Scuba。当事件发生时，这些调用会被执行，并且（在根据可选的采样率剔除部分数据之后）日志条目被写入Scribe。Scribe 是一个开源的分布式消息系统，用于收集、聚合和以低延迟交付大量日志数据。它由Facebook开发，并在Facebook广泛使用[5]。然后，一个尾随者进程订阅了为 Scuba 准备的主题，并通过 Scuba 的Thrift API将每一批新的行发送到 Scuba。(Thrift[7]是一个软件库，它为使用它定义的任何接口实现了跨语言的RPC通信)。这些传入的行完全描述了它们自己，包括它们的Schema。 对于每一批传入的行，Scuba 会随机选择两个叶子节点，并将该批行发送到有更多可用内存的叶子上。因此，每个表的行最终会被随机分区到集群中的所有叶子上。在任何表上都没有索引，尽管每个批次中的行在很短的时间内都有时间戳（然而，这些时间窗口可能在批次之间重叠，因为数据是在多个服务器上生成的）。 接收批处理的叶子节点将批处理文件的gzip压缩副本存储到磁盘上，以便持久化。然后它读取新行的数据，压缩每一列，并将行添加到内存中的表中。从一个事件发生到它被存储在内存中并可供用户查询的时间通常在一分钟之内。 内存（不是CPU）是 Scuba 的紧缺资源。我们目前每隔2-3周增加新机器，以跟上表和数据的增长速度。由于Scuba的目的是为了分析今天的数据，同时可能会进行周与周之间的比较，所以我们以接收新数据的同样速度删除旧数据，以限制表的大小。数据被修剪的原因有两种。 数据生命周期：由时间戳决定的行是否太老了。 数据大小: 该表是否已经超过了空间限制，而这一行是： 表中最古老的一个。 通常情况下, 大多数表的默认限制为30天1和100GB，尽管对于像fbflow这样保存网络流量数据的高容量表和记录广告点击和印象等创收数据的广告指标有更高的限制。每隔15分钟，一个cron作业就会驱逐超过其生命周期限制的数据。如果表超过了它的空间限制，表的最旧数据就会被驱逐，直到它低于其限制。 为了让一些数据保留的时间超过空间限制，Scuba 还提供了数据的子采样。在这种情况下，超过一定生命周期的的行的统一部分会被保留，其余部分会被删除。在未来，我们希望探索更复杂的抽样形式，如分层抽样，这中抽样可能会选择一组更有代表性的行。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:4:3","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"3.4 查询模型 Scuba 提供了三种查询接口，如图2右图所示。 图1中所示的基于网络的界面允许用户发出基于表格的查询，并从12种可视化方式中选择一种，包括表格、时间序列图、饼图、叠加区域图等。图4、图5和图6显示了另外三种可视化方式。在不同的可视化界面之间的切换只需要几秒钟的时间。 支持命令行界面接受SQL的查询。 支持基于Thrift的API允许从PHP、C++、Java、Python、Javascript等语言的应用程序代码中进行查询。 SQL接口和GUI本身使用Thrift接口向 Scuba 的后台发送查询。脚本也可以使用SQL或Thrift接口发出查询。最后，我们提供了一种机制支持用Javascript编写的用户定义函数进行查询。 Scuba 查询具有以下SQL查询的表达能力。 SELECT column, column, ..., aggregate(column), aggregate(column), ... FROM table WHERE time \u003e= min-timestamp AND time \u003c= max-timestamp [AND condition ...] GROUP BY column, column, ... ORDER BY aggregate(column) LIMIT number Scuba 的 SQL 支持的聚合函数包括传统的计数、最小、最大、总和和平均函数，以及其他通用函数，如总/分钟、百分比和直方图。WHERE子句必须包含一个时间范围，LIMIT子句的默认值是100,000行，以避免分组时的内存问题和客户端的渲染问题。GROUP BY和ORDER BY子句完全是可选的。 任何与字符串的比较都可能包括一个正则表达式。字符集的条件是 set-column 包括 string-list 的 any/all/none，set-column 为空。对字符串向量的条件是 vector-column 包括 string-list 的 any/all/none/start/end/within。字符串列表中字符串的顺序很重要。 Scuba 中不支持 join 表查询。当需要合并来自多个来源的数据时，通常在将数据移植到 Scuba 之前进行连接，保证 Scuba 中的已经是连接过的数据。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:4:4","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"3.5 执行查询 注: fanout 指数据扇出 图7显示了 Scuba 如何执行用户查询的步骤分解。聚合器和叶子之间的所有通信都是通过Thrift进行的。具体查询步骤如下: 客户端找到 Scuba 集群中的一个根聚合器并发送一个查询。Root Aggregator 收到查询，解析它，并验证它，以确保查询格式良好。 Root Aggregator 在集群中识别出另外四台机器，作为下一级的Intermediate Aggregator。这一步创建了一个五台机器的 fanouts（其他四台机器加上自己）。Root Aggregator用一个 sum 和一个 count 来替换任何平均函数（因此可以在最后进行聚合），并将查询发送给Intermediate Aggregators。 Intermediate Aggregator 再创建五个fanouts，并传播查询，直到每台机器上的（唯一）叶子聚合器收到查询。 Leaf Aggregator将查询发送到机器上的每个叶子节点服务器上并行处理。 Leaf Aggregator 从每个叶子节点服务器收集结果，并对它们进行聚合。聚合结果可以进行任何排序和限制约束，然而，对于它的限制，默认使用max(5 ∗ limit，100)，确保最后最高限制的所有组都能在树上一路传递。Leaf Aggregator还收集统计每个Leaf是否包含该表，它处理了多少行，以及有多少行满足条件并对结果有贡献。Leaf Aggrega-tor将其结果和统计数据返回给调用它的Intermediate Aggregator。 每个Intermediate Aggregator将收到的部分结果进行整合，并将其传播到聚合树上。 Root Aggregator 计算最终结果，包括任何平均值和百分比，并应用所有预设的的排序和限制条件。 Root Aggregator将结果返回给等待的客户端，通常在几百毫秒内。 关于Leaf Server如何处理查询的几个细节很重要。首先，每个Leaf Server可能包含零个或多个表的分区，这取决于表的大小和表存在的时间。非常新的或非常小的表可能只存储在我们集群中数千个叶子中的几个或几百个叶子上。 其次，Leaf Server必须扫描表的每个分区中时间范围与查询时间范围重叠的行。 非重叠的分区被跳过。这些分区的时间范围是 Scuba 唯一的 “索引 “形式。 第三，Leaf服务器优化了每个字符串列谓词的正则表达式匹配。每当字符串列使用一个字典（大致对应于每当字符串值很可能在列中重复的时候），Leaf服务器就会维护一个每查询的缓存，其中包含对每个字符串值匹配表达式的结果。这个缓存是以字符串的字典索引为索引的。 第四，如果一个聚合器或叶子服务器在超时窗口内（如10毫秒）没有响应，它的结果就会从最终计算中省略。我们发现，在实践中，这种方法效果很好，因为在回答Scuba查询时涉及大量的样本。少量数据的缺失并不会对平均和百分位数计算产生不利影响，通过忽略一些叶子实现的低得多的响应时间弥补了缺失数据。此外，Scuba 维护并检查一个独立的服务，以获得每个查询的每个表的预期行数。然后，它使用这个计数来估计数据的部分，以确保数据的准确性。 但数据准确性实际上是缺失的。如果分数是99.5%或更少，Scuba 会在GUI中打印一个警告。我们还在努力为那些需要100%准确结果的客户自动重复查询。 最后，每台物理机器上都运行着多个 Leaf 服务器和一个 Aggregator 服务器。每台机器都可以在聚合树的任何级别上提供聚合器服务器。 目前，聚合树的 fanouts 是五个。我们试验了 2,4,5,6 副本数的fanouts，经验上发现5个fanouts能产生最好的响应时间。独立地，Rosen等人[14]对聚合树的理论分析表明，响应时间最小的聚合树的fanouts是一个常数，与树中叶子节点的数量无关。他们的实证分析也表明，在他们的系统中，fanouts为5时导致响应时间最小。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:4:5","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"4. Scuba 的性能分析模型 如上一节所述，Scuba使用聚合树来执行用户查询。在本节中，我们构建了一个简单的系统分析模型。通过这个模型帮助我们了解单个查询的性能特征。 图8描述了这个模型的参数。我们首先描述用于查询处理的聚合树的参数。聚合树中fanout F和N台机器的预期级别数L如公式1所示。 大多数聚合器的实际 fanouts 量为 F 。然而，当 Scuba 集群中的机器数量 N 不是 F 的完美幂数时，最低一级的中间聚合器的 fanouts 量小于 F。等式 2 显示了聚合树中倒数第二层的fanouts。 F L-1代表聚合树中L-1层的聚合器数量。因此，R代表在树的最后一级，从F L-1中间聚合器中的每一个到达所有N个叶子聚合器的fanouts。回想一下，每台机器都有一个Leaf Aggregator）。 现在，我们可以用等式3描述L层的fanouts。 等式(3)中的最后一种情况描述了从叶子Aggregator到同一台机器上的叶子节点的fanoutD。 树中的每个Aggregator执行以下步骤。 将查询分发到每个子Aggregator并等待结果。 在收到每个子节点的结果时，将其纳入。当所有子代都有响应或查询超时时，停止等待。 将合并的聚合结果和响应统计信息返回给调用者。 请注意，一旦查询被Aggregator转发给它的子代，子代就会并行进行。所以总的响应时间为在公式4中，用L级的聚合树计算的查询由TL表示。 展开式(4)并结合式(3)可得式5。 TL因此是任何查询的预测时间（在没有查询争用的情况下）。我们通过在每个查询中插入TA、TD、TG和TS的实际值，并将它们与我们的实验结果进行比较，来验证（并完善）这个模型。然而，我们并没有在这里提出比较。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:5:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"5. 实验性评估 在本节中，我们介绍了在160台机器的测试集群上测量 Scuba 的响应时延和扩展性相关的实验结果。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:6:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"5.1 实验环境搭建 我们测试集群中的机器是英特尔Xeon E5-2660 2.20 GHz机器，内存为144 GB[19]。有四个机架，每个机架有40台机器，用10G以太网连接。操作系统是CentOS 5.2版本。 在这些实验中，我们将集群中的机器数量从10台变化到160台。在每个实验中，每台机器有8个Leaf Nodes和1个Aggregator。每个Aggregator总是作为叶子Aggregator，并可以另外作为中间和根Aggregator。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:6:1","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"5.2 实验数据和查询 实验的目的是为了隔离第4节中模型的各种参数，呈现最基础的 benchmark。因此，我们使用1个包含29小时价值的真实数据的表（从我们的生产集群复制）和2个非常简单的查询。表中的数据总量约为1.2TB。除非另有说明，每个叶子有1GB；每台机器有8个叶子，160台机器。(1 ∗ 8 ∗ 160 = 1280) 我们运行了两个不同的查询。 SELECT count(*), SUM(column1) as sum1, SUM(column2) as sum2 FROM mytable WHERE time \u003e= now()-3*3600 上面SQL中显示的第一个查询，在叶子上隔离扫描时间。它扫描了3个、6个或全部29个小时的数据（取决于常量），计算出3个聚合指标，并将这3个数字（和查询统计）传递到聚合树上（仅）。 SELECT count(*), sum(column1) as sum1, service, (time - now())/60*60 + now() as minute, FROM mytable WHERE time \u003e= now()-3*3600 and time \u003c= now() GROUP BY service, minute ORDER BY sum1 DESC LIMIT 1800 第二个查询比较复杂，尤其是在SQL中，如上图所示。该查询以 1 分钟的粒度生成过去 3、6 或 29 小时的时间序列图，为前 10 个服务中的每个服务绘制一条线。同样的查询在Scuba GUI中更容易执行，如图9所示。该查询为每个服务产生2个聚合指标，每分钟。1800=180分钟∗10个服务的限制。这个查询通过，并在聚合树的每一级聚合2个度量，每个度量都有1800点，其聚合时间TA是明显的。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:6:2","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"5.3 单机实验 第一组实验测试单个客户端的查询延迟。对于这些实验，我们使用29小时数据生命周期的版本的每个查询，并运行每个查询200次。我们绘制了平均响应时间，误差条表示最小和最大响应时间。 5.3.1 加速查询 我们首先测量了分布在20台机器集群中的数据的单次查询的速度。我们将每个Leaf的数据量从1GB到8GB不等。因此，数据总量从160GB到整整1.2TB不等。 图10显示了结果。每个叶子扫描数据的时间与数据量成正比。然而，聚合成本与每个叶子的数据量无关，它是查询和集群大小的函数。在这个实验中，集群大小是恒定的。20台机器，扇出量为5，树上有3个层次（1个根聚合器，5个中间聚合器，20个叶聚合器）。扫描查询只经过聚合树上的一个点，所以聚合需要的时间可以忽略不计。时间序列查询需要在树的每一级聚合大量的点，所以需要更长的时间。 5.3.2 扩展 接着，当我们将集群中的机器数量从10台变化到160台（每次将机器数量增加一倍）时，我们会衡量规模的扩大。每个Leaf有1GB的数据。 图11显示，扫描数据的时间（在每个Leaf上并行完成）是恒定的。聚合成本随着N的增长而呈对数增长。由于对于扫描查询来说，聚合成本可以忽略不计，所以随着机器数量的增加，其响应时间是恒定的。然而，时间序列查询需要在每个聚合器处聚合许多点，其响应时间随着聚合器的数量和聚合树的层数增加而增加。第4节中提出的模型对帮助我们理解这些扩展结果非常有用。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:6:3","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"5.4 多机实验 最后一个实验测试的是随着客户端数量从1个增加到32个，查询延迟和吞吐量。每个客户端连续发出200个查询，它们之间没有时间间隔。在这个实验中，我们使用160台机器，每个Leaf有1GB的数据。我们还使用了扫描和时间序列查询的3小时、6小时和29小时变体。3小时变体只需要扫描每个Leaf处大约10%的数据。 图12显示了当我们改变客户端数量时，每个查询的吞吐量。首先要注意的是，对于每个查询，吞吐量随着客户端数量的增加而上升，直到叶子上的CPU达到饱和。之后，吞吐量趋于平缓。对于所有的查询，在8个客户端之后，吞吐量是持平的。接下来的无表点是，在相同的时间范围内，扫描查询的吞吐量都比它们的时间序列查询对应的吞吐量高。这一点并不奇怪，因为扫描查询的速度更快。最后，对于给定的客户端数量，随着查询的时间范围（因此扫描的数据量）的增加，每种查询类型的吞吐量都会下降。 图13显示，查询的响应时间与客户端数量成正比增加，符合预期。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:6:4","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"6.相关工作 与 Scuba 相关的工作可分为四类。 首先，有其他的系统用于临时性的实时分析。HyPer[11]也是将数据存储在内存中，但是是在一台巨型的、昂贵的机器上。另一方面，Scuba 使用相对便宜的商品计算机集群，并且通过增加更多的机器轻松扩展。HyPer也不使用压缩，因此对于同样数量的数据，它需要更多的内存。 Powerdrill[10]和Dremel[12]是Google的两个用于分析的数据管理系统。两者和 Scuba 一样，都是高度分布式的，而且扩展性很好。Dremel提供了比 Scuba 复杂得多的半结构化和稀疏数据模型；据报道，Powerdrill的速度比Dremel快得多，但仍需要30-40秒/查询。在这两个系统中，数据的主副本都住在磁盘上，所以存储的数据量不那么重要。 Splunk[6]也是一个导入日志数据、分析数据并以图表形式查看的系统。它的目的是针对在 “云 “中产生和分析的数据。我们没有关于它无论是导入还是查询数据的速度的数据。 其次，有多种系统使用压缩来重新减少内存和/或磁盘的占用。Westman等人[21]采用基于行的布局，发现字典在压缩率和CPU使用量之间提供了很好的权衡。C-Store[18]和Vertica、SAP Hana[15]、Dremel和Powerdrill都采取了基于列的方法，我们接下来想尝试一下，以获得更好的压缩效果。 第三，上述系统都不能用准确率来换取响应时间，Scuba 有意这么做。然而，BlinkDB[9]可以在有界时间内或有界精度的情况下，在采样数据集上产生结果。虽然BlinkDB在获取查询之前需要预先计算分层样本，但我们希望通过实验并优化其底层技术来更好地约束和报告 Scuba 的不准确度。 最后，Scuba 中的所有数据都有时间戳，很多分析都是基于时间的。在20世纪80年代末，Snodgrass 创建了 TQuel[16，17] 来推理数据的时间和间隔。我们应该重新审视这项工作，看看是否有 Scuba 应该加入的功能。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:7:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Coding"],"content":"7. 总结 Scuba 与大多数数据库系统有多种不同，文章所描述的这些都集中在 Scuba 适合我们在 Facebook 的使用案例上。 Scuba 驱逐数据的速度与它摄入数据的速度一样快，因为所有的表数据都存储在内存中，而内存是稀缺资源。驱逐是自动的，不过每个表保留多少数据的参数是可以调整的。 Scuba 设计上期望表包含的是采样数据而不是原始数据，因为存储每个事件都会有太多的数据。采样率列会被特殊处理，查询结果包含一个原始计数和一个根据表中的采样率调整的计数。(可以有多个不同的采样率，每行可以有一个。) Scuba 的数据导入就是在代码中插入，并创建一个进程来监听这些事件。不需要任何模式声明；模式是从日志中推导出来的，它可以随着时间的推移而变化。 Scuba 并不打算成为一个完整的 SQL 数据库，它支持分组和聚合，但不支持连接或嵌套查询。它支持分组和聚合，但不支持连接或嵌套查询。它支持整数和字符串，但不支持浮动，尽管它也添加了字符串的集合和向量（但不支持类型的任意嵌套）。 我们想对Scuba进行一些修改。它目前是面向行的，尽管我们正在探索面向列的布局是否会更好。Scuba 可以使用本地警报；此外，支持用户在 Scuba 查询结果之上写警报。随着用户群的增长，Scuba 还需要继续扩展。 本文介绍的模型和实验是弄清它目前的扩展情况的第一步。 尽管如此，自两年前首次编写 Scuba 以来，Scuba 在 Facebook 的用户数、数据数和查询数都在飞速增长。Scuba 提供了导入和查询数据的灵活性和速度，这对 Facebook 的实时性能和数据分析至关重要。 ","date":"2021-02-05","objectID":"/zh-cn/scuba-diving-into-data-at-facebook/:8:0","tags":null,"title":"译《Scuba: Diving into data at Facebook》","uri":"/zh-cn/scuba-diving-into-data-at-facebook/"},{"categories":["Gossip"],"content":"难得出太阳的一天。 北京的天总是阴沉沉的，有时候好像是要开启一场庭审，让我紧张得四肢发麻。 二月的第一天，终于有了一些不同。 敞亮的日光，透过大片鲜绿的树叶凶猛地穿透进来。 一抬头，世界在光明刺眼的色调里猛涨。 浑身暖洋洋。 天上的云彩白的好像一个个凸出来的拳头。 我希望它们沉下来，重重砸在我脸上。 将我击碎打散， 那下午便也不用上班。 我给自己定了个目标，这一年里，要看够 1750 部电影。 没想一个月过去了，只看到了 1518 部。 要是电影可以投射在云层里，那我便天天都可以看电影了。 ","date":"2021-02-01","objectID":"/zh-cn/cloud-movie-projector/:0:0","tags":["Gossip"],"title":"云幕电影放映机","uri":"/zh-cn/cloud-movie-projector/"},{"categories":["Gossip"],"content":"「I hate people !」 早上摸鱼时从友邻广播里看到这句话. 广播里讲了个故事，一位很健谈的朋友，一边讨厌人类，一边等个红灯都能跟人聊五块钱的，从椰子鸡到加拿大冰球，什么都聊。 如此言行不一致, 遭到了质疑。 朋友解释道，people in general 才是让他厌烦之处。When it gets personal, 那世界就变得有趣起来。 我开始反思。 是啊, people 是个非常 general 的概念, 是无具象的. 但当讨论到 person. 那这个人开始有了名字，有了声调. 这个人从\"people\"的群体里脱离出来，变得具象起来。 every single person 各有各自值得喜欢的地方。 只要一想到一个人类，无论他或她在哪里，在做什么想什么，都在发出悠长的呼吸; 睡梦时眼皮微微滑动，在做梦 睡醒时眼皮被太阳直登登地打亮 这样鲜活的人，多浪漫呀. 哪儿会有人不喜欢具象的人呢。 反观下来，自己似乎常常追逐一些抽象的事物。 越是虚妄，越着迷。 像是对超级英雄有着的不切实际的幻想, 渴望超能力的降临. 而抽离回生活中，却只能靠具象之物来校正这份抽象，而无法放弃抽象本身。 即使那些肌肤相亲的实景实物让人感到愉悦，但也会怀疑是否是自己赋予了某种抽象的意义. 王小波说，人是轻易不能知道自己的。 因为我们的感官是向外的，能察觉到他人细微的变化。但对于自己却不十分了然。 我不太信。 我爱游离在幻想与现实的边界。 有时，我想我爱自己多过爱他人。 我希望我的自我永远滋滋作响，翻腾不休，就像火炭上的一滴糖。 只是转念一想， 椰子鸡也好吃，加拿大冰球，也挺酷。 还有铲冰激凌，炸得焦脆的牛奶块。 打开冰箱门满满的饮料。 冬天里猫咪柔软的肚子，以及情人温软的胸膛。 美好得令人惭愧。 你说，从今天起, 我也要开始学习爱人，爱他人，爱具象的人。 像王小波一样, 像友邻的朋友一样， I hate people, But I love person. ","date":"2021-01-29","objectID":"/zh-cn/i-hate-people/:0:0","tags":["Gossip"],"title":"I hate people","uri":"/zh-cn/i-hate-people/"},{"categories":["Coding"],"content":"摘要 原文链接：https://dl.acm.org/doi/pdf/10.1145/3342195.3387524 在许多公共和私有云系统中，用户需要指定资源量（CPU内核和RAM）的限制以为其工作负荷提供资源。 超出其限制的作业可能会受到限制或终止，从而导致最终用户请求的延迟或丢弃，因此人工操作人员针对这种问题出于谨慎考虑，会申请高于任务自身需要的配置。 从规模上讲，这将导致大量的资源浪费。 为了解决这个问题，Google使用Autopilot自动配置资源，同时调整作业中的并发任务数（水平缩放）和单个任务的CPU /内存限制（垂直缩放）。 Autopilot与人工操作员遵循相同的原则：Autopilot的主要目标是减少松弛（slack）（申请资源和实际资源使用之间的差异），同时最大程度地降低因内存不足（OOM）错误或 由于CPU节流，其性能下降。 Autopilot将机器学习算法应用到有关作业先前执行情况的历史数据上，再加上一组经过微调的启发式方法来实现这一目标。 在实践中，Autopilot工作只有23％的松弛（slack），而手动管理工作只有46％的松弛（slack）。 此外，Autopilot将受OOM严重影响的工作数量减少了10倍。 尽管有其优点，要确保Autopilot被广泛采用仍需付出巨大的努力，包括使尚未加入的客户容易看到潜在的建议，自动迁移某些类别的任务以及增加对自定义推荐器的支持。 在撰写本文时，Autopilot任务占Google资源使用的48％以上。 ACM参考格式： Krzysztof Rzadca，Pawel Findeisen，Jacek Swiderski，Przemyslaw Zych，Przemyslaw Broniek，Jarek Kusmierek，Pawel Nowak，Beata Strack，Piotr Witusowski，Steven Hand和John Wilkes。 2020年。 Autopilot：Google的工作负载自动缩放。 在第十五欧洲2020年4月27日至30日，计算机系统会议（EuroSys'20），希腊伊拉克利翁。 ACM，美国纽约，纽约，共16页。 https://doi. org/10.1145/3342195.3387524 1 介绍 许多类型的公共云和私有云系统要求其用户声明在执行期间其工作负载将需要多少个实例以及每个实例所需的资源：在公共云平台中，用户需要选择他们需要租用虚拟机（VM）的类型和数量； 在Kubernetes集群中，用户可以设置Pod副本的数量和单个Pod的资源限制; 在Google中，我们要求用户指定所需的容器数量以及每个容器的资源限制。 这些限制使云基础架构能够提供足够的性能隔离，从而使云计算成为可能。 但是限制（主要是）对用户造成了麻烦。 很难估计一个作业需要多少资源才能最佳运行：CPU功率，内存和同时运行的副本数的正确组合。 负载测试可以帮助找到初始估计值，但是随着资源需求随时间变化，这些建议将过时，因为许多最终用户服务工作具有每日或每周的负载模式，并且随着服务变得越来越流行，流量在更长的时间内发生变化 。 最后，处理给定负载所需的资源会随着基础软件或硬件堆栈的新功能，优化和更新而变化。如果CPU容量不足，超出请求的资源可能会导致性能下降，或者导致任务被杀死 内存不足（OOM）。 都不是好事。 从调研结果看，理性的用户将故意高估其工作所需的资源，从而导致对物理资源的不良利用。 一项分析[26]对在一个Google集群[27]上执行的为期一个月的作业跟踪显示，平均内存利用率为50％； 对阿里巴巴YARN集群的另一项分析[23]显示任务的峰值内存利用率从未超过80％。 针对配置资源的困难，一种常见的模式是采用水平自动缩放器，该缩放器通过监控终端用户流量或平均CPU利用率的变化添加或删除副本来缩放任务。 所有主要的云提供商（AWS，Azure和GCP）都提供水平自动扩展功能； 它在某些云中间件（如Kubernetes）中也可用。 较不常见的模式是使用垂直自动缩放来调整每个副本可用的资源量。 两种技术也可以组合。 Autopilot是Google在其内部云上使用的主要自动缩放器。 它提供水平和垂直自动缩放。 本文重点介绍Autopilot的内存垂直扩展，因为这种情况鲜为人知。 论文： 描述下Autopilot，以及它用于垂直自动缩放的两个主要算法：第一个算法依赖于历史用量的指数平滑滑动窗口； 另一个是基于从强化学习中借用的思想的元学习，该算法运行滑动窗口算法的许多变体，并为每个任务选择历史数据表现最佳的算法。（译注：强化学习：依赖海量的训练，并且需要精准的奖励。成本较高且比较复杂。元学习：具备自学能力，能够充分利用过去的经验来指导未来的任务。被认为是实现通用人工智能的关键。） 通过Google的工作负载采样评估Autopilot算法的有效性； 讨论为使我们的集群广泛采用Autopilot而采取的步骤。 2 通过Borg管理云资源 Autopilot的目标和制约因素来自Google的Borg基础架构，并且针对Google的工作负载进行了调整。 我们在此处提供了两者的简要概述：有关Borg的更多详细信息，请参见[34]，有关工作负载的更多详细信息，请参见[26、27、31、35]。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:0:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"2.1 机器、作业和任务 Google计算基础架构由分布在多个地理位置的许多集群组成。一个中值集群大约有10,000台物理机，并同时运行许多不同类型的工作负载。一台物理计算机可能会同时计算大量占用内存和CPU的批处理，提供存储和内存数据库的切片提供查询能力，还可以处理对延迟敏感的终端用户请求。 （We call a particular instance of a workload a job.）我们称工作负载的特定实例为工作。一项作业由一个或多个任务组成。任务在单个物理计算机上执行；一台机器可以同时执行多个任务。作业是与具有某些功能的服务（例如文件系统或身份验证服务）相对应的逻辑实体；任务执行实际工作，例如为最终用户或文件访问请求提供服务。一项作业持续数月的情况并不罕见，尽管在这段时间内我们可能会多次执行该任务运行的二进制文件。在作业发布时，新任务逐渐取代了运行旧二进制文件的任务。 我们运行的工作负载可以分为两类：服务和批处理。服务工作通常旨在严格保证查询响应时间的性能（例如，在95％的位置上，请求延迟服务级别目标或SLO≤50 ms）。如此严格的等待时间要求排除了OS内核决定之外的任何带内资源分配决策，因此服务作业具有为其明确请求预留的资源。相反，批处理作业旨在“快速”完成并退出，但通常没有严格的完成期限。服务性工作是我们基础架构能力的主要驱动力，而批处理工作通常会填充剩余或暂时未使用的能力，如[4]所示。 内存不足（OOM）事件终止单个任务。某些工作可以合理地容忍OOM事件。有些根本不是；还有一些介于两者之间。总体而言，由更多任务组成且状态较少的作业在单个任务终止时遇到的服务故障越小，因此对OOM容灾能力更高。有些作业需要低的，可重复的延迟来处理请求。有些没有。Autopilot会根据作业的声明大小、优先级和类别选做默认值，但允许我们的用户覆盖它们。Borg驱逐任务，为了安全性和OS更新，为更高优先级的任务腾出空间，并让我们更有效地将工作打包到计算机上。我们有意识的通过计算集群架构和应用的弹性来分散负荷。系统期望通过弹性解决任务驱逐和硬件故障问题。Borg发布了预期内最大驱逐率，观察到的驱逐率之间的差异也使我们可以自由地使用任务的资源设置进行实验-在了解任务需要时可以偶尔进行OOM。 （使用VM实时迁移之类的工具来隐藏外部Cloud VM的这些内部优化。）一个典型的服务工作有很多任务，并且负载均衡器将流量驱动到可用负载，因此丢失任务只会导致其负载平摊给其他任务。 这是正常现象，而不是灾难性的故障，它使我们能够更加积极地利用基础架构，并能够妥善处理偶发的硬件故障。 使用MapReduce [6]中使用的技术，在批处理作业中也内置了类似的弹性。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:0:1","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"2.2 Borg调度架构 每个集群都由我们专用的集群调度程序Borg的专用实例管理。 下面，我们简要介绍Borg架构和功能, 因为会直接影响Autopilot设计； 请参阅[34]以获取完整说明。 Borg有一个多复本的Borgmaster，负责制定计划决策，还有一个称为Borglet的代理程序，它在集群中的每台计算机上运行。 一台机器可以同时执行Borglet控制的数十个任务； 依次将其状态和资源使用情况报告给Borgmaster。 将新作业提交给Borgmaster时，它会选择一台或多台机器，这些机器上有足够的空闲资源来执行新提交的作业–或通过驱逐优先级较低的作业来腾出空间来创建这种情况。 在Borgmaster决定将任务的任务放置在何处后，它将启动和运行任务的过程委派给所选计算机上的Borglet。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:0:2","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"2.3 通过任务限制进行资源管理 为了获得可接受和可预测的性能，必须将一台计算机上运行的任务相互隔离。 与Kubernetes一样，Borg在单独的Linux容器中运行每个任务，并且本地代理设置容器资源限制以使用cgroup实现性能隔离。 与操作系统级别的传统公平共享不同，这可确保任务的性能在同一二进制文件，不同机器（只要硬件相同）和不同邻居（任务在同一机器上共同调度）的不同执行之间保持一致 ）[38]。 在我们的架构中，CPU和RAM是需要管理的关键资源。 我们使用术语限制来指代通常可以消耗的每种资源的最大允许量。 由于Borg通常将作业的任务视为可互换的副本，因此所有任务通常具有相同的限制。 作业以标准化的毫核心表示其CPU限制[31]，并且此限制由标准Linux内核cgroups机制强制执行。 如果争用很少（以总体CPU利用率衡量），则允许任务使用CPU超出其限制。 但是，一旦发生争用，就会强制执行限制，并且可能会限制某些任务以在其限制内运行。 作业以字节为单位表示其内存限制。 与标准Linux cgroup一样，作业的RAM限制的实施可能是硬性的也可能是软性的，并且作业的所有者在提交作业时声明了实施类型。 一旦使用硬RAM限制的任务超过其限制，该任务就会因内存不足（OOM）错误而被杀死，并将故障报告给Borgmaster。允许使用软RAM限制的任务可以内存超过其限制，但如果计算机上的总体RAM利用率过高，则Borglet将开始杀死超出其限制的任务（带有OOM错误），直到该计算机不再受到威胁（参见标准）为止。 cgroup实施，只是防止超限容器保留更多的内存）。 Borg允许作业在作业运行时修改其资源需求。在水平缩放中，作业可以动态添加或删除任务。在垂直扩展中，作业可以更改其任务的RAM和CPU限制。增加作业的RAM和CPU限制是一项潜在的高成本操作，因为某些任务可能不再适合他们的计算机。在这种情况下，这些计算机上的Borglet将终止一些优先级较低的任务；反过来，这些任务将重新安排到其他计算机上，并可能触发其他优先级更低的任务的终止。 （几年前，我们大大降低了优先级的有效数量，以减少这种等级的数量）。 尽管通常会过度设置作业限制，但存在一些背压：我们向服务用户收取他们保留的资源（而不是他们使用的资源）的费用，并且请求的资源会递减用户的配额-这是对用户的严格限制他们可以在集群中的所有作业中获得的资源总量。 这类似于对公共云中的VM使用定价和配额。 收费和配额都有帮助，但实际上效果有限：配置不足的弊端通常远远超过通过请求较少资源获得的收益。 我们发现这是一个反复出现的主题：在实践中，理论上可达到的效率通常很难实现，因为手动执行所需的努力或风险过高。 我们需要一种自动进行权衡的方法。 这就是Autopilot所做的。 3 Autopilot自动限制 Autopilot使用垂直缩放来微调CPU和RAM限制，以减少冗余（即资源限制与实际使用之间的差异），同时确保任务不会耗尽资源。 它还使用水平缩放（更改作业中的任务数）来适应更大的工作负载更改。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:0:3","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"3.1 架构 Autopilot的功能体系结构是三合一的闭环控制系统，一个用于每个作业级别的水平缩放，另两个分别用于每个任务资源（CPU和内存）的垂直缩放。该算法（在后续章节中详细介绍）充当控制器。Autopilot单独考虑作业-没有跨作业学习能力。 Autopilot的实现（图1）采用架构上的一组标准作业的形式：每个集群都有自己的Autopilot。每个Autopilot的资源推荐器（根据其历史使用情况来确定任务的大小）都作为单独的任务运行，并具有三个任务副本以提高可靠性。多复本的Autopilot服务会选择负责选择用于作业的主推荐器，并通过执行器作业将推荐信息（过滤后的）传递给Borgmaster。如果请求更改任务数量，则Borgmaster会相应地生成或终止任务。如果请求更改资源限制，则Borgmaster首先会做出任何需要适应这些限制的调度决定，然后联系适当的Borglet代理以应用更改。一个独立的监控系统跟踪每个任务使用多少资源。Autopilot只是从中订阅更新。 今天，我们的用户通过一个简单的标志设置就明确选择了使用Autopilot的工作。 我们正在将其设置为默认值，并且允许显式不使用。 用户还可以通过多个方面配置Autopilot行为，例如：（1）强制所有副本具有相同的限制，这对于只有一个活动主服务器的容错程序很有用；（2）增加限制，以便来自另一个群集中的副本作业的负载将能够立即故障转移到该群集。 将Autopilot的作业提交给Borgmaster时，它将暂时排队，直到Autopilot有机会为其提出初步资源推荐为止。 之后，它将继续进行正常的调度过程。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:1:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"3.2垂直（每任务）自动缩放 Autopilot服务根据要自动缩放的资源是内存还是CPU，来选择要用于作业的推荐器。 作业对资源不足事件的容忍度（延迟容忍度与否，OOM敏感度与OOM容忍度） 以及可选的用户输入（其中可能包括明确的推荐者选择）或其他参数来控制Autopilot的行为，例如可以设置的上限和下限。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:2:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"3.2.1预处理：汇总输入信号 推荐器使用预处理的资源使用信号。 大部分预处理是由我们的监控系统完成的，以减少历史数据的存储需求。 聚集信号的格式类似于[35]中提供的数据。 低级任务监控记录原始信号是针对一项工作的每个任务的时间测量序列。 我们将监视系统在任务time的时间recorded记录为𝑟𝑖[𝜏]。 此时间序列通常每1秒包含一个样本。 （例如，TABLE1。用于描述推荐者的符号 𝑟𝑖[𝜏] 每个任务的原始CPU / MEM时间序列（1s分辨率） 𝑠𝑖[𝑡] 按任务汇总的CPU / MEM时间序列（直方图，5分钟分辨率） 𝑠[𝑡] 每个作业的汇总CPU / MEM时间序列（直方图，5分钟分辨率） ℎ[𝑡] 每个作业负载调整后的直方图 𝑏[𝑘] 直方图的第k个bin的值（边界值） 𝑤[𝜏] the weight to decay the sample aged 𝜏 𝑆[𝑡] 时的移动窗口推荐𝑡 𝑚 模型（参数化的arg min算法） 𝑑𝑚 使用model模型的衰减率 𝑀𝑚 使用model的安全浮动空间 𝐿 推荐器测试的极限值 𝐿𝑚[𝑡] 使用模型𝑚推荐的极限值 𝐿[𝑡] ML推荐器的最终推荐 𝑜（𝐿）限度的超支成本𝐿 𝑢（𝐿）限制的底线成本𝐿 𝑤𝑜 超支成本的权重 𝑤𝑢 不足成本的权重 𝐿Δ𝐿 改变限制的惩罚权重 𝑚Δ𝑚 权改变模型的惩罚重 𝑑 衰减率，用于计算模型成本 𝑐𝑚[𝑡] 模型m的（已减少）历史成本 CPU或RAM使用率，或收到的查询数） 为了减少设置作业限制时存储和处理的数据量，我们的监视系统将𝑟𝑖[𝜏]预处理为汇总信号𝑠[𝑡]，该信号通常在5分钟的窗口内汇总值。 汇总信号𝑠[𝑡]的一个样本是一个直方图，总结了这5分钟内所有工作任务的资源使用情况。 更详细的内容。对于每个窗口，聚合的每个任务信号𝑠𝑖[𝑡]是一个矢量，其中的直方图保持在原始信号𝑟𝑖[𝜏]上，其中𝜏∈𝑡。对于CPU信号，此向量𝑠𝑖[𝑡] [𝑘]的元素对落入大约400个使用桶中的每个原始信号样本signal [𝜏]的数量进行计数：𝑠𝑖[𝑡] [𝑘] = |{𝑟𝑖[𝜏] : 𝜏 ∈ 𝑡 ∧𝑏 [𝑘 − 1] ≤ 𝑟𝑖[𝜏] \u003c 𝑏 [𝑘]}|，其中𝑏[𝑘]是第k个bin的边界值（这些值由监视系统固定）。对于存储信号，我们在直方图中仅记录5分钟窗口内任务的峰值（最大）请求（即，每个任务直方图𝑠𝑖[𝑡] [𝑘]仅具有一个非零值）。内存信号使用峰值而不是整个分布，因为我们通常希望提供（接近）峰值内存使用率：与CPU相比，任务对内存不足配置（因为它将以OOM结尾）更加敏感。它只会受到CPU限制）。 然后，我们通过简单地将𝑠[𝑡] [𝑘] = Σ𝑖 𝑠𝑖[𝑡] [𝑘]，将每个任务的直方图𝑠𝑖[𝑡]聚合为单个的每个作业的直方图𝑠[𝑡]。 我们没有明确考虑个别任务-例如，通过检查极端价值。 由于大多数Borg作业中的单个任务都是可互换的副本，因此除少数特殊情况外，我们对所有其他任务使用相同的限制。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:2:1","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"3.2.2 移动窗口推荐器 移动窗口推荐器通过使用聚合信号𝑠 的统计信息来计算限制。 我们希望限制随着使用量的增加而迅速增加，但要在负载减少后缓慢减小，以避免对工作量暂时下降的过快响应。 为了平滑对负载尖峰的响应，我们通过指数衰减权重𝑤[𝜏]对信号进行加权： 其中𝜏是样本年龄，而𝑡1/ 2是半衰期：权重减少一半之后的时间。 Autopilot适合长期运行的工作：我们将CPU信号的半衰期设为12小时，将内存的半衰期设为48小时。使用以下有关的统计信息之一来计算时间𝑡的推荐𝑆[𝑡]： 峰值(𝑆max) 返回最近采样的最大值，𝑆max [𝑡] = max𝜏 ∈ {𝑡−(𝑁 −1),…,𝑡 } {𝑏 [𝑗] : 𝑠[𝜏] [𝑗] \u003e 0}, 最近N个样本中非空存储桶的最大值，其中N是固定的系统参数。 加权平均 (𝑆𝑎𝑣𝑔)，计算平均信号值的时间加权平均值： 𝑠[𝜏]是直方图的平均使用率，%j 调整使用量(𝑆𝑝𝑗),首先计算负载调整后的衰减直方图ℎ[ℎ]，其第𝑘个元素ℎ[𝑡] [𝑘]将第𝑘个桶中的衰减样本数乘以负载𝑏[𝑘]： 然后返回该直方图的某个百分位数𝑃𝑗（ℎ[𝑡]）。 ℎ与标准直方图𝑠的区别在于，在𝑠中，每个样品都具有相同的单位重量，而在ℎ中，存储桶𝑘中的样品重量等于负载𝑏[𝑘]。 请注意，负载调整后的使用率的给定百分位数随时间的变化可能与相同的使用率百分位数显着不同。在许多情况下，我们要确保在设置限制以容纳提供的负载时可以提供给定负载的给定百分位数，而不是简单地计算瞬时观测到的负载可以处理的次数-即，我们希望根据负载而不是样本数量来加权计算。图2中说明了这种差异：如果将极限设置为1（时间的90％），则最后时刻的9/19单位负载将超过极限（虚线下方）。在这种情况下，负载调整后的直方图ℎ的计算如下。由负载调整后的直方图count计数的单个观测值可以解释为在一定负载（信号的当前电平）下处理的信号区域的单位。 ℎ等于ℎ[1] = 1·9（9个时间单位中的1的负载）和ℎ[10] = 10·10（1个时间单位中10的负载）。因此，ℎ的90％ile（即要求以极限值或低于极限值处理90％的信号区域所需的极限）为10 –在这种情况下，意味着可以在极限范围内处理整个信号. 图2.负载信号的90％ile可能与负载时间曲线积分的90％ile显着不同。在此示例中， 使用基于时间的样本，负载的90％ile为1个单位，但如果还考虑了负载的大小，则为10个单位 Autopilot根据信号使用以下统计信息和作业类行。 对于CPU限制，我们使用： 批处理作业：𝑆𝑎𝑣𝑔，这是因为，如果一项作业可以承受CPU限制，则对基础结构的最有效限制是该作业的平均负载，这使该作业得以继续进行而不会累积延迟。 服务作业：根据负载对延迟的敏感度，调整负载后的使用量是𝑆𝑝95（95％ile）或𝑆𝑝90（90％ile）。 对于内存，Autopilot根据作业的OOM公差使用不同的统计信息。 对于大多数大型作业，默认设置为“低”，对于小型作业，默认设置为“最小”（最严格），但可由用户覆盖： 𝑆𝑝98 适用于OOM容错低的作业。 𝑆max 适用于OOM容错小的作业。 对于具有中等OOM容错的作业，我们选择值来（部分）覆盖短负载峰值。 我们通过使用最大值𝑆𝑝60（加权的60％ile）和最大值0.5％（最大值）的一半来做到这一点。 最后，这些原始建议在应用之前先进行后处理。 首先，建议增加10％到15％的安全裕度（对于较大的限制较小）。 然后，我们采用过去一小时内看到的最大推荐值，以减少波动。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:2:2","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"3.2.3基于机器学习的推荐器 原则上，Autopilot解决了机器学习问题：针对一项工作，根据其过去的使用情况，找到一个限制，可以优化表示该工作和基础架构目标的功能。上一节中描述的方法（基于对移动窗口的简单统计来设置限制）指定了解决此问题的算法。相比之下，Autopilot的ML推荐器从成本函数（所需解决方案的规格）开始，然后针对每个工作选择合适的模型参数来优化此成本函数。这种自动化使Autopilot可以为每个作业优化先前方法为所有作业固定的参数，例如衰减率𝑡1/ 2，安全裕度或缩减稳定期。 在内部，ML推荐器由许多模型组成。推荐者针对每项工作定期选择效果最佳的模型（根据下面根据历史使用情况计算得出的成本函数）；然后由所选模型负责设置限制。每个模型都是简单的arg min类型算法，可最大程度地降低成本–模型因分配给arg min（译注：arg min 就是使后面这个式子达到最小值时的变量的取值）各个元素的权重而异。机器学习方法反复出现的问题之一是其结果的可解释性[8]：在Autopilot中，推荐器设置的限制必须向工作负责人解释。拥有许多简单的模型有助于解释ML推荐器的操作：单个模型大致对应于推断的工作特征（例如，稳定时间长的模型对应于利用率变化迅速的工作）。然后，考虑到所选模型所施加的权重，其决策很容易解释 更正式地说，对于信号𝑠，在时间𝑡，ML推荐器从一组模型{𝑚}中选择一个单个模型𝑚[𝑡]，用于推荐极限值。模型是参数化的arg min算法，可计算给定历史信号值的极限。模型𝑚由衰减率𝑑𝑚和安全裕度𝑀𝑚设置参数。 在每个时刻𝑡，模型都会测试所有可能的极限值𝐿（可能的极限值对应于直方图块的后续边界𝐿∈{𝑏[0]，…，𝑏[𝑘]}）。 对于每个极限值，模型都会根据最近的使用直方图𝑠[𝑡]计算当前的超支和超支成本，然后用历史值对其进行指数平滑。 超支成本𝑜（𝐿）[𝑡]计算最近直方图中超出限制𝐿的存储桶中的样本数： 同样，cost 𝑢(𝐿) [𝑡] 计算低于限制𝐿的存储桶中的样本数， 然后，模型选择一个极限𝐿 ′ 𝑚 [𝑡]，以使极限的可能变化最小化超限，超限和罚分Δ(𝐿, 𝐿′ 𝑚 [𝑡 − 1])的加权和： 如果𝑥≠𝑦，则Δ（𝑥，𝑦）= 1，否则为0。 （使用Kronecker delta，Δ（𝑥，𝑦）= 1-𝛿𝑥，𝑦。）此函数包含在大型系统中做出资源分配决策的三个关键成本。 溢出表示损失机会的成本–在服务工作中，发生溢出时，查询会延迟，这意味着某些最终用户可能不太愿意继续使用该系统。 营业不足表示基础设施的成本：工作储备的资源越多，所需的电力，机器和人员就越多。 惩罚项Δ有助于避免过于频繁地更改限制，因为这可能导致任务不再适合其当前计算机，从而导致其（或其他任务）被逐出. 最后，将限制增加安全系数𝑀𝑚，即 为了在运行时选择模型（从而优化特定作业的衰减率𝑑𝑚和安全裕度）），ML推荐器为每个模型维护其（指数平滑）成本𝑐𝑚，它是超限，不足和限制更改的惩罚： 由于历史成本包括在𝑐𝑚 [𝑡 − 1],中，因此给定模型的欠额𝑢𝑚和超额𝑜𝑚成本仅考虑最新成本，即直方图样本数超出最后一个样本的限制，因此𝑜𝑚 (𝐿𝑚 [𝑡], 𝑡) = sigma:𝑏 [𝑗]\u003e𝐿 𝑠[𝑡] [𝑗], and 𝑢𝑚 (𝐿𝑚 [𝑡], 𝑡) = sigma 𝑗:𝑏 [𝑗]\u003c𝐿 𝑠[𝑡] [𝑗]. 最后，推荐人选择的模型可以最大程度地降低此成本，但要更改限额和模型会受到其他惩罚： 总体而言，该方法类似于multi-armed bandit problem（参考：https://blog.csdn.net/coffee_cream/article/details/58034628） 多臂老虎机问题，其中匪徒的“手臂”与限制值相对应。但是，多臂匪的关键特性是，一旦选择了一个分支，我们就无法观察到所有其他分支的结果。相反，一旦知道下一个时间段的信号，Autopilot便可以计算所有可能极限值的成本函数-除非极少数情况下，激活极限太小且任务以OOM终止（我们展示OOM在4.3节中很少见）。这种完全的可观察性使我们的问题变得相当容易。 集成具有五个超参数：上面定义的成本函数中的权重（𝑑，𝑤𝑜，𝑤𝑢，𝑤Δ𝐿和𝑤Δ𝑚）。这些权重大致对应于美元机会与美元基础设施成本。我们在离线实验中调整这些超参数，在这些实验中，我们根据从代表性工作中获取的已保存迹线的样本来模拟Autopilot的行为。这种调整的目的是产生一种配置，该配置在大部分样本中占据着替代算法（例如，移动窗口推荐器）的主导地位，并且具有类似（或略低）数量的超限和极限调整，以及明显更高的利用率。这种调整是迭代的和半自动的：我们对权重的可能值执行参数扫描（详尽搜索）；然后手动分析异常值（表现异常差的作业）。如果我们认为这种行为是不可接受的，则在下一次参数扫描的迭代中汇总结果时，我们会手动增加相应作业的权重。 这些离线实验使用原始的（未调整的）使用轨迹，即它们不尝试根据新设置的限制来调整信号（例如，在OOM之后，应终止任务然后重新启动）。但是，根据特定的作业，OOM或CPU节流的影响可能有所不同–对于某些作业，OOM可能会增加未来的负载（因为终止的任务的负载由其他任务接管），而对于其他任务，这可能会导致服务质量下降（当服务质量下降时，最终用户会掉下来）。实际上，这不是问题，因为使用情况调整事件很少发生，而且我们会持续监控生产中的Autopilot，在这种情况下，很容易发现过频繁的OOM等问题。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:2:3","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"3.3水平自动缩放 对于许多作业，仅垂直自动缩放是不够的：单个任务不能超过其正在运行的计算机。 为了解决这个问题，Autopilot水平缩放可根据作业的负载动态更改作业中的任务数量（副本）。 水平和垂直扩展机制相互补充：垂直自动扩展确定单个任务的最佳资源分配，而水平自动扩展随着服务的受欢迎程度和负载的变化添加或删除副本。 水平自动缩放使用以下来源之一在时刻𝑡 推导出原始建议𝑛𝑟[𝑡]： CPU使用率：作业所有者指定（1）CPU使用率信号的平均窗口（默认为5分钟）； （2）视界长度𝑇（默认视界为72小时）； （3）统计𝑆：max或𝑃95，即95％ile； （4）目标平均利用率。 Autopilot根据最新T利用率样本的值time在时间time处计算副本数number [𝑡] = 𝑆𝜏∈[𝑡−𝑇，𝑡] {𝑖𝑟𝑖[𝜏]}。 然后，原始建议的副本数为𝑛𝑟[𝑡] =𝑟𝑆[𝑡] /𝑟∗。 目标大小：作业所有者指定用于计算任务数量的函数，即𝑛𝑟[𝑡] =𝑓[𝑡]。 该功能使用来自作业监视系统的数据。 例如，使用排队系统管理请求的作业可以按请求处理时间的95％缩放； 文件系统服务器可能会根据其管理的文件空间量进行扩展。 水平自动缩放比垂直自动缩放需要更多的自定义，垂直自动缩放不需要为绝大多数作业进行配置。即使在标准的CPU利用率算法中，作业所有者也必须至少设置目标平均利用率𝑟∗（这与配置公共云中的水平自动缩放的方式类似）。在我们的基础架构中，水平自动缩放主要用于大型作业。他们的所有者通常喜欢调整自动缩放器以优化特定于作业的性能指标，例如95％的延迟（直接通过指定目标大小；或通过实验性地改变目标平均利用率并观察对指标的影响来间接地）。 然后对原始建议𝑛𝑟[𝑡]进行后处理，以生成稳定的建议𝑛𝑠[𝑡]，该建议旨在减少任务数量的突然变化。 Autopilot为工作负责人提供以下平滑策略选择（对平均工作具有合理的默认值）： 递减的缩减会从𝑇𝑑最近的建议中返回最大建议：𝑛𝑠[𝑡] =max𝑡-𝑇𝑑，𝑡{𝑛𝑟[𝑡]}。因此，缩小是推迟到用户指定的时间，而放大是立即进行的。我们的大多数工作都使用𝑇𝑑：大约40％的员工使用2天； 35％的人使用3天。 缓慢衰减可避免同时终止太多任务。如果当前任务number [𝑛]的数量超过稳定的建议𝑛𝑠[𝑡]，则某些任务将每5分钟终止一次。选择一次终止的任务数以在给定时间内将任务数减少一半（98％的作业使用默认的一小时）。 延迟较小的更改在某种程度上与缓慢衰减相反：当建议与当前任务数之间的差异较小时，它忽略更改。 限制增长使工作所有者可以对正在初始化的任务（即尚未响应运行状况检查）的部分指定限制，从而限制任务的添加速度。 4 推荐系统质量 本部分使用来自我们生产工作负载的样本，探讨了Autopilot在Google上的有效性。 我们在这里集中讨论RAM的垂直扩展，因为OOM具有直接可测量的影响。 关于CPU扩展影响的概述，请参见[31]。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:3:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.1方法论 我们的结果基于监视系统的观察结果，该监视系统使用我们的基础结构监视所有作业。大规模的运营为我们提供了对Autopilot实际收益的良好统计估计，但是任何纯粹的观察性研究的不利之处在于，它无法控制工作获得的收益（Autopilot或手动设置的限值），因此我们需要尽可能弥补这一点。 观察性研究的另一种选择是A / B实验，在该实验中，我们会将Autopilot应用于随机抽样的一组工作样本中。尽管我们对小组工作进行了A / B研究，但迁移高优先级，大型生产工作需要得到工作所有者的明确同意，因此在统计学上意义重大。 另一个选择是使用记录的跟踪进行模拟研究，但是它们有其自身的偏差，并且我们没有可靠的方法来预测实际作业将如何响应诸如CPU限制之类的模拟事件（例如，最终用户观察延迟增加可能会断开连接，降低CPU使用率，或者重新发出查询，从而增加CPU使用率）或OOM事件（例如，如果问题是暂时性的过载，则任务可能会重新启动并成功执行，或者如果是由问题引起的，则再次出错内存泄漏）。 为了减轻可观察性研究的问题，我们使用从几个不同工作人群中抽样的结果。 第一个群体是在我们的作业集合中随机选择的20000个工作的（有偏见）样本。我们从以下四个类别中分别采样了5000个作业：具有使用硬RAM限制的手动设置的限制的作业；具有使用软RAM限制的手动设置的限制的作业；使用Autopilot移动窗口推荐器的作业；以及使用Autopilot ML推荐器的作业。只要我们控制了一些潜在问题，这些群体就可以为我们提供所有服务集对Autopilot效果的衡量指标： 大多数具有手动设置的RAM限制的作业都使用硬限制，而Autopilot将其所有作业切换到软RAM限制。 此开关本身可能会减少OOM的数量。 我们通过对相同数量的具有硬RAM和软RAM限制的作业进行采样来缓解此问题。 作业可能被迫使用手动限制，因为Autopilot无法正确设置其限制。 我们通过使用第二个群体来解决此问题，该群体包括在特定日历月开始使用Autopilot的500个工作样本。 我们报告了更改前后两个日历月的性能，以减轻二进制文件或负载特性可能已更改的风险。 因为我们只能抽样成功迁移的工作，所以即使这个群体也可能有偏见，但是我们的成功率很高，因此我们认为这不是一个重大问题。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:4:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.1.1 指标 我们报告的绩效指标基于整个日历日（在5分钟的汇总窗口内（我们的监控系统的默认设置），在日历天内（与我们对资源分配的收费方式保持一致）所取的样本），并且通常使用95％百分位数来实现 利用率和OOM率之间的适当平衡。 指标如下： 在一个日历日内，一项工作的足迹是任务平均限制的总和（每个任务均以当日的运行时间加权）。占用空间直接与作业的基础结构成本相对应：任务一旦请求资源，其他高优先级任务便无法回收它们。占地面积以字节表示；但是，我们通过将原始值（以字节为单位）除以一台（大型）计算机所拥有的内存量来对其进行归一化。因此，如果一个作业占用10台计算机，则意味着为其分配的RAM数量等于10台计算机的内存（这并不意味着它在专门用于此作业的10台计算机上分配）。 在一个日历日内，某项工作的相对空闲时间是（限制减去使用量）除以限制-即请求资源中未使用的部分。在这里，使用量是一个日历日报告的所有任务所有5分钟平均使用量值的95％，而限制是该24小时内的平均限制。 日历日内工作的绝对空闲时间（以字节为单位）直接衡量浪费：这是一项工作的所有任务的极限秒数减去使用秒数的总和，除以24×3600（一天）。这种聚集将更多的重点放在更大，成本更高的工作上。此处，limit-seconds是使用5分钟的平均值在任务的运行时间内所请求的内存限制的整数。我们在对占用空间进行归一化的同时对绝对松弛进行归一化，因此，如果算法的总绝对松弛为50，则我们浪费的RAM数量等于50台计算机的数量。实现绝对值的较小值是一个雄心勃勃的目标：它要求所有任务的限制几乎始终与使用相同。 相对OOM率是一天中某项工作经历的内存不足（OOM）事件数，除以该天该工作具有的平均运行任务数。它直接关系到我们的用户需要多少工作量才能容忍Autopilot对工作造成的额外不可靠性。由于OOM很少，因此我们还跟踪根本没有OOM的工作日数。 针对工作日报告指标（即，每个工作将在一个日历月中报告30或31个这样的值），并在所有报告日中为所有工作计算统计数据（例如，相对相对松弛中位数）。 当自动尝试尝试增加作业的限制时（例如，任务变得大于可用的配额，用户指定的边界甚至一台机器的大小），自动驾驶仪可能会达到扩展限制。 我们并未筛选出此类OOM，因为目前尚不清楚该工作的未来行为，并且此类事件的影响应独立于所使用的算法。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:4:1","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.1.2作业采样和过滤 我们显示了单个日历月（或4个月，以分析迁移的工作）在基础架构上执行的工作样本的性能。 尽管我们的基础架构可以运行许多类型的作业，但其规模是由高优先级的服务性作业驱动的，因为可以保证此类作业获得其声明为极限的资源。 因此，更严格的限制直接转化为更大的容量和未来基础架构扩展速度的降低。 因此，我们的分析重点是这些工作。 除非另有说明，否则我们仅考虑长期运行的工作（至少在整个日历月内提供服务的工作），因为这些工作对我们的基础架构影响最大[26]。 我们还将使用特殊，不寻常的SLO和使用自定义推荐程序的作业筛选出一些作业类别，以使讨论重点放在默认算法的质量上。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:4:2","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.2 减少预估与真实用量差值 与非Autopilot作业相比，Autopilot作业的松弛度要低得多。图3a显示了每个工作日的空闲时间的累积分布函数（CDF）。非Autopilot作业的平均相对松弛度为60％（硬性限制）至46％（软性限制）；而Autopilot作业的平均相对松弛度为31％（移动窗口）至23％（ML）。 非Autopilot的工作浪费了大量的能力。图3b显示了样本中作业绝对松弛的累积分布函数。我们的10,000个非Autopilot工作样本的总绝对松弛总和（按月平均）等于12,000多台机器；而Autopilot作业样本的绝对松弛量少于500台计算机。差异相当于数千万美元的机器成本。 但是，这些比较可能会有偏差，因为在构建这些样本时，我们控制了每个类别的作业数量，而不是资源的总量：如果所有Autopilot作业的使用率较低，而所有非Autopilot作业的使用率较高，则可能不管每个组的限制质量如何，最终都可以节省类似的绝对松弛量（但是，相对松弛度比较仍然有效）。为了解决这个问题，我们在图3c中显示了作业足迹的CDF。该图确认了与非Autopilot作业相比，Autopilot作业的占地面积确实较小。但是，正如我们在分析迁移到Autopilot的作业时所看到的那样，这种较小的占用空间至少部分是Autopilot减少作业限制的结果。此外，默认情况下，小型作业使用Autopilot（第5节）。 最后，我们分析了最近开始使用Autopilot的工作减少的情况（图4）。在迁移之前，几乎所有作业都使用了硬盘限制。迁移之后，几乎所有人都使用ML推荐器。我们的图表显示了超过4个月的工作寿命的结果。所有作业在同一日历月开始使用Autopilot，表示为第0个月（m0）。我们显示了前两个月中这些作业的效果，当作业使用手动限制时分别表示为m-1和m-2。以及迁移后两个月内的性能，当作业使用Autopilot设置的限制时表示为m + 1和m + 2。 图4a显示了每个工作日相对松弛的CDF。迁移前一个月的平均相对松弛度为75％，中位数为84％。迁移后的一个月，平均相对松弛度下降到20％，中位数下降到17％。迁移后的两个月内，松弛值的分布保持一致，这表明收益是持久的。 绝对的空闲时间（图4b）表明可以节省大量资金：在迁移之前，这些作业浪费了相当于1870台计算机的RAM容量；迁移后，这些作业仅浪费了162台计算机：通过迁移这些作业，我们节省了1708台计算机的容量。 迁移后的工作足迹的CDF（图4c）表明，工作足迹随着时间的推移而增加，表明流量的有机增长。迁移前两个月的工作总规模小于迁移前一个月；同样，迁移后一个月的总足迹要小于迁移后两个月的足迹。 m0的迁移逆转了这一趋势：尽管足迹逐月有机增长，但m + 1的足迹明显小于m-1的足迹。迁移后，足迹的增长速度也降低了，因为m + 2的CDF比m-2的CDF接近m-1。 500个已迁移工作的足迹分布（图4c）与我们整个机队采样的2万个工作的足迹分布（图3c）不同：迁移的工作所占足迹比整个船队要大。这是因为许多小型作业会Autopilot到m0之前，而m0是我们选择作为该样本参考月份的月份（有关详细信息，请参见第5节）。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:5:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.3可靠性 上一节演示了Autopilot可以大大减少浪费的容量。 但是，将这些限制设置为0的琐碎算法将通过这些指标获得更好的结果–以频繁的OOM为代价！ 在本节中，我们展示了Autopilot作业比非Autopilot作业具有更高的可靠性。 图5显示了按工作日划分的相对OOM的累积分布函数（CDF）。 OOM很罕见：超过99.5％的Autopilot工作日没有OOM。 虽然ML推荐器产生的无OOM的工作日数比移动窗口推荐器要多一些，但它也导致相对的OOM数略多（每任务日0.013比0.002）。 两种算法显然都占主导地位的手动限制设置。 由于存在硬RAM限制，因此约98.7％的工作日不使用OOM。 相对OOM率为0.069 /任务日。 软RAM限制作业的相对OOM比率较好，为0.019，但无OOM的工作日略少（97.5％）。 OOM的数量自然取决于相对的松弛度-较高的松弛度意味着更多的可用内存，因此一项任务应该很少出现OOM。图6中的线斜率表示OOM速率与松弛的相关程度，而截距则反映了OOM的总数。具有软限制的非Autopilot作业的回归线低于具有硬限制的非Autopilot作业的回归线；在使用移动窗口算法的Autopilot作业和使用软限制的非Autopilot作业之间也有类似的严格控制。但是，ML算法的回归与使用手动指定的软限制的作业线和使用移动窗口自Autopilot方案的作业线相交，建议对于松弛率低的作业使用更多的OOM，而对于松弛度较高的作业则使用更少的OOM。 表2.按算法，受OOM严重影响的工作日数。每行对应一个不同的阈值，用于将工作日归为受OOM严重影响的类别：例如，在第一行中，如果工作任务的5或1/7（以较高者为准）会严重影响工作日以OOM终止。 “硬”和“软”都有手动限制设置。 由于ML推荐器比滑动窗口推荐器产生的平均相对OOM比率更高，因此ML推荐器可能会过于积极地降低作业限制。但是，ML推荐器旨在偶尔对不会受到太大影响的工作进行OOM。正如我们在第2节中所解释的，我们的工作旨在吸收偶发的故障-只要有足够多的尚在执行的任务能够吸收已终止任务一旦处理的流量即可。这按预期工作，并且好处是可以节省更多资源。但是，可能还是有理由担心推荐者过于激进。 为了探讨这种担忧，我们将一个工作日归类为当OOM在一天中经历的OOM数量大于其阈值数量（例如4）或分数（例如1/7）中较大者时受到OOM的严重影响。表2显示了在某些阈值设置（从更宽松（最高）到更保守（最低））中，受OOM严重影响的工作日数。尽管绝对数略有变化，但是方法的相对顺序保持不变。 在非Autopilot作业中，我们惊奇地发现具有硬RAM限制的作业虽然具有相对较高的OOM（如上所述），但受OOM的影响较小。我们假设用户可能会手动为具有不稳定的内存使用模式的作业手动指定软限制，这特别难以设置。正如预期的那样，在Autopilot作业中，尽管使用移动窗口算法的作业的相对OOM较少，但是与使用ML算法的作业相比，它们更可能受到OOM的严重影响。 我们还更详细地研究了OOM如何集中于各个工作。如果大多数OOM仅发生在少数几个工作中，则可能表明自动缩放算法存在系统性问题-我们宁愿许多工作很少遇到OOM。我们分析了当月至少有一个OOM的作业。对于每项这样的工作，我们用至少一个OOM来计算天数（我们计算OOM天数，而不是简单地计算OOM或相对OOM，而是着眼于可重复性，而不是我们上面测量的幅度）。对于Autopilot ML，此类作业中有46％仅一次进行OOM； 4天或更短时间内完成80％的工作OOM。相比之下，在Autopilot移动窗口推荐器中，只有28％的工作完全是OOM一次； 21天或更短时间内完成80％的工作OOM。 在我们的第二个样本群体中（迁移到Autopilot的工作），OOM的数量太少，无法对相对OOM和严重OOM进行有意义的估计。在迁移前的一个月中，总共有348个工作日，其中至少有一个OOM；迁移后，这个数字减少到只有48。这些工作的迁移成功。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:6:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.4限制变更次数 手动控制的工作很少会改变其限制：在我们的10,000个手动限制的工作样本中，我们观察到一个月内有334次变化，或者每个工作日大约0.001次变化。 图7显示了Autopilot更改10,000个工作样本限制的频率：每个用户的频率比用户高出几百倍。 但是，它仍然相当稳定：在大约70％的工作日中，没有任何变化。 而99％ile的工作日一天内只有6（移动窗口）到7（ML）的限制变化。 考虑到即使被逐出，找到任务的新位置通常只需要几十秒钟，那么为节省大量资金似乎是一个合理的价格。 有人可能会争辩说，上一节中报告的OOM（和严重的OOM）的减少仅仅是由于比操作员更频繁地更改限制而引起的。 在图7中，AutopilotML和移动窗口具有相似数量的极限变化。 但是，如表2所示，Autopilot ML更有效地利用了这一中断预算。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:7:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"4.5及时行为 在前面的部分中，我们专注于长期运行的工作：我们的工作至少连续工作一个月。在本部分中，我们将根据工作年龄来分析Autopilot的性能。图8a显示了每个不同年龄段的1000个工作的相对松弛的CDF。 运行少于一天的作业的松弛度要比运行时间更长的作业高得多：这是Autopilot针对长期运行的作业进行调整的直接结果–历史记录越多，松弛度越低。但是即使在14天之后，松弛度仍高于上一部分中分析的稳态。 对相对OOM速率的分析（图8b）表明，Autopilot对于短期工作非常谨慎。对于持续时间少于24小时的作业，几乎没有OOM：但是，如果我们过滤掉短期作业（总任务持续时间少于1.5小时的作业），则OOM会比稳定状态更多。一旦我们考虑了7天或更早之前开始的工作，相对OOM比率就可以与稳态行为相提并论。 5 赢得用户的信任：增加采用率的关键 我们的基础架构为成千上万具有不同角色，经验和期望的内部用户提供服务。 较小或较新的服务通常由最初创建它们的软件工程师在生产中运行，而较大的，已建立的服务则具有专门的开发人员/操作团队。 为了增加Autopilot的采用率，我们不仅必须确保算法的质量可以接受，还必须确定并满足工程师对基础架构的需求。 本节讨论这些定性方面。 我们的经验巩固了[5]中描述的许多课程 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:8:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"5.1评估过程 我们与Autopilot一起开发了评估潜在推荐者的流程。推荐人首先在脱机模拟中使用代表作业样本的资源使用情况进行评估。尽管这样的评估还远远没有完成（我们将在4.1节中详细说明问题），但足以确定是否值得在推荐者身上投入更多的精力。如果是这样，我们将继续使用空运行，其中推荐程序与其他推荐程序一起作为生产Autopilot服务的一部分运行–已记录其建议，但未执行。在两个阶段中，我们分析通常的统计汇总，例如均值和高百分位数，但也要特别注意离群值-推荐者在其中表现特别差的工作。这些异常值有助于捕获实现中的错误以及算法的意想不到的后果。 之后，我们执行A / B测试，其中新推荐程序为选定集群中的一小部分用户提高生产限制。即使在此阶段完全失败的算法也不大可能造成灾难性的灾难：如果一个群集中的一项作业失败，该服务的负载均衡器会将其流量切换到其他群集，这些群集应具有足够的能力来应对浪涌。 最后，当新推荐人在A / B测试中获得好评时，我们逐渐将其推荐为整个车队的新标准。为了降低可能发生故障的风险，分批进行分批部署，它们之间有多个小时的间隔，如果发现异常，则可以回滚。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:9:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"5.2作业所有者可以轻松访问Autopilot限制 我们用于资源监视的标准仪表板（图9）显示了作业CPU和内存使用情况的分布以及Autopilot计算的限制-即使对于未Autopilot的作业（对于这些作业，Autopilot在模拟模式下运行）。 仪表板可帮助用户了解Autopilot的操作，并建立对Autopilot在Autopilot上启用的功能的信任：用户可以看到Autopilot将如何响应每日和每周的周期，新版本的二进制文件或突然变化的负载 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:10:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"5.3自动迁移 一旦我们对大型离线模拟研究和较小规模的A / B实验对Autopilot的操作有了足够的信任，就可以将其作为所有现有小型作业的默认设置（总计最多限制10台机器），并且 所有新工作。 提前通知了用户，他们可以选择退出。 这种自动迁移几乎没有用户反冲，从而大大提高了采用率。 (a)Autopilot作业的痕迹。 8:00之后，一些任务开始显示出更高的内存使用量； Autopilot迅速提高了工作限制。 (b)使用手动指定的限制以及在模拟模式下运行的Autopilot来跟踪作业。 新推出的产品存在内存泄漏，导致内存使用量稳定增加。 在工作开始到OOM时，工作的监视系统在17:30左右开始寻呼呼叫。 直到19:30，值班开发人员才设法提高工作的内存限制（18:20到19:00之间的限制振荡是来自监视系统的时间序列数据未对齐的产物） 。 如果该作业是自动执行的，则可能不会进行OOM设置，因为自动增加的限制将使dev / ops有更多时间发现内存泄漏并回滚到二进制的先前版本。 ","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:11:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Coding"],"content":"5.4用自定义推荐器覆盖推荐器 Autopilot的算法依靠历史CPU /内存使用情况来设置将来的限制或任务计数。但是，对于某些作业，其他信号可以更好地预测极限：例如，我们的文件系统服务器的负载几乎线性地取决于服务器控制的文件的总大小。此外，一些长期运行的服务在Autopilot之前已经开发了自己的水平自动缩放器，其中一些包含经过细化的，经过微调的逻辑，这些逻辑已经经过了多年的完善（尽管它们通常仅具有Autopilot的一部分功能，而它们并没有始终跟上Borg的变化）。 Autopilot的自定义推荐程序使用户可以保留此类算法的关键部分（计算任务数量或单个任务资源限制），同时将支持功能（如致动）委派给Autopilot生态系统。 定制推荐器很受欢迎：在提供三个月后，定制推荐器管理着我们全部机队资源的13％。 6 减少工程量 Google遵循减少劳动量的dev / ops原则：繁琐，重复的工作应该由机器而不是工程师来完成，因此我们在自动化上进行了投资。Autopilot就是一个例子。 随着工作量的增加，需要增加工作的限制。受欢迎的服务，即使不包括最初的快速增长阶段，也可能应该每两周或每月调整一次。而且每次推出新的二进制版本都可能需要调整限制。假设这些是手动完成的。我们假设手动调整大小平均需要30分钟的工作：更改配置文件，将更改提交到版本控制系统，查看建议的更改，初始化发布并监视其结果。对于我们的1万个具有手动限制的工作的样本，甚至334个手动限制的调整也代表了大约一个人月的工作量，并且这大大少于预期的更新次数。 Autopilot的水平缩放-向正在运行的作业中添加任务-自动处理自然的负载增长。Autopilot的垂直缩放可以处理每个任务的负载变化，也可以处理推出新二进制文件带来的影响。两者都代表着显着的人工减少。 我们询问了几位大型工作的老板，这些老板迁移到了Autopilot，以估计他们所经历的辛苦工作减少了。一项大型服务（由多个作业组成）报告，在迁移到Autopilot之前，他们每月执行大约8次手动调整大小。 另一项服务估计，Autopilot每月可以为他们节省2个小时的人工调整之前的人工工作。另一个服务的负载在群集之间和时间上变化很大，每月需要大约12次手动调整大小。另一个好处是减少了必须由待命的dev / ops工程师处理的中断（页面）。随着可靠性的提高，任务失败的频率降低，报告系统发出的警报也更少。对于集群中负载差异很大的作业，这种减少尤为明显：设置不同的手动限制的工作是经常出现的问题根源，甚至使监视变得复杂。一项服务报告说，迁移后，Autopilot增加了某些群集中的内存限制，这导致OOM的数量从每天大约2000个减少到可以忽略的数量。迁移后的将近一年中，另一个服务没有报告OOM。通话页面的数量从每周3个减少到少于1个（其余页面用于无关的问题）。 Autopilot的资源限制越来越严格，可能会暴露出一些漏洞，而这些漏洞却由于较大的限制而未被发现。众所周知，罕见的内存泄漏或越界访问是很难找到的。尽管Autopilot仪在大多数情况下都能很好地工作，但它可能仍需要针对一些工作进行自定义配置。因此，当作业迁移到Autopilot然后频繁开始进行OOM时，可能很难区分Autopilot配置错误和真正的错误。一个小组将这种问题归咎于Autopilot的内存限制设置算法，并在几周后才发现根本原因：很少触发的越界内存写操作。 最后，批处理作业会大量使用Autopilot（CPU启用了88％的此类作业）。我们推测这是因为Autopilot使用户甚至不必为此类工作指定限制 7 相关工作 尽管Autopilot执行，UX和某些自定义特定于Borg，但Autopilot解决的问题几乎在云资源管理中普遍存在。 预期的副本数及其资源需求将由许多云资源管理系统中的用户提供，因为调度程序使用它们将任务打包到机器上[14]。 Borg [34]，Omega [29]和Kubernetes [3]都要求用户在提交作业（Borg，Omega）或吊舱（Kubernetes）时设置此类限制。 YARN [32]要求应用程序（作业）声明容器（任务）的数量以及每个容器的CPU和RAM资源需求。在有些不同的情况下，用于HPC系统的调度程序（例如Slurm [37]）需要每个批处理作业来指定计算机的数量。 其他研究证实，当手动设置作业限制时，我们在基础架构中观察到的私有云利用率较低。 [30]分析了阿里巴巴的一万台机器的YARN集群的5天使用情况，并报告说80％的时间中RAM利用率低于55％。 [23]分析了一个短时间（12小时）的阿里巴巴跟踪，表明对于几乎所有实例（任务），峰值内存利用率为80％或更少。 [26]分析了30天的Google集群跟踪[27]，结果表明，尽管平均请求内存几乎等于总可用内存，但实际使用量（超过一小时窗口的平均值）低于50％。 设置更精确的限制的一种替代方法是超额订阅资源，即有意分配给机器任务，以使它们的要求总和高于本地可用的物理资源量。 [30]显示了一个系统过度订阅YARN群集中的资源。尽管超额预订可用于可以承受偶尔的速度下降的批处理工作负载，但它可能导致服务工作负载的尾部等待时间显着增加-这需要仔细的概率处理[2，21]。 水平和垂直自动缩放需要作业具有弹性。通常，众所周知，许多类别的应用程序都很难扩展。例如，JVM在其默认配置下不愿意释放堆内存。幸运的是，对于Autopilot而言，Google的绝大多数工作都是在考虑扩展的基础上进行的。 自动缩放是一个发达的研究领域。最近的调查包括[11，16，22]。大多数研究涉及水平自动缩放。 [17]通过实验分析了一些针对工作流的水平自动缩放算法的性能。 [10]建立了AWS和Azure中水平自动缩放器的概率性能模型。 [24]测量了AWS，Azure和GCE中水平自动缩放器的性能。虽然Autopilot还具有反应式水平自动缩放器，但本文主要集中于垂直缩放（在[16]中也称为调整大小或VM适应）。 Kubernetes垂直吊舱自动缩放器（VPA，[15]）使用移动窗口上的统计信息来设置容器的限制（例如，对于RAM，24小时内的第99个百分点）。 Kubernetes的方法直接受到我们移动窗口推荐器的启发（第3.2.2节）。 [25]提出了一种估计器，该估计器使用窗口中样本的中值和标准差之和。 我们描述了两个推荐器：一个基于从移动窗口计算出的统计信息，其中包含窗口参数（例如，长度）由工作所有者设置（第3.2.2节）；另一个根据成本函数自动选择移动窗口参数（第3.2.3节）。这些简单的统计数据的替代方法是使用更高级的时间序列预测方法，例如自回归移动平均（ARMA）（如[28]中也使用工作绩效模型），神经网络（如[18]中）， [9，20]中的递归神经网络或[13]中的自定义预测表明基于马尔可夫链的预测比基于自回归或自相关的方法表现更好。我们使用此类方法进行的初步实验表明，在绝大多数博格案例中，不需要ARMA的其他复杂性：作业倾向于使用较长的窗口（例如，移动窗口推荐器中内存的默认半衰期为48小时，第3.2.2节）；并且日间趋势足够小，以至于一个简单的移动窗口能够迅速做出反应。对于可以由用户配置的推荐器，我们相信参数具有简单的语义并且可以可预测地调整推荐器更为重要。 Autopilot不建立工作绩效模型：它不会尝试优化批处理工作的完成时间或服务于工作的最终用户响应延迟。我们发现，控制限制使工作所有者可以推理工作绩效（和性能问题），并在工作与基础架构之间进行区分。关注点的分离部分取决于群集和节点级调度程序。例如，Autopilot不需要考虑所谓的嘈杂邻居的性能问题，因为Borg通过特殊机制对其进行处理[38]。为了涵盖其余的一些特殊情况，Autopilot提供了水平缩放挂钩，以允许团队使用自定义指标甚至自定义推荐程序。相比之下，许多研究旨在直接优化工作的绩效指标，而不仅仅是分配的资源量。例如，在Quasar [7]中，用户指定性能限制，而不是限制，而调度程序负责满足这些限制。在公共云中，在其中将作业严格分配给VM，Paris [36]建议在具有代表性任务及其性能指标的情况下配置VM。 D2C [12]是一种水平自动缩放器，它通过学习每一层的性能参数（使用排队论建模）来缩放多层应用程序每一层中的副本数量。学习过程是在线的–不必预先对应用程序进行基准测试。 如果作业类别更窄，则可以进行更具体的优化。 Ernest [33]专注于批处理，机器学习工作，而CherryPick [1]也考虑分析工作。尽管本文着重于服务工作，但Google的88％批处理工作也使用了Autopilot（以CPU消耗量衡量）。 [19]使用强化学习（RL）来驱动服务工作的水平扩展（具有效用函数，考虑了副本的数量，吞吐量和任何违反SLO的响应时间）。 Autopilot的ML推荐器借鉴了RL的一些想法，例如根据模型的历史表现选择模型和极限值。 8 结论 弹性伸缩对于云来说效率、可靠性、运维工作量至关重要。手动设置的限制不仅浪费资源（平均配置限制太高），而且随着负载增加或新版本的服务推出时，会导致频繁违反限制。 自动扩缩对于云来说效率、可靠性、运维工作量至关重要。手动设置的限制不仅浪费资源（平均配置限制太高），而且随着负载增加或新版本的服务推出时，会导致频繁违反限制。Autopilot是Google使用的垂直和水平自动缩放工具。通过自动提高限制设置的精度，它减少了资源浪费并提高了可靠性：内存不足错误不那么常见，不那么严重，并且减少任务间项目影响。使用简单的","date":"2021-01-10","objectID":"/zh-cn/google-autopilot-scaling/:12:0","tags":["Kubernetes"],"title":"译《Autopilot: workload autoscaling at Google》","uri":"/zh-cn/google-autopilot-scaling/"},{"categories":["Product"],"content":"枚举目前我迁移工作电脑需要装的 app 清单。后续考虑加到同步工具里。 ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:0:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"开发 Vscode Sublime Sourcetree Github Desktop Iterm2 Postman Cyberduck Docker ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:1:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"命令行 oh-my-zsh homebrew homebrew cask tmux ack ag bat autojump job fselect ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:2:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"画图 draw.io desktop XMind Sketch ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:3:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"数据库 Sequel Pro (for MySql) Navicat (for PostgreSql) Redis Manager ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:4:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"IM Wechat Qiye Wechat Slack ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:5:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"浏览器 Chrome ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:6:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"笔记 Typora Notion ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:7:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"RSS 订阅 Leaf ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:8:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"壁纸 Iruve ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:9:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"系统 CleanMyMac Dozer ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:10:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"密码管理 1Password ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:11:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"音乐 NetEase Music Spotify ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:12:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"视频 IINA ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:13:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"Util Tool Paste Yoink Beyond Compare Handshake Quicklook ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:14:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Product"],"content":"GTD Fantastical 2  ","date":"2020-12-25","objectID":"/zh-cn/apps-liat-for-macbook/:15:0","tags":["Productivity","Awesome"],"title":"一名开发的 Macbook 常用 app 整理","uri":"/zh-cn/apps-liat-for-macbook/"},{"categories":["Coding"],"content":"背景 最近需要给 k8s 集群升级 docker, 预期升到 19.03.x. 遇到一些问题, 记录下 ","date":"2020-07-10","objectID":"/zh-cn/upgrade-dockerd-in-k8s/:1:0","tags":["Kubernetes","Docker"],"title":"升级 k8s 集群 docker","uri":"/zh-cn/upgrade-dockerd-in-k8s/"},{"categories":["Coding"],"content":"调试期间遇到的问题: 集群中的 ingress 是以 daemonsets 方式部署, 通过 node-selector 选择节点定死. 而 kubectl drain node 并不 evict daemonset pods. 故在升级/重启 dockerd 期间会造成 ingress 短暂不可用. 而现有的 lb 的 health check 不能 cover 这一点, 仍会有流量打入.会导致升级期间 ingress 流量黑洞问题. 并且 dockerd 拉起来后, 有些 daemonsets 由于 ingress 自身 livenessProbe 等原因在 dockerd 升级期间持续 crashLoopbackoff 了, 一个原因是仍然 mount 一份旧的 docker overlay. 在 delete pod 重启后恢复. 故需要一个手段在升级后重启 ingress pods. Ref https://github.com/kubernetes/kubernetes/issues/75482#issuecomment-511476698 但目前我没找到一个优雅的方式 restart all daemonsets pods on node, 只能通过脚本逐个杀 ","date":"2020-07-10","objectID":"/zh-cn/upgrade-dockerd-in-k8s/:2:0","tags":["Kubernetes","Docker"],"title":"升级 k8s 集群 docker","uri":"/zh-cn/upgrade-dockerd-in-k8s/"},{"categories":["Coding"],"content":"ingress qcloud lb health check 问题 由于目前我们 ingress 外部还接了 qcloud lb 做一层负载均衡, 从而引入了新问题. 遇到了没有重启 kubeproxy/calico 时网络问题(具体原因未知)导致 lb -\u003e ingress 异常, 从而导致 http 504 结论: 需要对所有 pod 进行重启最保险. 重启命令: kubectl get pods --all-namespaces -owide|awk -v host=$HOSTNAME '{if ($8 == host) system (\"kubectl -n \" $1 \" delete pods \" $2 \" --grace-period=0 \" \" --force \")}' ","date":"2020-07-10","objectID":"/zh-cn/upgrade-dockerd-in-k8s/:2:1","tags":["Kubernetes","Docker"],"title":"升级 k8s 集群 docker","uri":"/zh-cn/upgrade-dockerd-in-k8s/"},{"categories":["Coding"],"content":"背景 最近在豆瓣做面向开发者的 op 报警系统, 最头疼的是下列几种情况 脉冲型报警 单个原因引起的报警发散 多个报警为同个诱因 由于目前现有的一些告警系统都强依赖后面的监控 agent/cmdb , 比如 open-falcon/prometheus 虽然都支持了告警收敛这一套, 但要从豆瓣的 statsd+icinga 迁移到 alertManager 需要折腾一阵。而我实际只想要一个中间轻量的 alert exporter 去做告警聚合/收敛. 所以我期望实现下列 feature: 对于告警可配置梯度, 比如 apperr \u003e 50, 则立马报警不做 retry, 并且 notify_interval 为 1 分钟; 对于 50 \u003e apperr \u003e 20 则有3次 retry, retry 周期为 1 分钟, 报警周期为2分钟. 对于 apperr \u003c 1 则 retry 5 次, retry 周期 3 分钟, 报警周期 5 分钟. 告警配置中可用 与|或|非 的方式进行组合告警,告警收敛. 如 [rules] apperr - script: xxx - label: http, service, statsd, codeerr - warning_threshold: 0.5 - critical_threshold: 1.0 http500: - script: xxx - label: http, codeerr, prom prom-server-down: - script: xxx - label: prom, data-plane ... apperr | http500 表示两者其一报警了, 则 mute 另一个报警. apperr \u0026\u0026 http500 表示当两种同时发生时才告警. apperr \u0026\u0026 !http500 表示 apperr 发生但 http500 不发生时告警, 若 apperr, http500 同时发生, 则不触发该条报警规则收敛掉 apperr 报警, 只报出 http500 告警按标签聚合 对于历史数据的分析预测 ","date":"2020-07-07","objectID":"/zh-cn/alert-convergence/:1:0","tags":["Architecture","Alert"],"title":"告警收敛设计","uri":"/zh-cn/alert-convergence/"},{"categories":["Coding"],"content":"数据源 目前我们的告警数据源大致可分为 prom, statsd 以及其他(比如 mfs http api, dae-monitor-agent 抓的 cgroups 数据, shuai 数据…), 但大头基本是 prom. ","date":"2020-07-07","objectID":"/zh-cn/alert-convergence/:1:1","tags":["Architecture","Alert"],"title":"告警收敛设计","uri":"/zh-cn/alert-convergence/"},{"categories":["Coding"],"content":"告警流程 目前应该告警的流程如下: dae ossetup -\u003e bridge api 获取告警 meta data dae ossetup 将 meta data render icinga template -\u003e http 请求发给 monitor app -\u003e 刷 icinga-server 配置 icinga-server 根据配置中的 cmd 执行命令如 /usr/bin/dae-app-nagios-monitor dae-apperr $args1 $args2 ... -\u003e dae-sa-tools dae-apperr 告警 -\u003e bridge 告警元数据 api -\u003e prometheus/statsd 的 data-storage 层 bridge api 返回对应的指标, 由 dae-sa-tools 判断阈值后返回 icinga 识别的 warning/critical/unknown 等报警状态码 icinga-server 根据该状态码进行报警 而这中间是 icinaga 直接拿着 return code 进行告警控制, 故打算实现一个 alert-exporter 来做告警收敛, 预期的第三步骤流程变为 alert-exporter 按照规则定期抓取数据源指标(如 bridge api, prometheus, statsd 等)放 redis icinga 执行 cmd -\u003e alert-exporter -\u003e 按配置的收敛规则从 redis 数据进行分析是否告警 ","date":"2020-07-07","objectID":"/zh-cn/alert-convergence/:1:2","tags":["Architecture","Alert"],"title":"告警收敛设计","uri":"/zh-cn/alert-convergence/"},{"categories":["Coding"],"content":"思路 基于这篇论文《运维监控系统告警收敛的算法研究与应用》的思路, 有了以下想法 alert-export 需要记什么： 告警 name 告警 meta data(配置项) 告警 value 告警 timestamp 告警 recover time ~告警的检查周期和提醒周期应该是指数周期降频收敛的算法, 即以 2^n 递增报警周期~ 告警分组, 做相同标签收敛 对于频繁自愈的报警打标签降频 对于历史告警数据可根据组合报警预测更大范围的告警, 和预测误报 ","date":"2020-07-07","objectID":"/zh-cn/alert-convergence/:1:3","tags":["Architecture","Alert"],"title":"告警收敛设计","uri":"/zh-cn/alert-convergence/"},{"categories":["Coding"],"content":"(译) eBPF Tracing 简明教程与示例 ","date":"2020-01-13","objectID":"/zh-cn/ebpf-leanring1/:1:0","tags":["Linux","eBPF"],"title":"eBPF learning 01","uri":"/zh-cn/ebpf-leanring1/"},{"categories":["Coding"],"content":"背景 原文链接 http://www.brendangregg.com/blog/2019-01-01/learn-ebpf-tracing.html 原文作者 Brendan Gregg 出版时间 01 Jan 2019 翻译时间 11 Jan 2020 之前开 2019 ShangHai KubeConf 听了几场 eBPF 的分享, 最近才开始深入研究, 故打算把研究 Linux performance 的大佬 Brendan Gregg 的文章《Learn eBPF Tracing: Tutorial and Examples》 作为 eBPF 入门翻译一遍, 希望对其他非英语母语的开发者有帮助。 ","date":"2020-01-13","objectID":"/zh-cn/ebpf-leanring1/:1:1","tags":["Linux","eBPF"],"title":"eBPF learning 01","uri":"/zh-cn/ebpf-leanring1/"},{"categories":["Coding"],"content":"译文 在 2019 年的 Linux Plumber’s 大会上至少有 24 场关于 eBPF 的讲座, eBPF 迅速地成为了炙手可热的技术。所以也许你也计划在新的一年里开始学习 eBPF! 如今 eBPF 在一些主流技术中都有其身影, 如虚拟内核指令集(VKIS), 由于其继承自大名鼎鼎的 Berkeley Packet Filter (BPF) , 所以它有较多的用途，如: 网络性能分析, 防火墙, 安全追踪, 设备驱动等。这些基于 eBPF 的比如一些性能追踪的产品， 在网上也不乏教程好文档。说到「追踪」，我们常认为是一些性能分析和一些提供更多观察点的工具。你可能也已经用过其中的一些工具，如 tcpdump, strace 和其他的一些 tracer 在这篇文章里我会介绍 eBPF 追踪, 并针对初学者, 稍有经验的人以及一些老手在内容上做些简单划分便于阅读。总的来说分为以下内容: 初学者: 运行 bcc 工具 稍有经验: 开发 bpftrace 工具 老手: 开发 bcc 工具, 为 bcc\u0026bpftrace 提 PR ","date":"2020-01-13","objectID":"/zh-cn/ebpf-leanring1/:1:2","tags":["Linux","eBPF"],"title":"eBPF learning 01","uri":"/zh-cn/ebpf-leanring1/"},{"categories":["Coding"],"content":"初学者 1. 什么是 eBPF, bcc, bpftrace 和 iovisor? eBPF 对于 Linux 而言相当于 JavaScript 对于 HTML 的作用, 即相比于一个静态的 HTML 网站, JavaScript 能声明一些小的程序用于监听一些运行在浏览器虚拟内核上的一些用户事件如鼠标的点击等。所以有了 eBPF, 相比于一个固定的黑盒的 LInux 内核, 你也可以在用户态编写一些简单的脚本来获取一些内核事件如磁盘 I/O 等, 同样的, 这些事件也是运行在 Linux 安全内核虚拟机里。事实上呢, eBPF 比纯 JavaScript 更高级些, 它更相当于一个运行着 JavaScript 的 v8 虚拟机，且 eBPF 如今也已是 Linux 内核的一部分。 直接在 eBPF 中编写代码是非常难得, 就像你要直接写些 v8 虚拟机二进制代码一样。实际上也没啥人直接写 v8 代码, 大家都是直接写更友好的 JavaScript 或者一个基于 JavaScript 的上层框架(jQuery, Angular, React 等)。 这件事在 eBPF 也一样, 人们不会直接写内核代码, 而是通过 eBPF 框架编写一些上层代码。拿性能追踪来说，主要用的是 bcc 和 bpftrace 。这两个工具并不在 内核代码中, 它们独立于 GitHub 上另一个 Linux 基金会的项目叫 iovisor 2. eBPF 性能追踪的简单例子 先介绍一个基于 eBPF 的工具, tcplife tcplife 可以展示出完整生命周期的 TCP sessions, 并且可以看到进程 id (PID), 进程命令名 (COMM), 发送和接受的字节数以及 TCP 连接的持续时间 (单位是 ms), 如图: 需要注意的是 eBPF 并没有开放给用户可以通过旧的内核技术来重写 TCP 生命周期的入口，不过假如哪天开放了, 这样的入口也会带来一些性能上的额外开销,一些内核安全问题等等,所以我们永远不会使用这样的工具。eBPF 给工具带来的特性应该是可实践的, 便捷的，安全的。拿 tcplife 举个例子, 它并不会追踪每一个包, 这样会带来很多额外的开销，相反的，eBPF 只会追踪 TCP 生命周期的事件, 这些时间并不会这么频繁, 从而给系统带来的开销会低得多, 我们也可以从容地让这个工具 7x24 小时地在生产环境运行。 3. 怎么使用 eBPF 呢? 对于新手来说, 可以说是从 bcc 开始使用, 具体教程可以参照这篇文章 在 Ubuntu 系统上, 可以简单地这么安装 我呢是通过运行 opensnoop 来测试 bcc 是否 work 的, 所以如果你也这么搞过, 那么恭喜你, 你也用过 eBPF 啦！ 如今一些公司如 Netflix 和 Facebook 已经默认在所有服务器上安装了 bcc 所以也许你也可以开始这么搞。 4. 新手教程 如果你想直接学习 bcc 那么也可以看看这篇教程, 这是一个对于新手来学习 eBPF 性能追踪极好的出发点。 作为新手呢, 你不需要一开始就写些 eBPF 代码。bcc 有接近 70 多个衍生工具你都可以直接使用。这篇教程也会带你逐步过一遍以下 11 个趴(part): execsnoop opensnoop ext4slower (or btrfs*, xfs*, zfs*) biolatency biosnoop cachestat tcpconnect tcpaccept tcpretrans runqlat profile 一旦上述内容你都尝试过后, 你可能就会有了一个全局性的概念: 网上已经有过很多较为完整的 man pages 和示例来介绍这十一个趴(part)了。在官方 repo (bcc/tools) 中的示例文件 *_example.txt 展示了上述概念的具体释义，比如 biolatency_example.txt. 中你可以了解到如何用 bcc 来追踪 block I/O 时延, repo 中也有很多我写的示例文件, man pages 和具体工具，满打满算估摸着有 50 多篇 blog 了吧 不过目前还欠缺的是在生产环境中的一些示例。因为这些 blog 绝大多数是我在 eBPF 刚诞生不久，我们并没在生产环境中运行而只在测试实例上跑，所以大部分示例都是在一定场景下构造的。不久后也许我们会再新加些生产环境的示例，而这部分也需要大家的帮助，比如当你使用一个 eBPF 工具解决了某些生产环境问题时, 可以考虑留些现场和截图，并且提些 PR 加到示例文件中。 ###稍有经验 这一章节的假设读者应该都已经使用过 bcc 及其他各种相关工具了, 如果你对二次开发 eBPF 工具感兴趣的话, 那么最佳的入手点就在于先尝试从 bcc 切换到 bpftrace , 这是一个更高级更易入门的语言。但 bpftrace 也有不足之处, 它不像 bcc 那样支持自定化, 所以大概率你可能最终使用完仍然会切换回 bcc 想要深入了解 bpftrace 的话可以参考这篇文章 , 这是一个相对较新的项目, 所以在我写这篇文章时可能这个工具并没有被成熟的引入各个 Linux 包管理系统，不过在未来，应该可能通过类似 apt-get install bpftrace 或类似的方式来安装, 目前只能通过编译源码包安装。 1. bpftrace 教程 我也写了另一个教程会教你如何通过单行命令玩转 bpftrace bpftrace One-Liners Tutorial 这个教程分为 12 章节来逐步叫你学习 bpftrace ， 一个简单示例如下: 这句命令会找出所有打开系统调用追踪点的 PID 和文件路径 2. bpftrace 参考指南 想了解更多有关 bpftrace 的话，可以参考我的这篇文章 bpftrace Reference Guide 你可以从中了解到有关 bpftrace 语法，探针和一些内置组件。 3. bpftrace 示例 bpftrace repo 中有近 20 个工具，你可以简单参考 bpftrace Tools 比如其中用于追踪 block I/O 时延的脚本 如同 bcc, 这些工具也有详尽的 man pages 和示例, 比如 biolatency_example.txt ","date":"2020-01-13","objectID":"/zh-cn/ebpf-leanring1/:1:3","tags":["Linux","eBPF"],"title":"eBPF learning 01","uri":"/zh-cn/ebpf-leanring1/"},{"categories":["Coding"],"content":"老鸟 1. 开发 bcc 这儿是我整理的一些关于开发及二次开发 bcc 的相关文档: bcc Python Developer Tutorial bcc Reference Guide 在 bcc/tools/ 目录下有很多 *.py 文件可供学习, 这些 py 文件大约可以分成两部分: BPF 内核 C 代码的 Python 实现(或 lua, C++ 实现) Python 的一些 BPF 用户态工具 开发 bcc 工具确实是需要成熟的技术的，因为这可能会涉及到一些细碎的内核问题或应用底层问题 2. 提贡献 我们非常欢迎以下贡献 提交 bcc issues 提交 bpftrace issues 对于 bpftrace, 有个 bpftrace Internals Development Guide 教程, 如果你正在写些 llvm IR 的代码, 那么这部分的开发对你是个挺大的挑战…… 当然, 提贡献这部分也可以涉及内核 eBPF (aka BPF) 的引擎。 如果你浏览过 bcc 和 bpftrace 的 issues 的话, 你会看到很多优化相关的请求，诸如 bpftrace kernel tag 等等。此外，持续关注 netdev 邮件列表来料及到最新的内核 BPF 开发也是需要做的，这部分通常有些会在下一个 release 中 merge 到 Linux 主分支上。 此外，除了编写代码，你也可以写些测试，包构建，博客或者演讲分享来帮助我们。 ","date":"2020-01-13","objectID":"/zh-cn/ebpf-leanring1/:1:4","tags":["Linux","eBPF"],"title":"eBPF learning 01","uri":"/zh-cn/ebpf-leanring1/"},{"categories":["Coding"],"content":"总结 eBPF 确实做了很多事情。在这篇文章里我只是简单介绍了如何学习 eBPF 性能分析和追踪，总的来说是以下内容 初学者: 把 bcc 工具跑起来 稍有经验: 开发 bpftrace 工具 老鸟: 开发 bcc 工具, 对 bcc \u0026 bpftrace 做贡献 同时在这个页面 eBPF Tracing Tools 有这篇文章更详细的内容, 其中涉及细节较多, 感兴趣的话可以仔细阅读。 祝你好运！ ","date":"2020-01-13","objectID":"/zh-cn/ebpf-leanring1/:1:5","tags":["Linux","eBPF"],"title":"eBPF learning 01","uri":"/zh-cn/ebpf-leanring1/"},{"categories":["Gossip"],"content":"这些日子隐隐觉得你并不开心, 我十分自责, 总觉得是那天妖精们藏起了星星。 北京的天总是阴沉沉的，有时候好像是要开启一场庭审，让我紧张得四肢发麻，不知道你会不会也因此闷闷不乐。这样的日子，可真叫人腻味，我沮丧得想睡觉。唯独那天看着你把脸藏在被子里沉沉地入睡，我欢喜得不行，这一天何其矜贵，希望你做个好梦。 所以我特别期望能有个间隙去看看被藏起来的东西，看星星也是我生活中为数不多的理想主义。 那时你说我喋喋不休，要知道我是个时刻沸腾的人，能滋滋作响一整天，总有一些莫名其妙的巨大能量，但我也要悄悄地藏起来，可要把我所有的能量分成一天天的给你。这样我就能沉静许多，不过其实一见到你，那些跳脱的念头总能平静下来。 就像一只海豚，终于跳进了阿芙罗狄忒的海湾，有海鸥有落日，不免沉浸其中。 这样的日子，我也想藏起来，担心有一天我不会记得，只期望有那么几个瞬间，偶然念及你的名字，心底像冒出海葵与青荇，世界变量明亮开朗起来。 所以，我还想看着你笑，看着你看电影时甜甜地睡着，你说，我哪能让你一个人生闷气呢。 你说未来会有多少无生趣的毫不起眼的夜晚，我就跋涉到灯塔上，威胁妖精们把藏起来的光，投射下来。 加油啊，阿芙罗狄忒。 ","date":"2020-01-09","objectID":"/zh-cn/dont-cry-aphrodite/:0:0","tags":["Gossip"],"title":"阿芙罗狄忒你不要哭泣","uri":"/zh-cn/dont-cry-aphrodite/"},{"categories":["Coding"],"content":"背景 之前一段时间正好接触到 kubernetes cronjob, 在接入时遇上了在一定量级下 cronjob schedule delay 的问题, 故开始读了下代码, 发现了一些问题并试着调优了下 ","date":"2019-12-14","objectID":"/zh-cn/talk-about-k8s-cronjob/:0:1","tags":["Golang","Kubernetes","Linux"],"title":"Talk about Kubernetes cronJob controller","uri":"/zh-cn/talk-about-k8s-cronjob/"},{"categories":["Coding"],"content":"存在的问题 按生产环境实际测试来看约 250-375 个 */1 * * * * 每分钟 interval 的 cronjob 就会产生 delay, cronjob 和 controller manager 没有异常 event 但新产生的 job 出现了延迟, 由于我们设置了 startingDeadlineSeconds 故累加起来的 delay 最终导致了 cron 任务严重滞后 ","date":"2019-12-14","objectID":"/zh-cn/talk-about-k8s-cronjob/:0:2","tags":["Golang","Kubernetes","Linux"],"title":"Talk about Kubernetes cronJob controller","uri":"/zh-cn/talk-about-k8s-cronjob/"},{"categories":["Coding"],"content":"代码解读 出于分析上述问题的目的, 读了下 cronjob controller 的代码, 代码量不多, 可能由于没上 GA 的原因, 整个 controllor 代码的设计也比较过程式, 不会像其他组件用到一些比如 Informer, refractor之类的组件读起来相对晦涩 下面开始解读下 release1.17 分支的 k8s cronjob controller 代码 Controller struct type Controller struct { kubeClient clientset.Interface jobControl jobControlInterface sjControl sjControlInterface podControl podControlInterface recorder record.EventRecorder } cronjob controller 结构体, 即下文中常见的 jm(jobManager) , 主要包了 k8s internal api clinet kubeclinet, jobControl 和 sjControl k8s job 控制块，cronjob controller 会直接操作 job, 由 job 再去创建 pod, 并不会直接接触到 pod 对象(包括读) 入口函数 Run: // Run starts the main goroutine responsible for watching and syncing jobs. func (jm *Controller) Run(stopCh \u003c-chan struct{}) { defer utilruntime.HandleCrash() klog.Infof(\"Starting CronJob Manager\") // Check things every 10 second. go wait.Until(jm.syncAll, 10*time.Second, stopCh) \u003c-stopCh klog.Infof(\"Shutting down CronJob Manager\") } cronjob controller 是个单线程单执行流的调度器, 由固定每 10s 的 interval 的 goroutine 做一次 syncAll 调用 主 loop 函数 syncAll // syncAll lists all the CronJobs and Jobs and reconciles them. func (jm *Controller) syncAll() { // List children (Jobs) before parents (CronJob). // This guarantees that if we see any Job that got orphaned by the GC orphan finalizer, // we must also see that the parent CronJob has non-nil DeletionTimestamp (see #42639). // Note that this only works because we are NOT using any caches here. jobListFunc := func(opts metav1.ListOptions) (runtime.Object, error) { return jm.kubeClient.BatchV1().Jobs(metav1.NamespaceAll).List(opts) } js := make([]batchv1.Job, 0) err := pager.New(pager.SimplePageFunc(jobListFunc)).EachListItem(context.Background(), metav1.ListOptions{}, func(object runtime.Object) error { jobTmp, ok := object.(*batchv1.Job) if !ok { return fmt.Errorf(\"expected type *batchv1.Job, got type %T\", jobTmp) } js = append(js, *jobTmp) return nil }) if err != nil { utilruntime.HandleError(fmt.Errorf(\"Failed to extract job list: %v\", err)) return } klog.V(4).Infof(\"Found %d jobs\", len(js)) cronJobListFunc := func(opts metav1.ListOptions) (runtime.Object, error) { return jm.kubeClient.BatchV1beta1().CronJobs(metav1.NamespaceAll).List(opts) } jobsBySj := groupJobsByParent(js) klog.V(4).Infof(\"Found %d groups\", len(jobsBySj)) err = pager.New(pager.SimplePageFunc(cronJobListFunc)).EachListItem(context.Background(), metav1.ListOptions{}, func(object runtime.Object) error { sj, ok := object.(*batchv1beta1.CronJob) if !ok { return fmt.Errorf(\"expected type *batchv1beta1.CronJob, got type %T\", sj) } syncOne(sj, jobsBySj[sj.UID], time.Now(), jm.jobControl, jm.sjControl, jm.recorder) cleanupFinishedJobs(sj, jobsBySj[sj.UID], jm.jobControl, jm.sjControl, jm.recorder) return nil }) if err != nil { utilruntime.HandleError(fmt.Errorf(\"Failed to extract cronJobs list: %v\", err)) return } } 首先 pager.New(pager.SimplePageFunc(jobListFunc))通过 Pager 调用了 jobListFunc 回调函数, 用于 list 出所有 namespace 下的 k8s job 对象, 并将这些 jobs 加入 slice 中, 这个 slices js := make([]batchv1.Job, 0) 用于在之后对 sync 单个 cronJob 时作为是否已经 trigger job 的判断 同理 pager.New(pager.SimplePageFunc(cronJobListFunc)).EachListItem list 所有 cronjob 对象并对每个对象调用 syncOne 做实际 cronjob 调度, 在调度完后调用 cleanupFinishedJobs 完成清理工作 ​ - 对于成功执行的 job 根据 HistoryLimit 进行 apiserver 中的资源清理 ​ - 对于执行失败的 job 按 limitBackoff 的限制进行重试 - 若处于非上述两种状态的 job 则忽略 主调度函数 syncOne func syncOne(sj *batchv1beta1.CronJob, js []batchv1.Job, now time.Time, jc jobControlInterface, sjc sjControlInterface, recorder record.EventRecorder) { nameForLog := fmt.Sprintf(\"%s/%s\", sj.Namespace, sj.Name) // 首先在之前的 batchv1.Job slice 中顺序查找是否有当前 cronJob 的子 job 并且看看是否有不在 jobActive 列表中的孤儿，以及已经执行完成但是还在 Active 列表中的 job，根据 job 状态记录 event (UnexpectedJob, SawCompletedJob)，删掉不对应的状态 childrenJobs := make(map[types.UID]bool) for _, j := range js { childrenJobs[j.ObjectMeta.UID] = true found := inActiveList(*sj, j.ObjectMeta.UID) if !found \u0026\u0026 !IsJobFinished(\u0026j) { recorder.Eventf(sj, v1.EventTypeWarning, \"UnexpectedJob\", \"Saw a job that the controller did not create","date":"2019-12-14","objectID":"/zh-cn/talk-about-k8s-cronjob/:0:3","tags":["Golang","Kubernetes","Linux"],"title":"Talk about Kubernetes cronJob controller","uri":"/zh-cn/talk-about-k8s-cronjob/"},{"categories":["Coding"],"content":"调优 首先对于 pager.List 尝试替换成 informer watch 的机制, 思路也比较简单, 原先是通过 pager.List 传入的回调来获取 namespace 下所有的 job/cronjob, 现在改为在新建 controller 时注册进 watch event, 监听到变更事件时通过 k8s 封装好的 internal.api.sharedInformer 取回并构造成相同 struct 的对象即可 先看一段原本 pager.List 的代码: cronJobListFunc := func(opts metav1.ListOptions) (runtime.Object, error) { return jm.kubeClient.BatchV1beta1().CronJobs(metav1.NamespaceAll).List(opts) } err = pager.New(pager.SimplePageFunc(cronJobListFunc)).EachListItem(context.Background(), metav1.ListOptions{}, func(object runtime.Object) error { ... } 接着按上述方式在 controller 中先注册 event: type Controller struct { kubeClient clientset.Interface jobControl jobControlInterface sjControl sjControlInterface podControl podControlInterface recorder record.EventRecorder // Codes after refractor queue workqueue.RateLimitingInterface cronjobSynced cache.InformerSynced syncHandler func(key string) error cronjobLister batchv1beta1Lister.CronJobLister } // 接着注册 Informer 并声明 CronJobListener 对应的 callback(add/update/delete) func NewCronJobController(kubeClient clientset.Interface) (*CronJobController, error) { jm, err := NewController(kubeClient) if err != nil { return nil, err } queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"cronjob\"), } cronjobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ }) return jm, nil } k8s informer 的机制需要有 event trigger 来判断在什么事件下触发, 故我们可以简单先加上增删改时的 trigger 故上述代码修改为 func (jm *CronJobController) addCronjob(obj interface{}) { d := obj.(*batchv1beta1.CronJob) glog.V(4).Infof(\"Adding CronJob %s\", d.Name) jm.enqueue(d) } func (jm *CronJobController) updateCronjob(old, cur interface{}) { oldC := old.(*batchv1beta1.CronJob) curC := cur.(*batchv1beta1.CronJob) glog.V(4).Infof(\"Updating CronJob %s\", oldC.Name) jm.enqueue(curC) } func (jm *CronJobController) deleteCronjob(obj interface{}) { d, ok := obj.(*batchv1beta1.CronJob) if !ok { tombstone, ok := obj.(cache.DeletedFinalStateUnknown) if !ok { utilruntime.HandleError(fmt.Errorf(\"couldn't get object from tombstone %#v\", obj)) return } d, ok = tombstone.Obj.(*batchv1beta1.CronJob) if !ok { utilruntime.HandleError(fmt.Errorf(\"tombstone contained object that is not a CronJob %#v\", obj)) return } } glog.V(4).Infof(\"Deleting CronJob %s\", d.Name) jm.enqueue(d) } // 接着注册 Informer 并声明 CronJobListener 对应的 callback(add/update/delete) func NewCronJobController(kubeClient clientset.Interface) (*CronJobController, error) { jm, err := NewController(kubeClient) if err != nil { return nil, err } queue: workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"cronjob\"), } // event trigger cronjobInformer.Informer().AddEventHandler(cache.ResourceEventHandlerFuncs{ AddFunc: jm.addCronjob, UpdateFunc: jm.updateCronjob, DeleteFunc: jm.deleteCronjob, }) // 有了 event trigger 后需要处理当事件触发获取到 cronjob 时该做什么操作 // 按之前的逻辑来看应该对每个 cronjob 调用 syncOne jm.cronjobLister = cronjobInformer.Lister() jm.cronjobSynced = cronjobInformer.Informer().HasSynced jm.syncHandler = jm.syncOne return jm, nil } 接着上述代码只是声明了一堆需要注册的 event trigger(增删改 cronjob 时将cronjob对象放入 workQueue) 和 event handler(syncOne) 但 syncAll 主函数仍然做着原先的工作, 所以我期望的是在主函数中阻塞的获取 workQueue 中的 job, 并按 FIFO 的方式 process 每个 job (暂时没必要对于每个 job 起 goroutine syncOne, 会增加复杂性, 本身的处理能力猜测是足够的) 故将 syncAll 主函数代码改为 // 提供 enqueue 函数用于将 batchv1beta1.CronJob 入队 func (jm *CronJobController) enqueue(cronjob *batchv1beta1.CronJob) { key, _ := controller.KeyFunc(cronjob) sjs := sjl.Items glog.V(4).Infof(\"Found %d cronjobs\", len(sjs)) jobsBySj := groupJobsByParent(js) jm.queue.Add(key) } // 提供一个从 informer 中获取对象的 key 并 enqueue 到 worker queue 的函数 func (jm *CronJobController) EnqueueCronjob() { cjobs, err := jm.cronjobLister.List(labels.Everything()) if err != nil { fmt.Errorf(\"Could not list all cronjobs %v\", err) return } for _, job := range cjobs { jm.enqueue(job) } } // 在 syncAll 主函数中起 goroutine 开始 watch cronjob, 一旦拿到 job 则调用注册的 syncOne 函数 func (jm *CronJobController) syncAll() { ... go wait.Until(func() { EnqueueCronjob","date":"2019-12-14","objectID":"/zh-cn/talk-about-k8s-cronjob/:0:4","tags":["Golang","Kubernetes","Linux"],"title":"Talk about Kubernetes cronJob controller","uri":"/zh-cn/talk-about-k8s-cronjob/"},{"categories":["Gossip"],"content":"你像个琢磨不透的妖精。 前一天晚上我们一起打游戏到深夜，今天我却变成了冷漠的『这一位』 这么说，你是不是没那么喜欢我。 每每至此，我就又开始瞎想些我们分离的日子。 我知道多疑是深植在我们的根性里，我对你却如此猜忌，我真是个坏家伙。 昨天看书抄了一句话 「对于见面我看得较重，对于分别我看得较轻，这是人生取巧之一法，否则聚少离多，悲哀多于欢乐，一生只好负着无尽痛苦的债了。」 我却把别离看得很重，有时甚至有些宿命的情调，会觉得终有一天我们会分别，我害怕那天的到来。却又觉得这样揪着伫念的心，想必很会让你担心吧。所以总试着做些改变，我想着只要我们真正相爱，哪怕一天，一小时，那我可千万不能把我的苦难流露给你，我想看着你笑，特别想。 今晚回家路上，见着一位小孩在卖盐水菠萝，周围散着飞蝇，狗叫和蝉鸣热闹得扎耳。可是都与你无关，顿时对菠萝失去的兴趣。 到家时甚是沮丧，像是被一种抑郁缠绕其中，唯独想着你，写起日记来，才感觉所有的温情开始流到笔端。 你说，今夜何时我才能听见你的声音。 今天我和室友们说，我已经很久没有做梦了，像是被盗梦人收割走了灵光。 我骗了他们，昨夜我又梦到了你，梦见我变成了聊斋志异里的促织，趴在你臂上，挠得你痒痒的。 我不太愿意去想这意味着什么，醒来时只觉得心理暖洋洋的，这股暖流正逐渐透过我的细胞壁溢到空气里，我多想与你分享这间空气。 希望你不要介意，那我一定说个不停。 ","date":"2019-08-05","objectID":"/zh-cn/you-are-my-sunshine/:0:0","tags":["Gossip"],"title":"你就是我的灵光","uri":"/zh-cn/you-are-my-sunshine/"},{"categories":["Gossip"],"content":"真想听一听见你的声音啊。 今晚倘若再不能见着你，我准睡不着。 已经三个夜晚你没有理我了，你是不是故意躲着我。 今天是星期五，我一点都开心不起来，一想到不能像前几个周末一样宅在小屋子里看《乐队的夏天》，我的灵魂就像被汲取干了一般。 我渴望见着你，我们打一架，把心里的不痛快都说出来，好不好。 我又渴望抱抱你，这几天你是开心还是伤心呢，我全然不知。 这几天下来，我逐渐开始认清自己，我实在是个坏人，你不要为我不安。 啊你突然回来了，心跳好快，我不想写了，我有好多好多话跟你说，我看着你的眼睛，啥都说不出来，我想把你的美写下来，写不出，我想写诗，写雨，但满脑子想的都是你。 ","date":"2019-08-02","objectID":"/zh-cn/badly-wanted/:0:0","tags":["Gossip"],"title":"请你不要吃掉我","uri":"/zh-cn/badly-wanted/"},{"categories":["Gossip"],"content":"前天，我们吵架了，但主要是我一人暗暗生闷气。 在此之前，我总认为自己已经是不依附于任何情感的，也鲜有真正让我生气的事儿。 没想到在不经意间听到你提起那只臭猪的名字，忍不住暴跳如雷。 我是惶恐的，极其自私的，真的。 也许是我自己卑鄙无耻的胡猜，但一想到有一天会瞥见你清澈的晶状体房水里倒映的是别人的身影，我就懊恼。 这些天你不怎么想理我，我能听到办公室的时针滴答滴答的毫无意义的钟摆。我把标记过的好多小说和电影都跳着翻了翻，却总是静不下心。我一度以为即便不再那么鲜活地见到你，我也能很平静地走过去，能逃遁到更远的地方去。 王小波说的是对的，人是轻易不能知道自己的。 人可以对别人有最细微的感觉，但对自己就迟钝得多。 后知后觉地反应过来，耍的这些小脾气真的会让你担心。我以为自己会很得意，会觉得自己是被需要的，但才发现自己多么丑陋。我不愿意让你再焦心了。我喜欢看你笑，笑起来皱着鼻子，团成一只小饭团。可是我这么稀里糊涂地，又让你伤心了。 我真是无可救药的大笨蛋，你说我这个人还有值得原谅的地方吗。 我好难过，感觉自己有个什么决断做不出来，生闷气的那天晚上，我走到了很远的地方，看了很久的天空，夜晚的光收敛进漫天的星云中，我多希望里面会泄出金光，那我一定把你叫出来，想那些金萤的东西包裹住我们。 你知不知道，有时候我脑洞很大，整个故事的来龙去脉，悲欢离合，我都想明白了，只是我还不能狠下心去告诉你，你一直在我理想生活的构建里。我多想告诉你我的秘密，我想把我所有感官，所有体验都分享给你，我是一个时刻沸腾着的人，永远滋滋地响，翻腾不休。但一想到你，小心翼翼地，我就能平静下来。 明天就是星期五了，我真怕你从此不再理我，希望你明天能开始理我，要不，你可以吃掉我。 ","date":"2019-08-01","objectID":"/zh-cn/cutanzi-panda/:0:0","tags":["Gossip"],"title":"大醋坛子熊猫侠","uri":"/zh-cn/cutanzi-panda/"},{"categories":["Gossip"],"content":"「你快看这朵云，舒张得恰巧。像刚出城堡骑着穿山甲的鹫鹰骑士，持着大枪冲向巨龙，刺破了一片天，天外的金光像流萤一样泄了出来」 刚在百老汇看完《旺角卡门》, 饿得五脊六兽, 急冲冲地想去家粤菜馆子点一碗炖鸡老汤。 记得第一次聊起这部电影，是给你写信后的两三周，你也说王家卫的电影里从来不会讲这俩人是因为什么爱上的，却好像都是无缘无故就爱上了。只有状态，毫无因果。 这难道不迷人吗，我天生爱这些没由头的东西，它们是弹丸偏离轨道击碎的窗户，只能听见其响，意识过来时，玻璃渣已碎了一地。这些难以琢磨的不期而至的碰撞，总是不经意地夺走你的时光，直到踩在玻璃渣上刺痛了脚才反应过来。 但我仍然好奇在长久的一段时间后，阿娥会不会反应过来，那个藏起来的杯子，一直没有被找到。从此以往地，便是像每一天这样灰暗愁闷的日子，也得爱，也得焦心。她得有多难过。 莱蒙托夫说「也许我爱的已不是你，而是对你付出的热情。就像是一座神庙，即使荒芜，仍然是祭坛。一座雕像，即使坍塌，仍然是神」 我最近又陷入了这种状态，第一次知悉你要去北方，不免觉得北方那么冷，由不得任性，满脑子叮嘱你得穿秋裤。 等我真正到了北方，才发现北方的冬天是下雪的，一点也不冷，有几片孤花趁着下雪天旺盛地生长，有猫咪对着窗台飘的雪花跳脚。我憋足了劲儿说，你们都盛开起来吧，把白茫茫一片裸露的大地撕裂，把沸腾的鲜血都撒在上面。约莫半载，雪地里开始飘出红色的花瓣，风却慢慢把这些玫瑰吹散，最后枯竭得像精卫填海，填不进白茫茫一片，精疲力竭。 再后来，屋子搬来了年轻人，像所有青年才俊一样，卯住劲投身入热枕中。就像你当时走过的路，我总想着这些发自内心的不愉悦却要被汲汲营营的名誉、条条框框的规则、战战兢兢的人情所淹没，就感到无比的孤独。 我看你像大部分的水滴一样，流入了那条大河里，川流地河水互相摩擦生热逐渐漫起水雾，那些浓浊的雾气徘徊在你我之间，我想它散开，因为我怕看不见你，我又想它永远在这，因为我怕看不见你。 就像基因编辑里人们趋之若鹜地想要拼接自己的 DNA 移植上最优的属性，巴不得是个天生的六边形战士，流水工厂式的繁殖，一代一代商品化的高级人，予取予求地寻找另一位六边形战士，聚群成一类分不清的基因链，齐步行走着，像是动物世界里驼铃的迁移。我暗暗期望你不会是那个空茫的人之一，能有间隙从真实人生中探出头放空出来，去做些心怡的事儿，也期望之中有人能游到更加真实的、离得不远的一些去处，逃离这些日复一日消磨着人的远足。 去粤菜馆子的路上，看着窗外，这座城市像上了发条一般运转，循规蹈矩，咿呀作响，日复一日地用同一种模式运营着所有公民。仿佛悬浮于城市上空仍有一台工业时代的冒着仙气的蒸汽机，张牙舞爪地喷射着火焰，热飒了熊猫侠。似乎北方的夏天远比冬天令人生厌得多，不知道你那儿今夜起风吗。 起风了，就该有云，云卷云舒，然后再被风吹散。 归途路上，又不禁胡思乱想，那时候听你说想看成都的云，巴不得把天空扯得稀碎，结果只剩下三三两两形单影只的几朵，经不起妄想。 飘飘然地，在归途路上，遇见了它，像奔向风车的骑士，喜欢极了，我多想你也看到这朵云。 于北京， 粤菜馆子路上， 肚子饿饿饿。 2019/07/27 ","date":"2019-07-27","objectID":"/zh-cn/i-want-you-to-see-this-sky/:0:0","tags":["Gossip","Bong"],"title":"我想你也看到这朵云","uri":"/zh-cn/i-want-you-to-see-this-sky/"},{"categories":["Coding"],"content":"背景 最近在升级集群的 kube-prxoy 并开启 ipvs mode, 引发了一些线上故障 ","date":"2019-04-29","objectID":"/zh-cn/kubeproxy-ipvs-accident/:1:0","tags":["Iptables","Kubernetes","Linux"],"title":"记一次升级 kube-proxy ipvs 引发的线上故障","uri":"/zh-cn/kubeproxy-ipvs-accident/"},{"categories":["Coding"],"content":"替换原因 由于豆瓣的集群使用 calico + kube-proxy iptables mode + puppet iptable 脚本管理 三个组件共同操作同一份 iptables, 容易出现 race condition 问题, 并且还会互相抢占 iptables 锁, 是个 Mutex unsafe 的操作, 不易于维护. 故打算尽量减少操作 iptables 的部分, 替换成 ipvs ","date":"2019-04-29","objectID":"/zh-cn/kubeproxy-ipvs-accident/:2:0","tags":["Iptables","Kubernetes","Linux"],"title":"记一次升级 kube-proxy ipvs 引发的线上故障","uri":"/zh-cn/kubeproxy-ipvs-accident/"},{"categories":["Coding"],"content":"事故回溯 @400000005cbea9a81eaf9164 W0423 13:58:54.514773 14016 server.go:195] WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated. Please begin using a config file ASAP. @400000005cbea9a81fe30854 W0423 13:58:54.534952 14016 server_others.go:287] Flag proxy-mode=\"\" unknown, assuming iptables proxy @400000005cbea9a81ff0bc24 I0423 13:58:54.535856 14016 server_others.go:140] Using iptables Proxier. @400000005cbea9a820d714e4 I0423 13:58:54.550947 14016 server_others.go:174] Tearing down inactive rules. @400000005cbea9a828a386bc I0423 13:58:54.681728 14016 server.go:448] Version: v1.11.1 @400000005cbea9a82be9b10c I0423 13:58:54.736441 14016 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_max' to 786432 @400000005cbea9a82be9b8dc I0423 13:58:54.736512 14016 conntrack.go:52] Setting nf_conntrack_max to 786432 @400000005cbea9a82be9b8dc I0423 13:58:54.736566 14016 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_established' to 86400 @400000005cbea9a82be9bcc4 I0423 13:58:54.736625 14016 conntrack.go:98] Set sysctl 'net/netfilter/nf_conntrack_tcp_timeout_close_wait' to 3600 @400000005cbea9a82bea457c I0423 13:58:54.736765 14016 config.go:202] Starting service config controller @400000005cbea9a82bea7074 I0423 13:58:54.736778 14016 controller_utils.go:1025] Waiting for caches to sync for service config controller @400000005cbea9a82bec31ac I0423 13:58:54.736850 14016 config.go:102] Starting endpoints config controller @400000005cbea9a82bec3594 I0423 13:58:54.736867 14016 controller_utils.go:1025] Waiting for caches to sync for endpoints config controller @400000005cbea9a831e37a0c I0423 13:58:54.836917 14016 controller_utils.go:1032] Caches are synced for service config controller @400000005cbea9a831e3dbb4 I0423 13:58:54.837000 14016 controller_utils.go:1032] Caches are synced for endpoints config controller @400000005cbea9aa0f11fe7c E0423 13:58:56.252807 14016 proxier.go:1340] Failed to delete stale service IP 172.19.119.127 connections, error: error deleting connection tracking state for UDP service IP: 172.19.119.127, error: error looking for path of conntrack: exec: \"conntrack\": executable file not found in $PATH @400000005cbea9aa0f12f494 E0423 13:58:56.252887 14016 proxier.go:1340] Failed to delete stale service IP 172.19.0.53 connections, error: error deleting connection tracking state for UDP service IP: 172.19.0.53, error: error looking for path of conntrack: exec: \"conntrack\": executable file not found in $PATH @400000005cbea9aa0f13b014 E0423 13:58:56.252939 14016 proxier.go:1340] Failed to delete stale service IP 172.19.68.99 connections, error: error deleting connection tracking state for UDP service IP: 172.19.68.99, error: error looking for path of conntrack: exec: \"conntrack\": executable file not found in $PATH Failed to delete stale service IP 172.19.68.99 connections, error: error deleting connection tracking state for UDP service IP: 172.19.68.99, error: error looking for path of conntrack: exec: \"conntrack\": executable file not found in $PATH = = 物理机上没装 conntrack 的包 = = 以及 WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated. Please begin using a config file ASAP. 替换时首先遇到了物理机未安装 conntrack 导致 kube-proxy 拉不起来的问题 接着在升级 kube-proxy 期间, 我通过 另起了 kube-proxy daemonset 执行了 docker exec -it $(docker ps | grep hyperkube | awk '{print $1 \" ./hyperkube kube-proxy --cleanup\"}' ) 逐步清理了集群节点上的 iptables 规则. 此时便发生了线上故障. 归因为网络不通. podip 互 ping 丢包. 后续排查发现, 发现故障节点因为清理缘故 iptables 中少了这一条规则, docker bridge network 下的 container 向外发包时，没有做 SNAT, 对端收到包但无法正确回包，导致通信失败 -A POSTROUTING -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE 故所有依赖 docker0 访问的非 k8s container 均出现了网络异常. ","date":"2019-04-29","objectID":"/zh-cn/kubeproxy-ipvs-accident/:3:0","tags":["Iptables","Kubernetes","Linux"],"title":"记一次升级 kube-proxy ipvs 引发的线上故障","uri":"/zh-cn/kubeproxy-ipvs-accident/"},{"categories":["Coding"],"content":"解决办法 临时在节点上批量执行以下脚本刷上 iptables 规则 sudo iptables -t nat -I POSTROUTING 3 -s 172.17.0.0/16 ! -o docker0 -j MASQUERADE ","date":"2019-04-29","objectID":"/zh-cn/kubeproxy-ipvs-accident/:4:0","tags":["Iptables","Kubernetes","Linux"],"title":"记一次升级 kube-proxy ipvs 引发的线上故障","uri":"/zh-cn/kubeproxy-ipvs-accident/"},{"categories":["Coding"],"content":"额外引发的另一个 dns 查询黑洞事故 在升级 kube-proxy ipvs mode 后, 意外发现每当 coredns down 重启后, 必定出现短暂的 dns query 异常并伴随着一波 502. 总结原因如下: 当执行 kube-proxy 升级时会 drain k8s 节点 drain node –\u003e evict 掉节点 coredns pod 时 kubeproxy 来不及删掉旧的规则导致 ipvs rules 里仍有一条不存在后端的 realserver , 导致请求走到这条规则后 dns 解析进黑洞直到 5s timeout kube-proxy 会先把要删除的 rs 权重置为0，然后加入到待删除 rs list 里由另一个 1mins 周期的循环来删，但在删除逻辑中如果仍有连接保持着则会出现删除失败但权重为0的过渡期，这期间便是有问题的, 会带来短暂的不可用 I0514 16:23:54.047540 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:53/UDP/172.18.81.97:53 I0514 16:23:54.047603 1 graceful_termination.go:171] Not deleting, RS 172.19.0.53:53/UDP/172.18.81.97:53: 0 ActiveConn, 3 InactiveConn I0514 16:23:54.260987 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:9153/TCP/172.18.81.97:9153 I0514 16:23:54.261050 1 graceful_termination.go:174] Deleting rs: 172.19.0.53:9153/TCP/172.18.81.97:9153 I0514 16:23:54.437893 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:53/TCP/172.18.81.97:53 I0514 16:23:54.437952 1 graceful_termination.go:174] Deleting rs: 172.19.0.53:53/TCP/172.18.81.97:53 I0514 16:24:09.602189 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:53/UDP/172.18.81.97:53 I0514 16:24:09.602330 1 graceful_termination.go:171] Not deleting, RS 172.19.0.53:53/UDP/172.18.81.97:53: 0 ActiveConn, 2 InactiveConn I0514 16:25:09.602466 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:53/UDP/172.18.81.97:53 I0514 16:25:09.602601 1 graceful_termination.go:171] Not deleting, RS 172.19.0.53:53/UDP/172.18.81.97:53: 0 ActiveConn, 2 InactiveConn I0514 16:26:09.602770 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:53/UDP/172.18.81.97:53 I0514 16:26:09.602917 1 graceful_termination.go:171] Not deleting, RS 172.19.0.53:53/UDP/172.18.81.97:53: 0 ActiveConn, 2 InactiveConn I0514 16:27:09.603060 1 graceful_termination.go:160] Trying to delete rs: 172.19.0.53:53/UDP/172.18.81.97:53 I0514 16:27:09.603182 1 graceful_termination.go:171] Not deleting, RS 172.19.0.53:53/UDP/172.18.81.97:53: 0 ActiveConn, 1 InactiveConn 直到所有连接都释放后才能正常删除 I0514 16:37:09.605442 1 graceful_termination.go:174] Deleting rs: 172.19.0.53:53/UDP/172.18.79.142:53 I0514 16:37:09.605468 1 graceful_termination.go:93] lw: remote out of the list: 172.19.0.53:53/UDP/172.18.79.142:53 ","date":"2019-04-29","objectID":"/zh-cn/kubeproxy-ipvs-accident/:5:0","tags":["Iptables","Kubernetes","Linux"],"title":"记一次升级 kube-proxy ipvs 引发的线上故障","uri":"/zh-cn/kubeproxy-ipvs-accident/"},{"categories":["Coding"],"content":"解决办法 有 coredns 加上 fallback dns 服务以及 lvs 探活 确保 health check 过了才会正常打入后端 ","date":"2019-04-29","objectID":"/zh-cn/kubeproxy-ipvs-accident/:6:0","tags":["Iptables","Kubernetes","Linux"],"title":"记一次升级 kube-proxy ipvs 引发的线上故障","uri":"/zh-cn/kubeproxy-ipvs-accident/"},{"categories":["Coding"],"content":"Lexus Lee ","date":"2018-09-14","objectID":"/zh-cn/k8s-service-endpoints/:0:0","tags":["Golang","Kubernetes","Linux"],"title":"浅谈 k8s service\u0026kube-proxy","uri":"/zh-cn/k8s-service-endpoints/"},{"categories":["Coding"],"content":"背景 最开始听到同事 k8s 分享时比较困惑我的一个问题是 k8s 怎么实现一个私有 ip(虚拟 ip，以下简称 vip)到另一个私有ip收发包的。 不过其实我想知道的应该是 k8s 通信机制，它是怎么实现服务发现的，新建的 pod 是怎么感知到的，万一有些 pod 节点变更 vip 变了 k8s 是如何感知的。 基于这个问题，做一下关于 k8s service\u0026kube-proxy 的分享。 ","date":"2018-09-14","objectID":"/zh-cn/k8s-service-endpoints/:0:1","tags":["Golang","Kubernetes","Linux"],"title":"浅谈 k8s service\u0026kube-proxy","uri":"/zh-cn/k8s-service-endpoints/"},{"categories":["Coding"],"content":"Service\u0026kube-proxy 概述 首先我建了一个 replcas = 4 lebel: app=service_test_pod的 python server deployment 来打出当前 pod 的 Hostname apiVersion:apps/v1kind:Deploymentmetadata:name:service-testspec:replicas:4selector:matchLabels:app:service_test_podtemplate:metadata:labels:app:service_test_podspec:containers:- name:simple-httpimage:python:2.7imagePullPolicy:IfNotPresentcommand:[\"/bin/bash\"]args:[\"-c\",\"echo \\\"\u003cp\u003eHello from $(hostname)\u003c/p\u003e\\\" \u003e index.html; python -m SimpleHTTPServer 9999\"]ports:- name:httpcontainerPort:9999 可以看到启动了4个 pod service-test-69764ddb4c-4hr2x 1/1 Running 0 8s 172.18.234.21 brand6 service-test-69764ddb4c-gstft 1/1 Running 0 8s 172.18.83.225 belba2 service-test-69764ddb4c-nnv29 1/1 Running 0 8s 172.18.156.140 brand2 service-test-69764ddb4c-vx5pn 0/1 ContainerCreating 0 8s \u003cnone\u003e belba3 我必须挨个 curl 才能得到他们的 hostname lilingzhi@belba1 ~/k8s/test $ curl 172.18.234.21:9999 \u003cp\u003eHello from service-test-69764ddb4c-4hr2x\u003c/p\u003e 但不能让其他 pod 直接通过 vip 访问这些 pod ，需要一个更上层的一个抽象，把这4个提供相同服务的 pod 打包成一个对外的服务，通过某个入口地址来访问它，并且把请求均衡到4个pod上，这样一层的抽象包装是实现一个服务网络(service mesh)的基础。 而 k8s service 就是做这个的。 首先我们来看下什么是 k8s service: A Kubernetes Service is an abstraction which defines a logical set of Pods and a policy by which to access them - sometimes called a micro-service. The set of Pods targeted by a Service is (usually) determined by a Label Selector (see below for why you might want a Service without a selector) 而打包pod成service并发布的微服务得支持 k8s 内部及外部的访问。 故 k8s service 提供了以下三种暴露 service 入口的模式: ClusterIP: use a cluster-internal IP only - this is the default and is discussed above. Choosing this value means that you want this service to be reachable only from inside of the cluster. NodePort: on top of having a cluster-internal IP, expose the service on a port on each node of the cluster (the same port on each node). You’ll be able to contact the service on any :NodePort address. LoadBalancer: on top of having a cluster-internal IP and exposing service on a NodePort also, ask the cloud provider for a load balancer which forwards to the Service exposed as a :NodePort for each Node. 可以简单理解为 ClusterIp 是提供对内的访问入口，NodePort 和 LoadBalancer 是提供对外的，不过 LoadBalancer 是在暴露 NodePort 基础上提供可以接入外部的 LB。 那么我新建一个 service 给刚刚的 4 个 pod lilingzhi@belba1 ~/k8s/test $ kubectl expose deployment service-test --type=\"NodePort\" --port 9098 --target-port=9999 service/service-test exposed 可以看到 lilingzhi@belba1 ~/k8s/test $ kubectl get service -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR service-test NodePort 172.19.97.3 \u003cnone\u003e 9098:30255/TCP 16s app=service_test_pod service-test 这儿就映射到 clusterIp 的 172.19.97.3:9098 端口上 再看下 endpoints lilingzhi@belba1 ~/k8s/test $ kubectl get endpoints -o wide NAME ENDPOINTS AGE service-test 172.18.156.140:9999,172.18.193.66:9999,172.18.234.21:9999 + 1 more... 3m 可以看到也建了一个包含 4个 host:port 的元组的 endpoint 通过不断 curl service ip:port 会发现请求已经均衡到4个 pod 上了 lilingzhi@belba1 ~/k8s/test $ curl 172.19.97.3:9098 \u003cp\u003eHello from service-test-69764ddb4c-4hr2x\u003c/p\u003e lilingzhi@belba1 ~/k8s/test $ curl 172.19.97.3:9098 \u003cp\u003eHello from service-test-69764ddb4c-gstft\u003c/p\u003e lilingzhi@belba1 ~/k8s/test $ curl 172.19.97.3:9098 \u003cp\u003eHello from service-test-69764ddb4c-4hr2x\u003c/p\u003e lilingzhi@belba1 ~/k8s/test $ curl 172.19.97.3:9098 \u003cp\u003eHello from service-test-69764ddb4c-gstft\u003c/p\u003e 由于 k8s service 路由是通过 kube-proxy 决定的，默认是走的 iptables 转发的(可换成用户态 proxy 或 ipvs)，所以查一下相应的 iptables 规则 $ sudo iptables -L -v -n -t nat Chain KUBE-SERVICES (2 references) 0 0 KUBE-MARK-MASQ tcp -- * * !172.18.0.0/16 172.19.97.3 /* default/service-test: cluster IP */ tcp dpt:9098 0 0 KUBE-SVC-LY73ZDGF4KGO4YFJ tcp -- * * 0.0.0.0/0 172.19.97.3 /* default/service-test: cluster IP */ tcp dpt:9098 看到有条 chain KUBE-SVC-LY73ZDGF4KGO4YFJ 定义了 service-test 的转发规则，于是查看相应的 chain Chain KUBE-SVC-LY73ZDGF4KGO4YFJ (2 references) pkts bytes target prot opt in out source destination 0 0 KUBE-SEP-2T6K76SEPIPV3QKW all -- * * 0.0.0.0/0 0.0.0.0/0 /* default/service-test: */ statistic mode random probability 0.25000000000 0 0 KUBE-SEP-75XULILUFIH","date":"2018-09-14","objectID":"/zh-cn/k8s-service-endpoints/:0:2","tags":["Golang","Kubernetes","Linux"],"title":"浅谈 k8s service\u0026kube-proxy","uri":"/zh-cn/k8s-service-endpoints/"},{"categories":["Coding"],"content":"kube-proxy 源码分析 因为 kube-proxy 源码相对比较少，所以读了下源码，但还是蛮复杂的 kube-proxy 会作为 daemon 跑在每个节点上，对 api-server 中的 service \u0026 endpoint 进行 watch ,一旦检测到更新则往 iptables 里全量推送新的转发规则，那么我们根据 kubernetes/cmd/kube-proxy/proxy.go 里找到 kube-proxy 真正的入口函数 Run() func (s *ProxyServer) Run() error { ... serviceConfig := config.NewServiceConfig(informerFactory.Core().V1().Services(), s.ConfigSyncPeriod) serviceConfig.RegisterEventHandler(s.ServiceEventHandler) go serviceConfig.Run(wait.NeverStop) endpointsConfig := config.NewEndpointsConfig(informerFactory.Core().V1().Endpoints(), s.ConfigSyncPeriod) endpointsConfig.RegisterEventHandler(s.EndpointsEventHandler) go endpointsConfig.Run(wait.NeverStop) // This has to start after the calls to NewServiceConfig and NewEndpointsConfig because those // functions must configure their shared informer event handlers first. go informerFactory.Start(wait.NeverStop) // Birth Cry after the birth is successful s.birthCry() // Just loop forever for now... s.Proxier.SyncLoop() return nil } 这就是 kube-proxy 的入口函数，实际上即起一个 proxy server daemon 可以看到通过 informerFactory 新建了2个 config 对象(serviceConfig, endpointsConfig), 这个 informerFactory 是什么呢？ k8s 里所有资源都存在 etcd 中提供 api 通过 apiserver 的接口访问，其中有个核心的公共组件即 informer 是对 apiserver 资源访问的一层包装，其中包括 api 访问, localcache 等… 所以这里的两个 config 对象即用来获取 etcd 中 service 和 endpoints 的信息，他们都调用了 RegisterEventHandler 注册了一个回调函数，这个函数用来监听变更并发送变更信号。 最后用 goroutine 跑起来。 之后的 informerFactory.Start() 则用来初始化 informer 对象注册的回调函数。 最后把 proxy server loop 跑起来 Proxier.SyncLoop() 等待信号。 接着我们先看下 service 的 NewServiceConfig 这个函数，因为 endpoint 估计也是类似的。 // NewServiceConfig creates a new ServiceConfig. func NewServiceConfig(serviceInformer coreinformers.ServiceInformer, resyncPeriod time.Duration) *ServiceConfig { result := \u0026ServiceConfig{ lister: serviceInformer.Lister(), listerSynced: serviceInformer.Informer().HasSynced, } serviceInformer.Informer().AddEventHandlerWithResyncPeriod( cache.ResourceEventHandlerFuncs{ AddFunc: result.handleAddService, UpdateFunc: result.handleUpdateService, DeleteFunc: result.handleDeleteService, }, resyncPeriod, ) return result } 看到它结构体里就两个对象 lister 和 listerSynced，所以我们接着看下 serviceInformer type ServiceInformer interface { Informer() cache.SharedIndexInformer Lister() v1.ServiceLister } 看到是个通用对象 SharedIndexInformer 即上述提及的公共组件，故不往里深究。 接着回来看注册进去的回调函数，举个栗子，这儿的 UpdateFunc: result.handleUpdateService 最终会调到 iptables.go 里的 OnServiceUpdate func (c *ServiceConfig) handleUpdateService(oldObj, newObj interface{}) { oldService, ok := oldObj.(*v1.Service) if !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", oldObj)) return } service, ok := newObj.(*v1.Service) if !ok { utilruntime.HandleError(fmt.Errorf(\"unexpected object type: %v\", newObj)) return } for i := range c.eventHandlers { glog.V(4).Infof(\"Calling handler.OnServiceUpdate\") c.eventHandlers[i].OnServiceUpdate(oldService, service) } } func (proxier *Proxier) OnServiceUpdate(oldService, service *v1.Service) { if proxier.serviceChanges.Update(oldService, service) \u0026\u0026 proxier.isInitialized() { proxier.syncRunner.Run() } } 所以监听到变更之后调用的这个 proxier.syncRunner.Run() 是什么呢，得看下这个 syncRunner 在做什么 proxier.syncRunner = async.NewBoundedFrequencyRunner(\"sync-runner\", proxier.syncProxyRules, minSyncPeriod, syncPeriod, burstSyncs) func (bfr *BoundedFrequencyRunner) Run() { // If it takes a lot of time to run the underlying function, noone is really // processing elements from \u003crun\u003e channel. So to avoid blocking here on the // putting element to it, we simply skip it if there is already an element // in it. select { case bfr.run \u003c- struct{}{}: default: } } func (bfr *BoundedFrequencyRunner) Loop(stop \u003c-chan struct{}) { glog.V(3).Infof(\"%s Loop running\", bfr.name) bfr.timer.Reset(bfr.maxInterval) for { select { case \u003c-stop: bfr.stop() glog.V(3).Infof(\"%s Loop stopping\", bfr.name) return case \u003c-bfr.timer.C(): bfr.tryRun() case \u003c-bfr.run: bfr.tryRun() } } } syncRunner 里注册了个 syncProxyRules 的回调函数，而刚刚 updateSync 中触发的 run 函数则用 select 发送了一个 bfr.run 信号，之前所提及的 proxy server 一旦收到","date":"2018-09-14","objectID":"/zh-cn/k8s-service-endpoints/:0:3","tags":["Golang","Kubernetes","Linux"],"title":"浅谈 k8s service\u0026kube-proxy","uri":"/zh-cn/k8s-service-endpoints/"},{"categories":["Coding"],"content":"最后 最后我们来理一下 service \u0026\u0026 endpoint \u0026\u0026 kube-proxy 的关系。 service / endpoint 是pod对外暴露访问地址的封装，Kube-proxy 用来管理这些封装，做一些 ensure\u0026update 的操作 ","date":"2018-09-14","objectID":"/zh-cn/k8s-service-endpoints/:0:4","tags":["Golang","Kubernetes","Linux"],"title":"浅谈 k8s service\u0026kube-proxy","uri":"/zh-cn/k8s-service-endpoints/"},{"categories":["Coding"],"content":"背景 今天下午连续收到了腾讯云 CPU overload 报警 登服务器一看, 有个 postgres 账户跑的进程把 CPU 占满了，进程名特别奇怪。 ","date":"2018-08-11","objectID":"/zh-cn/scarllet-sql-attack/:0:1","tags":["Security","Linux"],"title":"记一次 postgresql 斯嘉丽约翰逊攻击的排查","uri":"/zh-cn/scarllet-sql-attack/"},{"categories":["Coding"],"content":"排查 于是根据 pid 到 /proc/20619/stack 下看到有一长串的 [\u003cffffffff81841ff2\u003e] entry_SYSCALL_64_fastpath+0x16/0x71 ，似乎短时间里发起大量的系统调用(prepare)并且还在不断增长。 接着 cat /proc/20619/cmdline 发现执行的是 /var/lib/postgresql/9.5/main/Ac2p018-0 这个坏家伙，查看发现这是个二进制文件，看不出问题，猜测和 postgresql 数据库有关，看起来不像是什么数据库维护脚本，第一反应是被数据库攻击了，于是查看 /var/lib/postgresql/.bash_history 和 /var/lib/postgresql/.psql_history 发现一条记录都没，显然是被手动清空了，更加确定是被 hack 了。担心已经被拿到 root 权限了，于是通过 lastlog 和 last 查看登录状态，所幸之前的 root 账户的 ip 都是我自己的，只有 postgres 这个账户看起来异常。 接着到 /var/lib/postgresql/9.5/main/pg_log 下查看数据库日志，抓到了几个奇怪的地方： 有一个长连接持续从 http://aluka.info/x6 下载文件， 5144 --2018-08-11 15:47:30-- http://aluka.info/x6 5145 Resolving aluka.info (aluka.info)... 103.27.110.206 5146 Connecting to aluka.info (aluka.info)|103.27.110.206|:80... connected. 5147 HTTP request sent, awaiting response... 200 OK 5148 Length: 2758655 (2.6M) 5149 Saving to: ‘xmm’ 5150 5151 0K .......... .......... .......... .......... .......... 1% 399K 7s 5152 50K .......... .......... .......... .......... .......... 3% 601K 5s 5153 100K .......... .......... .......... .......... .......... 5% 592K 5s 5154 150K .......... .......... .......... .......... .......... 7% 1.69M 4s 5155 200K .......... .......... .......... .......... .......... 9% 754K 4s 5156 250K .......... .......... .......... .......... .......... 11% 422K 4s 5157 300K .......... .......... .......... .......... .......... 12% 405K 4s 5158 350K .......... .......... .......... .......... .......... 14% 179K 5s 5159 400K .......... .......... .......... .......... .......... 16% 81.1K 8s 5160 450K .......... .......... .......... .......... .......... 18% 35.1K 13s 5161 500K .......... .......... .......... .......... .......... 20% 117K 13s 5162 550K .......... .......... .......... .......... .......... 22% 86.3K 14s 5163 600K .......... .......... .......... .......... .......... 24% 122K 14s 5164 650K .......... .......... .......... .......... .......... 25% 169K 13s 5165 700K .......... .......... .......... .......... .......... 27% 171K 13s 5166 750K .......... .......... .......... .......... .......... 29% 93.8K 13s 5167 800K .......... .......... .......... .......... .......... 31% 94.9K 13s 5168 850K .......... .......... .......... .......... .......... 33% 101K 13s 5169 900K .......... .......... .......... .......... .......... 35% 53.6K 14s 5170 950K .......... .......... .......... .......... .......... 37% 94.3K 14s 5171 1000K .......... .......... .......... .......... .......... 38% 77.0K 13s 5172 1050K .......... .......... .......... .......... .......... 40% 73.6K 13s 5173 1100K .......... .......... .......... .......... .......... 42% 97.2K 13s 5174 1150K .......... .......... .......... .......... .......... 44% 130K 13s 5175 1200K .......... .......... .......... .......... .......... 46% 194K 12s 5176 1250K .......... .......... .......... .......... .......... 48% 173K 12s 5177 1300K .......... .......... .......... .......... .......... 50% 109K 11s 5178 1350K .......... .......... .......... .......... .......... 51% 82.9K 11s 5179 1400K .......... .......... .......... .......... .......... 53% 134K 10s 5180 1450K .......... .......... .......... .......... .......... 55% 106K 10s 5181 1500K .......... .......... .......... .......... .......... 57% 188K 10s 5182 1550K .......... .......... .......... .......... .......... 59% 51.4K 9s 5183 1600K .......... .......... .......... .......... .......... 61% 54.6K 9s 5184 1650K .......... .......... .......... .......... .......... 63% 86.8K 9s 5185 1700K .......... .......... .......... .......... .......... 64% 160K 8s 5186 1750K .......... .......... .......... .......... .......... 66% 97.9K 8s 5187 1800K .......... .......... .......... .......... .......... 68% 153K 8s 5188 1850K .......... .......... .......... .......... .......... 70% 127K 7s 5189 1900K .......... .......... .......... .......... .......... 72% 117K 7s 5190 1950K .......... .......... .......... .......... .......... 74% 63.8K 6s ","date":"2018-08-11","objectID":"/zh-cn/scarllet-sql-attack/:0:2","tags":["Security","Linux"],"title":"记一次 postgresql 斯嘉丽约翰逊攻击的排查","uri":"/zh-cn/scarllet-sql-attack/"},{"categories":["Coding"],"content":"反思 这次主要的原因是 postgres 配置权限时偷懒导致服务器变成挖矿僵尸。 postgres pg_hba.conf 里的用户认证 method 应改成 md5 方式 数据库 superuser 只配置只能 local 访问禁止远程访问 腾讯云安全组里数据库端口 outbound 应尽量限制 ip 段 ","date":"2018-08-11","objectID":"/zh-cn/scarllet-sql-attack/:0:3","tags":["Security","Linux"],"title":"记一次 postgresql 斯嘉丽约翰逊攻击的排查","uri":"/zh-cn/scarllet-sql-attack/"},{"categories":["Coding"],"content":"Go 实现装饰器 类似于 Java 中的装饰器模式, 在 go 中也会有需要 patch 一个函数的需求. 一个常见的场景就是比如计算某个函数耗时的 decorator, 那么利用 functional programming 思想. 在 go 中的实现如下: func timeSpent(inner func(n int) int) func(op int) int { return func(n int){ start := time.Now() ret := inner(n) fmt.Println(\"time spent: \", time.Since(start).Seconds()) return ret } } 当然实际场景中可能更多需要的是一种 pipeline 装饰器的用法, 比如一个 http handler 需要进行 Auth 认证, 认证通过后需要进行耗时计算, 计算完耗时需要进行 statsd 打点. 那么对应的实现如下: func StatsdTiming() func(http.handler) http.Handler { return func(next http.Handler) http.Hanlder { fn := func(w http.ResponseWriter, r *http.Request) { lbSpanTime := time.Duration(0) if parsed, err := strconv.ParseFloat(r.Header.Get(\"X-Request-Start\"), 64); err == nil { sec := int64(parsed) nsec := int64((parsed - float64(sec)) * 1000000000) lbStartTime := time.Unix(sec, nsec) lbSpanTime = time.Now().Sub(lbStartTime) statsdClient.TimingDuration(fmt.Sprintf(\"reqtime\", lbSpanTime)) } } } } func Auth() func(http.handler) http.Handler { return func(next http.Handler) http.Hanlder { function_name_str := runtime.FuncForPC(reflect.ValueOf(input).Pointer()).Name() function_name_array := strings.Split(function_name_str, \"/\") module_method := strings.Split(function_name_array[len(function_name_array)-1], \".\") module := module_method[0] method := module_method[1] if module != \"Login\" { abort(\"Not Auth\") } } } http.HandleFunc(\"/hello\", Handler(hello, Auth, StatsdTiming)) ","date":"2018-02-10","objectID":"/zh-cn/go-decorator/:1:0","tags":["Golang"],"title":"Golang 实现装饰器","uri":"/zh-cn/go-decorator/"},{"categories":["Coding"],"content":"坦率地讲 服务熔断 \u0026 服务降级 ","date":"2018-02-01","objectID":"/zh-cn/service-fallback/:1:0","tags":["Service","Architecture","Linux"],"title":"坦率地讲 服务熔断 \u0026 服务降级","uri":"/zh-cn/service-fallback/"},{"categories":["Coding"],"content":"背景 之前遇到个问题，发现一个系统如果拆分了太多业务类服务，或者依赖于大量的第三方服务，就很容易因为某个服务的故障导致整个系统不可用，比如 模块中使用了 Elastic Search 进行监控，但是 ES 突然挂了，相关的 api 的调用报错导致级联的服务全部阻塞，那么应该要有规避由 ES 调用 raise 出的异常或者调用超时而导致整个模块或整个系统崩溃的保护措施。 使用 AWS 或 阿里云 的 ECS 服务来作为 micro-service 的载体，但是 ECS 服务故障或者过载了导致整个业务链无法正常进行，那么应有对应的降级或者限制调用频度的方案来进行保护。 ","date":"2018-02-01","objectID":"/zh-cn/service-fallback/:1:1","tags":["Service","Architecture","Linux"],"title":"坦率地讲 服务熔断 \u0026 服务降级","uri":"/zh-cn/service-fallback/"},{"categories":["Coding"],"content":"服务熔断 服务熔断和电路熔断是一个道理，如果一条线路电压过高，保险丝会熔断，防止出现火灾，但是过后重启仍然是可用的。 而服务熔断则是对于目标服务的请求和调用大量超时或失败，这时应该熔断该服务的所有调用，并且对于后续调用应直接返回，从而快速释放资源，确保在目标服务不可用的这段时间内，所有对它的调用都是立即返回，不会阻塞的。再等到目标服务好转后进行接口恢复。 熔断的方式有很多，最出名的奶飞的 hystrix 项目里有很全面的实践，这里便先列个比较偷懒的案例。 举个栗子， # Elastic search service decorator def api_trend(func): def wrapper(*args, **kwargs): # Call elastic search service to get api trend elastic_search_api_call() # Custom function return func(*args, **kwargs) return wrapper # Custom task to do stuff @api_trend def custom_func(foo): retrun foo() 假设代码中的 @api_trend 是个调用 Elastic Search 服务来监控 api 执行情况的装饰器，那么如果 Elastic Search 服务挂了，则后续的 custom_func(foo) 也不会成功执行或者被阻塞。所以我们需要做的就是阻止后续的程序继续调用 @api_trend 或者 elastic_search_api_call() 这两位老哥，把 custom_func(foo) 隔离开，这样虽然暂时失去了监控，但是仍能保证业务能正常执行。 所以基于这点，我们可以简单地加个熔断控制器开关来隔离故障接口。 from threading import Timer # Melt down flag FUSE = True # Melt down recover func def recover(): FUSE = True return # Melt down decorator def melt_down(threshold=5, inteval=60, timeout=300, recover_time=3600): def wrap_melt(func): def wrapper(*args, **kwargs): is_fuse = True while threshold \u003e 0 and is_fuse: try: func(timeout, *args, **kwargs) is_fuse = False exception Exception, e: is_fuse = True threshold -= 1 continue time.sleep(inteval) FUSE = is_fuse if not FUSE: tr = threading.Timer(recover_time, recover) tr.start() return FUSE return wrapper return wrap_melt # Elastic search service decorator def api_trend(func): def wrapper(*args, **kwargs): # Call elastic search service to get api trend if FUSE: elastic_search_api_call() # Custom function return func(*args, **kwargs) return wrapper # Custom task to do stuff @melt_down @api_trend def custom_func(foo): return foo() 通过在调用 @api_trend 之前加上熔断控制器，进行目标服务的接口调用，如果在规定的重试次数内均未成功，则认为该服务在这一段时间内不可用，对于该 api 的所有调用全都用一个 FUSE_FLAG 进行隔离，并且设置一个定时 Thread, 在一定时间后重新打开 FUSE_FLAG，恢复目标服务的调用。 ","date":"2018-02-01","objectID":"/zh-cn/service-fallback/:1:2","tags":["Service","Architecture","Linux"],"title":"坦率地讲 服务熔断 \u0026 服务降级","uri":"/zh-cn/service-fallback/"},{"categories":["Coding"],"content":"服务降级 当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。 对于复杂系统而言，会有很多的微服务通过 rpc 调用，从而产生一个业务需要一条很长的调用链，其中任何一环故障了都会导致整个调用链失败或超时而导致业务服务不可用或阻塞。 这种情况下，可以暂时去掉调用链中故障的服务来进行降级，其中降级策略又有很多种，比如限流，接口拒绝等，这里就挑个简单的来举栗。 比如一个电商系统，用户模块，商品模块，订单模块，支付模块，物流模块分别是5个存在相互依赖性的服务，但是如果用户要下单购买个商品则可能需要一条长调用链依次 Call 到这5个模块。 # Call chain user = UserModule.sender.get_user() product = ProductModule.sender.get_product(user.selected) order = OrderModule.sender.post_order(product) payment = PaymentModule.sender.post_payment(order) logistics = LogisticsModule.sender.post_logistics(payment) 这时候如果物流模块崩了，那么很可能在最终购买商品的流程会被回滚，导致用户购买商品不成功，然而实际上，物流模块即便失效，仍应允许进行商品查看，下单，购买等，所以，坦率地讲，我们应该对这5个模块进行一个上下游依赖的剥离，使之变为纯净的 rpc 调用。 简单地说， from xmlrpclib import ServerProxy MODULE_TO_ENABLE = [ 'UserAgent', 'ProductAgent', 'OrderAgent', 'PaymentAgent', 'LogisticsAgent' ] def custom_call(): return foo() def call_nothing(): return class LogisticsAgent(object): self.sender = ServerProxy(\"http://{host}:{port}\".format(host=host, port=port)) if self.__class__.__name__ in MODULE_TO_ENABLE: self.sender.call = custom_call else: self.sender.call = call_nothing pass # Call chain if self.current_agent not in MODULE_TO_ENABLE: pass 这样通过 diable Call chain 中不重要的一环来确保其他模块可以正常使用。 ","date":"2018-02-01","objectID":"/zh-cn/service-fallback/:1:3","tags":["Service","Architecture","Linux"],"title":"坦率地讲 服务熔断 \u0026 服务降级","uri":"/zh-cn/service-fallback/"},{"categories":["Coding"],"content":"浅谈 Workflow 设计 LexusLee ","date":"2017-12-03","objectID":"/zh-cn/workflow-design/:1:0","tags":["Python","Workflow"],"title":"浅谈 Workflow 设计","uri":"/zh-cn/workflow-design/"},{"categories":["Coding"],"content":"背景 最近刚接触到 workflow 相关的东西，之前都没有造过这方面的轮子，所以看了一些框架总结了一下我认为的好的 Workflow 的设计应该是怎样的。 ","date":"2017-12-03","objectID":"/zh-cn/workflow-design/:1:1","tags":["Python","Workflow"],"title":"浅谈 Workflow 设计","uri":"/zh-cn/workflow-design/"},{"categories":["Coding"],"content":"什么是 Workflow ? Workflow 是一些可重复执行的事件按特定的顺序\u0026路径组合成的事件流，这个组成的事件流通常是为了满足某一个流程较长的任务。 这些事件通常是不可再被细分，是具有原子性的。每个原子事件可能包含执行任务、文档或数据。这些事件按照提前声明好的规则组合起来就成了一个 Workflow . e.g. 如上图所示，Workflow 类似软件工程中的流程图，指定了每个节点可能出现的路径分支，节点执行的事情以及节点的终结状态。 ","date":"2017-12-03","objectID":"/zh-cn/workflow-design/:1:2","tags":["Python","Workflow"],"title":"浅谈 Workflow 设计","uri":"/zh-cn/workflow-design/"},{"categories":["Coding"],"content":"如何设计 Workflow ? 比较经典的 Workflow design pattern 应该满足以下几个元素： 路径覆盖 事件原子性 有效的状态迁移 路径覆盖 路径覆盖是和节点状态相关的，通常的节点状态有如下几种： Start —— 开始 jobs ，标明 workflow 起点 Maybe —— 表示这个任务可能会执行，但不一定会执行，它的执行依赖于一定条件，比如上层节点的输出 Likely —— 和 Maybe 节点类似，但是比它的优先级更高，是作为与 Maybe 节点共享父节点的默认路径节点 Future —— 表示 workflow 执行体认为该路径一定会到达的节点，Future 节点的任务在不被 cancel 的情况下一定会执行 Waiting —— 表示当前任务是个阻塞任务，还在执行中，需要等待执行完毕才能进入下个路径 Ready —— 表示 Waiting 节点的任务已执行完，作为 Waiting 节点的 handler Complete —— 表示整个 workflow 的 Jobs 已经全部执行完毕，为终结节点 Cancel —— 表示任务被明确终止了，在状态迁移过程中不作为最终状态 事件原子性 pass ","date":"2017-12-03","objectID":"/zh-cn/workflow-design/:1:3","tags":["Python","Workflow"],"title":"浅谈 Workflow 设计","uri":"/zh-cn/workflow-design/"},{"categories":["Movie"],"content":"《向阳处的她》浩介先生终于有猫啦！ 有一年的搞笑诺贝尔物理学奖颁给了课题： 「一只猫能同时处于固体状态和液体状态吗？」 通过第三人称视角研究了猫的各种形态，《向阳处的她》这部电影就给研究了一个狠狠的耳光。 初看预告的时候，我觉得剧情有点俗气了，尽是那些小年轻喜欢听的清新治愈系故事。 从大笨钟一样有仪式感的电车中眺望初冬阳光下的漫无边际的江之岛，海鸥擦着行人的帽檐飞过，整个江之岛臣服在他们身后清冽的疾风中，满心都是光怪陆离的感觉。 在整部电影边缘哑光的胶片镜头下，这种自然和工艺碰撞的部分，让男主浩介和女主真绪在一起的每一刻回忆都清晰起来。 想起了有间破旧的猫屋，在清晨穿过黑黢黢的山路，第一次遇见了那只俄罗斯蓝猫，灰扑扑毛绒绒的。 雾气和露水都凉，它却暖乎乎的，抱在手里，甜蜜轻佻地抬头望着你。 这样的眼波，实在太熟悉了。 这也算每个猫奴的终极梦想吧， 能和自己的主子一起享受这些瞬间，吃酒抱猫，真是快意。 不过作为一只猫来说，它能记住的东西寥寥。 真绪也是。 真绪和浩介只能讲一个阅后即焚的故事，讲完了，便谁也记不得了。 一整个篇幅的起承转合喜怒哀乐，长的看不见头，甜的美不胜收，却就这么消失了，空虚极了。 所以，我喜欢一切永恒的事物，结局一定要是完美的，相逢在爱乐之城最后淡蓝色的酒吧里，高司令一定要冲下去，拉着她的手，说出另一个时间线里的那句话，“天呐我的老伙计啊我们在一起吧我再也不要错过你！” 可惜不是，高司令最后那张波澜不惊的脸，似笑非笑地看着错过的美景，却没有冲下去至少道个别。 我讨厌这种隔岸观火的结局，这让人感觉所有的付出都像是徒劳。 那些不再提起的好或者坏，等于从未发生过。 我想浩介一开始，也不享受这种的感觉。 所以他和我一样，本能地抗拒这种别离。 一想到过去一起亲历的风景会变成大脑中的未解之谜，就忍不住仓皇地逃离。 你看，这就是不通达的人啊。 我常常会纠结于那些曾经走过的路，铺过的床，抽过的耳光，夜半的心慌，愚蠢的结果和湿不开的枕巾。 却忘了任何一种情绪的发生，都是当下的体验，那些快乐的、痛苦的、失望的，早就驻留在我记忆中很久很久了。 「“即使回忆会消失，还是要努力去经历创造回忆的过程啊”」 「要当人类就必须做一些徒劳的事吧”」 日落时的余晖在湖面闪烁出莹莹的水光，有关真绪的回忆像水波一样散去。 突然就理解了人生最漫长而陈杂的部分，我们带着好奇心一步步走来，也多么希望有些事情能够变成永恒的，即便终是徒劳的，但亲爱的至少有机会和你一起经历了一段记忆。 可能只一句话或者一个动作，挺淡的，不至于教人常常想起。但只要想起来就觉得心头一暖，可以支撑我走过很多很冷的日子。 ​​​​ 就像片尾曲一样，Wouldn’t it be nice. ","date":"2017-09-25","objectID":"/zh-cn/girl-in-the-sunny-place/:1:0","tags":["Gossip","Movie"],"title":"《向阳处的她》浩介先生终于有猫啦！","uri":"/zh-cn/girl-in-the-sunny-place/"},{"categories":["Coding"],"content":"关于关闭 Socket 的一些坑 LexusLee ","date":"2017-09-06","objectID":"/zh-cn/tcp-close-socket/:1:0","tags":["TCP","Linux"],"title":"关于关闭 Socket 的一些坑","uri":"/zh-cn/tcp-close-socket/"},{"categories":["Coding"],"content":"背景 最近踩到一个 “Socket 连接持续处于 Fin_Wait2 和 Close_Wait 状态无法关闭” 的坑中。起因是在维护大量连接时调用 socket.close() 时，看到部分连接并没有正常关闭，而是从 ESTABLISHED 的状态变成 FIN_WAIT2 并且连接状态没有后续迁移，而对端的连接状态则是从 ESTABLISHED 变成了 CLOSE_WAIT 。 后来发现这和 TCP/IP 栈的4次挥手断开连接有关，列出一些踩坑时的收获。 ","date":"2017-09-06","objectID":"/zh-cn/tcp-close-socket/:1:1","tags":["TCP","Linux"],"title":"关于关闭 Socket 的一些坑","uri":"/zh-cn/tcp-close-socket/"},{"categories":["Coding"],"content":"Socket 连接关闭的流程 先看一张 Socket 关闭连接的状态迁移路径图: 在 Client 端调用 socket.close() 时，首先会往对端(即 Server 端)发送一个 FIN 包，接着将自身的状态置为 FIN_WAIT1 ，此时主动关闭端(即 Client 端)处于持续等待接收对端的响应 FIN 包的 ACK 回应状态，此时对端的状态是处于 ESTABLISHED ，一旦收到了 Client 发来的 close 连接请求，就回应一个 FIN 包，表示收到该请求了，并将自身状态置为 CLOSE_WAIT，这时开始等待 Server 端的应用层向 Client 端发起 close 请求。 这时 Client 端一旦收到 Server 端对第一个 FIN 包的回应 ACK 就会将进入下一个状态 FIN_WAIT_2 来等待 Server 发起断开连接的 FIN 包。在FIN_WAIT_1 的 time_wait 中， Server 端会发起 close 请求，向 Client 端发送 FIN 包，并将自身状态从 CLOSE_WAIT 置为 LAST_ACK ，表示 Server 端的连接资源开始释放了。同时 Client 端正处于 FIN_WAIT2 状态，一旦接收到 Server 端的 FIN 包，则说明 Server 端连接已释放，接着就可以释放自身的连接了，于是进入 TIME_WAIT 状态，开始释放资源，在经过设置的 2个 MSL 时间后，状态最终迁移到 CLOSE 说明连接成功关闭，一次 TCP 4次挥手 关闭连接的过程结束。 通常会出现状态滞留的情况有下面几种: Client 处于 FIN_WAIT1 , Server 处于 ESTABLISHED =\u003e 这种情况通常是连接异常，socket.close() 发送的 FIN 包对端无法收到。由于 TCP FIN_WAIT 自身有 Timeout, 在 Timeout 后如果还没有收到响应，则会停止等待。这种情况在 DDoS 攻击中比较常见，Server 端在某一时刻需要处理大量 FIN_WAIT1 时就会卡死。解决方法是修改 /etc/sysctl.conf 的 net.ipv4.tcp_fin_timeout 来提高 Timeout 值，保证大量连接能正常在超时时间内收到响应，当然这对服务器负载有要求。而如果是异常 ip 在某时间段内发送大量流量的 DDoS 攻击，则可以在 iptable 上手动封 ip 或者开启防火墙。 Client 处于 FIN_WAIT2, Server 处于 CLOSE_WAIT =\u003e 这种情况通常是 Server 端还在使用连接进行读写或资源还未释放完，所以还没主动往对端发送 FIN 包进入 LAST_ACK 状态，连接一直处于挂起的状态。这种情况需要去检查是否有资源未释放或者代码阻塞的问题。通常来说 CLOSE_WAIT 的持续时间应该较短，如果出现长时间的挂起，那么应该是代码出了问题。 Client 出于 TIME_WAIT, Server 处于 LAST_ACK =\u003e 首先 TIME_WAIT 需要等待 2个 MSL (Max Segment Lifetime) 时间，这个时间是确保 TCP 段能够被接收到的最大寿命。默认是 60 s 。解决方案是: 1. 调整内核参数 /etc/sysctl.conf 中的 net.ipv4.tcp_tw_recycle = 1 确保 TIME_WAIT 状态的连接能够快速回收，或者缩短 MSL 时间。 2. 检查是否有些连接可以使用 keepalive 状态来减少连接数。 此外，如果在单台服务器上并且不做负载均衡而处理大量连接的话，可以在 /proc/sys/net/ipv4/ip_local_port_range 中减少端口的极限值，限制每个时间段的最大端口使用数，从而保证服务器的稳定性，一旦出现大量的 TIME_WAIT 阻塞后续连接，是比较致命的。 ","date":"2017-09-06","objectID":"/zh-cn/tcp-close-socket/:1:2","tags":["TCP","Linux"],"title":"关于关闭 Socket 的一些坑","uri":"/zh-cn/tcp-close-socket/"},{"categories":["Coding"],"content":"Socket.terminate() 和 Socket.close() 此外还遇到了另一个小问题，在关闭连接时，一开始用的是 socket.terminate() ，然而 netstat 时却发现大量连接没有释放，后来发现 Python Socket 的 terminate() 只是发送 socket.SHUT_WR 和 socket.SHUT_RD 来关闭通道的读写权限而并没有释放连接句柄。导致了连接已经无法使用，但仍然处于 ESTABLISHED 状态。 解决方法就是使用 socket.close() 来替换 socket.terminate() 后来又看到如果是 DDoS 攻击的话，可能会阻塞住 socket.close() ，导致后续连接未关闭，大量流量进入服务器。 所以比较好的方式是在 socket.close() 之前先调用 socket.terminate() 关闭通道的读写权限，再调用 socket.close() ","date":"2017-09-06","objectID":"/zh-cn/tcp-close-socket/:1:3","tags":["TCP","Linux"],"title":"关于关闭 Socket 的一些坑","uri":"/zh-cn/tcp-close-socket/"},{"categories":["Design"],"content":"Sketch 锤子水箱图标仿制心得 ","date":"2017-08-22","objectID":"/zh-cn/sketch-icon-understanding/:1:0","tags":["Icon","Sketch"],"title":"Sketch 锤子水箱图标仿制心得","uri":"/zh-cn/sketch-icon-understanding/"},{"categories":["Design"],"content":"背景 第一次按照教程走了一遍，发现做出来的颜色、高光和饱和方面都很差，拟物化的感觉不是很明显。 ","date":"2017-08-22","objectID":"/zh-cn/sketch-icon-understanding/:1:1","tags":["Icon","Sketch"],"title":"Sketch 锤子水箱图标仿制心得","uri":"/zh-cn/sketch-icon-understanding/"},{"categories":["Design"],"content":"第二次修改心得 接着私信问了下大牛意见，再加上自己对比官方图，总结了几个修改点： 水箱外边框应该用深色来突显波纹的\"反光” 水箱边框的外阴影应该用过饱和的颜色来突出在黑色幕布下的力度 水箱内圈应该再加一层内嵌阴影来表达拟物化的层次感 气泡的构造！！！越深的气泡饱和度应该越高而透明度应该越低，这样贴近整个水波纹的颜色，而在水面的气泡容易和被水面的底材颜色覆盖，所以应该多加一层外边框做高亮，所以气泡应该分两层处理，水底和水面 icon 的波纹扭曲程度应该更夸张些 本身水箱整体的颜色是冷色系偏暗，所以 icon 颜色上应该有反差，不应该用原来黑色的，而应该换成白色的 下面是第二次修改后的图，整体感觉拟物化的效果好了一些 ","date":"2017-08-22","objectID":"/zh-cn/sketch-icon-understanding/:1:2","tags":["Icon","Sketch"],"title":"Sketch 锤子水箱图标仿制心得","uri":"/zh-cn/sketch-icon-understanding/"},{"categories":["Product"],"content":"LexusLee ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:0","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"背景 很久不用 Windows 开发，最近刚入职开发环境在 Windows 上，故列举了一下我自己开发机上安装的一些实用 app ，方便以后直接按清单上整个世界。 也算是一波安利了 ：D ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:1","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"IDE Pycharm (Python) PhpStorm (PHP) Webstorm (Javascript) Sublime + happypeter Sublime Config (Front-end) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:2","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"终端 Windows 的终端和系统命令真是 real 难用，从 Linux 切换过来很不习惯，气死了！！！ Rapid Environment Editor (Windows 环境变量配置工具，免去每次都到我的电脑里的繁杂操作) oh-my-zsh (一开始发现有相关的 hack 但是发现只适用于 Win10 ，由于Win7没有Bash 故删除) Cmder + Powershell (Terminal集成) Chocolatey (类似于 apt-get / yum 这种的包管理工具) Xshell (SSH登录) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:3","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"Python Tool pip (第三方库管理) virtualenv (Python 环境隔离) supervisor (Python 进程控制, 搭配 Nginx 食用更佳！) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:4","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"数据库工具 Navicat (一个GUI可同时管理MySQL, PgSQL 等主流数据库) DataGrip (JetBrains 的 SQL 神器，用来管理和编写 SQL ) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:5","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"测试 Postman (测试接口神器，我的 Chrome 是自带了) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:6","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"代码管理 Git SourceTree (Git GUI) Beyond Compare(神器，对比查看文件修改) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:7","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"GTD 管理 我一直保持着 GTD 记录的习惯，所以蛮依赖这些 app 的，因人而异吧，有些人喜欢自己管理时间。 momentum (一个轻便的 chrome 插件，每次打开新 Tab 标签都能看到今天制定的任务)、 Doit.im (一个在线的 GTD 网站，免去多平台同步的麻烦，今年一直在用这个) Inkdrop (桌面便签提醒) Slack (主要用到我自己写的日程提醒 slack bot) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:8","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"听歌 主要是解决版权问题，所以使用了两个客户端。 网易云音乐 (国内) Spotify (国外) Pandora (国外, 解决 Spotify 地域限制问题) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:9","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"视频 VLC (开源垃圾桶 media app) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:10","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"文档写作 Typora (Markdown写作) 印象笔记 (主要是用到它的多平台同步，用于和我自己的电脑同步文档) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:11","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Product"],"content":"让你的 Windows 体验更佳！ 敲黑板！！ 下面这些工具可能直接和开发工具不相关，但是可以提高开发速度和开发心情！感觉 Windows 搭配这些 app 用起来就和柯基的屁股一样滑。 这里是我参照少数派上的一篇文章: 这 8 款应用，让 Windows 也拥有 OS X 的优秀特性 整理的我自己比较喜欢的第三方系统增强应用，让 Windows 用起来体验超爽！ 公司的内网自带科学上网，真是超级 Nice ！所以我就不说 赛风 psiphon3 (一个支持多平台的免费的科学上网工具) Wox + Everything (类似于Win10小娜和 MacOS 上的 Alfred，但是比小娜好处在于更准确的 Everything 的多磁盘检索，并且可以自定义插件，像我这种电影狂热分就很需要豆瓣电影的插件) Seer (按一下空格即可进行文件快速预览，在文件命名很杂乱时有用🏊) MacType (把 Windows 超难看的默认宋体字体改成了渲染后圆滑的 Console 字体) Virgo (虚拟桌面，仿 MacOS 分屏使用，当一个桌面应用多到放不下，就按下 ALT+1/2/3/4 轻松切屏 ) AutoHotKey (用于自定义快捷键，把app里那些反人类的快捷键赶紧改了吧！) F.lux (自动调节屏幕亮度) ","date":"2017-07-08","objectID":"/zh-cn/awesome-windows-apps/:0:12","tags":["Productivity","Awesome"],"title":"Awesome Windows Apps","uri":"/zh-cn/awesome-windows-apps/"},{"categories":["Coding"],"content":"PySpark 词频统计搜索引擎的设计思路 记录下我的思路 ","date":"2017-05-09","objectID":"/zh-cn/spark-search-engine/:1:0","tags":["Spark","Architecture"],"title":"PySpark 词频统计搜索引擎的设计思路","uri":"/zh-cn/spark-search-engine/"},{"categories":["Coding"],"content":"Whoosh + jieba 中文检索 ","date":"2017-04-26","objectID":"/zh-cn/whoosh-search/:1:0","tags":["Python","Whoosh","Jieba"],"title":"Whoosh+jieba 中文检索","uri":"/zh-cn/whoosh-search/"},{"categories":["Coding"],"content":"背景 最近项目要用到 Whoosh 一个 Python 编写的索引检索模块，发现比较少中文资料并且看了学长的代码也好多不懂，故自己照着官网文档撸了一遍，把我自己的理解和官网一些不太清楚的解释写下来。 ","date":"2017-04-26","objectID":"/zh-cn/whoosh-search/:1:1","tags":["Python","Whoosh","Jieba"],"title":"Whoosh+jieba 中文检索","uri":"/zh-cn/whoosh-search/"},{"categories":["Coding"],"content":"快速上手 几个核心对象 Index 和 Schema 对象 在使用 Whoosh 前，首先需要创建的就是 index 对象，index 对象是一个全局索引。在创建 index 对象前首先要声明 index 对象的一些属性，所以需要在创建一个用于包装这些属性的 schema 对象。schema 有很多 Fields(一个 Field 是 index 对象的一个信息块，即需要被我们检索的内容) 举个栗子，以下代码创建了一个包含 “title” 和 “path” 和 “content” 三个 Fields 的 schema 对象 from whoosh.fields import Schema, TEXT, ID schema = Schema(title=TEXT, path=ID, content=TEXT) 创建 schema 对象时需要用关键字来映射 Field name 和 Field type，如上的 title=TEXT 一旦创建好了 schema 对象，接着就是使用 create_in 方法来创建 schema 的索引 import os.path from whoosh.index import create_in if not os.path.exists(\"index\"): os.mkdir(\"index\") idx = create_in(\"index\", schema) 接着可以用以下两种方法打开一个已创建的索引 # 方法一 使用FileStorage对象 from whoosh.filedb.filestore import FileStorage storage = FileStorage(idx_path) #idx_path 为索引路径 idx = storage.open_index(indexname=indexname, schema=schema) # 方法二 使用open_dir函数 from whoosh.index import open_dir idx = open_dir(indexname=indexname) #indexname 为索引名 IndexWriter 对象 一旦有了 index 对象，我们就需要在 index 里写入需要被检索的信息，所以 IndexWriter 对象就是用来提供一个 add_document(**kwargs) 方法来在之前声明的各种 Fields 里写入数据 writer = idx.writer() #IndexWriter对象 writer.add_document( title=u\"Document Title\", path=u\"/a\", content=u\"Hello Whoosh\" ) # Field 和 schema 中声明的一致 writer.commit() # 保存以上document 需要注意的是： 不是每个 Field 都要赋值 Field 传值一定是 unicode 类型的值 如果有一个 Field 同时要被当做索引并保存之，那么可以用一个 unicode 值来做索引同时保存另一个对象 writer.add_document(title=u\"Title to be indexed\", _stored_title=u\"Stored title\") 如果需要异步处理可以创建异步的 IndexWriter 对象 from whoosh.writing import AsyncWriter writer = AsyncWriter(index=index) 如果需要Buffer进行处理可以创建 BufferedWriter 对象 from whoosh.writing import BufferedWriter # period是多次commit的最大间隔时间，limit是需要缓存的最大数量 writer = BufferedWriter(index=index, period=120, limit=20) Searcher 对象 在开始搜索索引之前，我们需要创建 searcher 对象 searcher = idx.sercher() 但是一般来说不会这么创建搜索器 searcher ，这样做没法来索引检索完成后关闭搜索器释放内存(只要知道 searcher 很吃内存就行)，我们一般用 with 来创建 searcher 对象从来保证搜索器使用完毕后可以被正确关闭 with idx.sercher() as searcher: ... 以上写法等同于 try: searcher = idx.searcher() ... finally: searcher.close() 搜索器的 search() 方法需要传入一个 Query 对象，我们可以直接构造一个 Query 对象或者使用 query parser 来解析一个查询字段 举个栗子 # 直接构造查询对象 from whoosh.query import * myquery = And([Term(\"content\", u\"apple\"), Term(\"content\", \"bear\")]) 默认的 QueryParser 允许使用查询原语 AND 和 OR 和 NOT 就像 SQL 一样简单！ # 使用解析器解析查询字段 from whoosh.qparser import QueryParser parser = QueryParser(\"content\", idx.schema) myquery = parser.parse(querystring) 构造完查询对象后，就可以使用搜索器的 search() 方法来进行检索 results = searcher.search(myquery) print(results[0]) {\"title\": \"Document\", \"content\": \"Hello Whoosh\"} 更通常的我们使用分页查询 search_page() 的方法 results = searcher.search_page(myquery, page_num, page_len) 结合 jieba 分词使用 Whoosh 的基本用法如上，接着我要在 QueryString 中加入结巴分词分析模块 由于 jieba 0.30 之后的版本已经添加用于 Whoosh 的分词接口: ChineseAnalyzer, 所以还是很方便的 首先在 Whoosh schema 对象的创建的 whoosh.fields.TEXT，默认的声明 TEXT 时字段的 FieldAttributes 默认有个属性 analyzer analyzer 是一个带有 call 魔术方法的类，用来进行 TEXT 词域的分析，在调用时会把 TEXT 域里的值进行 call 处理 analyzer 接收的参数是一个 unicode 字符串，返回值是字符串切分，举个栗子 e.g.( ​ param = “Mary had a little lamb” ​ return = [“Mary”, “had”, “a”, “little”, “lamb”] ) 使用的是 Whoosh 的 StandardAnalyzer ，是英文的分词器。为了对接上 jieba，做中文分词，需要把 TEXT(analyzer=analysis.StandardAnalyzer()) 换成 jieba 的 ChineseAnalyzer 即可 from __future__ import unicode_literals from jieba.analyse import ChineseAnalyzer analyzer = ChineseAnalyzer() schema = Schema(title=TEXT(stored=True), path=ID(stored=True), content=TEXT(stored=True, analyzer=analyzer)) idx = create_in(\"test\", schema) writer = idx.writer() writer.add_document( title=\"test-document\", path=\"/c\", content=\"This is the document for test\" ) writer.commit() searcher = idx.searcher() parser = QueryParser(\"content\", schema=idx.schema) for keyword in (\"水果\",\"你\",\"first\",\"中文\",\"交换机\",\"交换\"): print(\"result of \",keyword) q = parser.parse(keyword) results = searcher.search(q) for hit in results: print(hit.highlights(\"content\")) print(\"=\"*10) 还是很方便的。 ","date":"2017-04-26","objectID":"/zh-cn/whoosh-search/:1:2","tags":["Python","Whoosh","Jieba"],"title":"Whoosh+jieba 中文检索","uri":"/zh-cn/whoosh-search/"},{"categories":["Coding"],"content":"终端配置总结 ","date":"2017-03-30","objectID":"/zh-cn/terminal-config/:1:0","tags":["Linux","Config","Terminal"],"title":"终端配置总结","uri":"/zh-cn/terminal-config/"},{"categories":["Coding"],"content":"背景 最近公司新购置了好几台 Linux 服务器然后配置一些服务的时候很不习惯，估计是我平时自己的 zsh + oh-my-zsh 用多了，故想整理下 .bash_rc 和 .zshrc 我个人的一些配置，这些配置包含了一些 alias 快捷命令和命令行系统配置，可以让终端变得快捷易用，今晚再写个 shell 脚本实现快速修改 .bash_rc 的配置。 ","date":"2017-03-30","objectID":"/zh-cn/terminal-config/:1:1","tags":["Linux","Config","Terminal"],"title":"终端配置总结","uri":"/zh-cn/terminal-config/"},{"categories":["Coding"],"content":".bash_rc/.zshrc 配置汇总 终端不自动执行命令 # If not running interactively, don't do anything case $- in *i*) ;; *) return;; esac .bash_history 文件(同理 .zsh_history )不重写而是使用附加模式记录命令 # append to the history file, don't overwrite it shopt -s histappend 增加 .bash_history 、 .zsh_history 文件记录阈值，超过阈值后自动清空 HISTFILESIZE=2000 根据命令行长短自动调节终端行列显示排版 # check the window size after each command and, if necessary, # update the values of LINES and COLUMNS. shopt -s checkwinsize 开启适合编程的命令行提示 feature if ! shopt -oq posix; then if [ -f /usr/share/bash-completion/bash_completion ]; then . /usr/share/bash-completion/bash_completion elif [ -f /etc/bash_completion ]; then . /etc/bash_completion fi fi 常用的 alias 声明 — 简写部分 比较容易懂。 alias cls='clear' alias vi='vim' alias ll='ls -l' alias la='ls -a' 常用的 alias 声明 — 效果增强部分 这部分让 ls 和 grep 都带有关键字亮色的提示，让 alert 提示的错误信息在终端中显示起来更友好。 alias ls='ls --color=auto' alias grep='grep --color=auto' alias alert='notify-send --urgency=low -i \"$([ $? = 0 ] \u0026\u0026 echo terminal || echo error)\" \"$(history|tail -n1|sed -e '\\''s/^\\s*[0-9]\\+\\s*//;s/[;\u0026|]\\s*alert$//'\\'')\"' 常用的 alias 声明 — 效果增强部分2 这部分增加了在 MacOS 系统中显示和隐藏文件的快捷命令，和在终端中切换 bash 和 zsh 的快捷命令。 alias showfile='defaults write com.apple.finder AppleShowAllFiles -boolean true ; killall Finde r' alias hidefile='defaults write com.apple.finder AppleShowAllFiles -boolean false ; killall Find er' alias switchbash='chsh -s /bin/bash' alias switchzsh='chsh -s /bin/zsh' 全部的配置汇总如上，一是方便我自己做个备份，二是大家可以按需使用。 ","date":"2017-03-30","objectID":"/zh-cn/terminal-config/:1:2","tags":["Linux","Config","Terminal"],"title":"终端配置总结","uri":"/zh-cn/terminal-config/"},{"categories":["Gossip"],"content":"如果大脑有一套日志系统就好了 ","date":"2016-07-16","objectID":"/zh-cn/brain-log-system/:1:0","tags":["Gossip"],"title":"如何大脑有一套日志系统就好了","uri":"/zh-cn/brain-log-system/"},{"categories":["Gossip"],"content":"背景 最近很健忘，今天下午撸代码的时候，基本上每执行一个模块就要将调用函数、执行结果、友好信息等记录到日志系统里。 然后我突然想到，如果生物的大脑有一套日志系统就好了! 数千年后，虽然不知道人类迭代更替得如何，但是到时候的智慧生物只需要读取我们这种远古生物的日志系统，就可以看到这个人(或者动物)的一生。 跟传记一样，像一部电影。 对于我这种极其害怕死了之后被遗忘的动物来说，实在很兴奋。 ","date":"2016-07-16","objectID":"/zh-cn/brain-log-system/:1:1","tags":["Gossip"],"title":"如何大脑有一套日志系统就好了","uri":"/zh-cn/brain-log-system/"},{"categories":["Gossip"],"content":"我是这么细想的 为啥？ 我如果想去了解某个已故的人，如果是名人，辣么可以找到他的传记或者史记等资料等，方式很多。 但如果是不出名并且也没有写自传的习惯的人，辣么我可能会去找他的后裔，然后通过不那么客观的角度来了解那个人。甚至，我可能根本找不到方式去了解这个人。 这对于不容易在历史上留名的人来说不公平。 我觉得每个人都值得被记录下来，平凡的人自然有平凡的人会去在乎，在他死后有渠道去查看他的一生。 咋整？ 在每个人刚出生的时候， 在身体上某个部位，这个部位随年龄递增缩产生的变动不大，在这个部位植入一块芯片。 (最佳选择)这块芯片能否通过某种信号，和大脑产生联系，从而能够自动记录日志。 (次之)如果不能与大脑产生联系，辣么就用手动录入日志。但是手动录入日志的话，可能有哪天我偷懒，或者我忘记了，而错过了关键信息。 这样一段时间(可能是一天，也可能是完成某件事情，或者触发了某个事件，或者未来时间的计数单位)结束，我能够在脑子里(回顾?)记录下我今天进行的行为(甚至可以细化到大脑的活动，产生的思维的记录？) 举个栗子： e.g.( [2016.07.10 16:17 小虚 \u003e 由于写代码时烦躁分心胡思乱想产生了奇怪的想法，并且怕自己健忘，于是在\"简书\"边听着 Adam Levine 的歌边写了一篇名为《如果大脑有一套日志系统就好了》的 blog ] ) 其中记录方式，可能是通过在脑袋里思考时产生的某种信号，解析后在芯片里进行读写… (进入瞎编模式！) 估计实现不了，辣么手工录入日志还是比较容易的！ 虽然这种方式不很便捷。 这就会产生一个问题：芯片容量的问题。 万一遇到话唠型选手咋整？ 一天能整个 10 万字的大新闻。 我一开始的想法是容量尽量大的芯片，但是这样成本高，不如限制字数吧。 正好我喜欢情多话少的 ：D 最后， 在人死后，焚烧之前，取出这块芯片，使用某种储存方式保存起来。 数千年后，人们想了解我，只需要读取这块芯片，就能知道我每一天做了什么，我在想什么… 可能出现的问题 记录日志时带有不真实性。也就是说，你甚至可以胡扯出今天一天做了什么事儿。比如我可以说: 我今天和高圆圆一起去看了《大鱼海棠》，我感到小鹿乱撞。 在婴儿期，还没有成形的意识，以及在老年期，意识不清晰了，无法记录日志，这段时间的日志是空白的。 语言问题。这个好说，现在翻译引擎很屌了。 是否应该允许他人能编辑本人的日志？不应该。 日志系统是个芯片，总有办法被摧毁的，如果遇到恶性事件，那是无可奈何的。 具体如何记录的实现问题。 嗯，就酱！ ","date":"2016-07-16","objectID":"/zh-cn/brain-log-system/:1:2","tags":["Gossip"],"title":"如何大脑有一套日志系统就好了","uri":"/zh-cn/brain-log-system/"},{"categories":["Coding"],"content":"背景 最近接触到用 Twisted 来写个 RPC 服务器，对高并发、性能和大量长连接时的稳定性方面有要求，所以应该在 Twisted 的基础上再造些轮子，最后考虑用 Twisted + gevent 来实现 「异步+协程」的部分。 分别简要介绍下 Twisted 和 gevent。 ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:1:0","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"Twisted Twisted是用 Python 实现的基于事件驱动的异步的网络引擎框架。它封装了大部分主流的网络协议(传输层或应用层)，如 TCP、UDP、SSL/TLS、HTTP、IMAP、SSH、IRC以及FTP等，在这我主要会用到 TCP 协议。 使用 Twisted 的好处在于，它是以事件驱动编程实现的，所以提供了事件注册的回调函数的接口，每次接受到请求，获得了事件通知，就调用事件所注册的回调函数( Node.js 程序员可能比较熟悉)。这让我不必去操心服务器事件驱动的编写。 并且，在网络引擎方面，有心跳包和粘包的三方库，非常完善。 然而，Twisted 有一个缺陷，它的异步有点问题，单个连接建立后是一个进程，在进程里用多线程实现并发，但多个连接建立后仍然会出现同步阻塞的情况，所以这就要引入 gevent 来填充其性能上的缺陷。 ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:2:0","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"gevent gevent 是一种基于协程的 Python 网络库，它用到 greenlet 提供的，封装了 libevent 事件循环的高层同步API。 如果你不知道什么是协程，那么可以简单这么理解： 协程就是由程序员自己编码实现调度的多线程。 而 gevent 对 greenlet 协程进行了封装，同时 gevent 提供了看上去非常像传统的基于线程模型编程的接口，但是在隐藏在下面做的是异步 I/O ，所以它以同步的编码实现了异步的功能。 ##开搞 ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:3:0","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"Step 1 完成基础框架 首先由于我要编写一个 RPC 服务器(使用 TCP 协议)，所以需要先实现一个 TCP 服务器。 # server.py from twisted.internet.protocol import ServerFactory, ProcessProtocol from twisted.protocols.basic import LineReceiver from twisted.internet import reactor PORT = 5354 class CmdProtocol(LineReceiver): client_ip = '' # 连接建立接口 def connectionMade(self): # 获得连接对端 ip self.client_ip = self.transport.getPeer().host print(\"Client connection from %s\" % self.client_ip) # 连接断开接口 def connectionLost(self, reason): print('Lost client connection. Reason: %s' % reason) # 数据接收接口 def dataReceived(self, data): print('Cmd received from %s: %s' % (self.client_ip, data)) class RPCFactory(ServerFactory): # 使用 CmdProtocol 与客户端通信 protocol = CmdProtocol # 启动服务器 if __name__ == \"__main__\": reactor.listenTCP(PORT, RPCFactory()) reactor.run() Twisted 提供3个非常基础的接口使程序员进行重写: connectionMade() 连接建立后执行操作 connectionLost() 连接断开后执行操作 dataReceived() 接收到数据后触发操作 这3个接口通常来说是必须的，以此基础上进行完善，可以看到我只是先输出了友好信息。 这样简单完成了一个 TCP 服务器，可以看出 Twisted 网络引擎的架构如下： 先由程序员来制定一个或多个协议(该协议可以继承各种底层网络协议)。 接着指定唯一一个工厂，这个工厂必须声明使用的协议对象。 使用 reactor 选择监听模式、监听工厂和端口，开启服务器。 ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:3:1","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"Step 2 完善基础框架 显然，这个 TCP 服务器基础框架显得有些单薄，我首先想到的是需要进行多客户端的控制及 ip 记录，故应有个队列来实时更新连接入服务器的 ip。 并且，最近有好几部电影在豆瓣我标记了，我想和高圆圆一起去看，所以不能一直盯着屏幕来观察反馈，所以需要一个日志系统来记录反馈信息。 故增加一个 log.py 日志系统文件： # log.py import os import logging import logging.handlers from twisted.python import log #当前执行文件所在地址 CURRENT_PATH = os.getcwd() #日志文件路径 LOG_FILE = CURRENT_PATH + '/rpcserver.log' # 全局日志模块 gl_logger = None class log(Protocol): def init_log(): global gl_logger try: os.makedirs(os.path.dirname(LOG_FILE)) except: pass # 实例化handler handler = logging.handlers.RotatingFileHandler(LOG_FILE, maxBytes=1024 * 1024, backupCount=1) fmt = '[%(asctime)s][%(levelname)s][%(filename)s:%(lineno)d:%(funcName)s] - %(message)s' # 实例化formatter formatter = logging.Formatter(fmt) # 为handler添加formatter handler.setFormatter(formatter) # 获取名为rpcserver的logger gl_logger = logging.getLogger('rpcserver') # 为logger添加handler loggergl_logger.addHandler(handler) handlergl_logger.setLevel(logging.DEBUG) gl_logger.info(\"----------------------------------\") 并在 server.py 中添加如下代码： (添加多连接控制，把 print 替换为 log.msg 来打印日志) # server.py from twisted.internet.protocol import ServerFactory, ProcessProtocol from twisted.protocols.basic import LineReceiver from twisted.internet import reactor from twisted.python import log PORT = 5354 class CmdProtocol(LineReceiver): client_ip = '' # 连接建立接口 def connectionMade(self): # 获得连接对端 ip self.client_ip = self.transport.getPeer().host log.msg(\"Client connection from %s\" % self.client_ip) # 进行多连接控制 if len(self.factory.clients) \u003e= self.factory.clients_max: log.msg(\"Too many connections. Disconnect!\") self.client_ip = None self.transport.loseConnection() else: self.factory.clients.append(self.client_ip) # 连接断开接口 def connectionLost(self, reason): log.msg('Lost client connection. Reason: %s' % reason) if self.client_ip: self.factory.clients.remove(self.client_ip) # 数据接收接口 def dataReceived(self, data): log.msg('Cmd received from %s: %s' % (self.client_ip, data)) class RPCFactory(ServerFactory): # 使用 CmdProtocol 与客户端通信 protocol = CmdProtocol # 设置最大连接数 def __init__(self, clients_max=10): self.clients_max = clients_max self.clients = [] # 启动服务器 if __name__ == \"__main__\": log.startLogging(sys.stdout) reactor.listenTCP(PORT, RPCFactory()) reactor.run() ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:3:2","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"Step 3 增加 rpc 实例 既然是 rpc 服务器，辣么接下来就要实现一个简单的远程命令调用，既然之前写了日志模块，那就写一个对应的远程日志查看调用吧！ 对了，写到这里，已经是 02：53 了，我不知道为什么开始胡思乱想起来。 我想大概是因为越是无端的，越是会心念着… 嗷，跑题了… 远程调用呢， Twisted 提供了一个敲好用的子进程父类 ProcessProtocol 这个类提供了2个接口: outReceived 用来接收和外发数据 processEnded 进程结束回调 于是，我在 server.py 中加入以下代码: # 打印日志 class TailProtocol(ProcessProtocol): def __init__(self, write_callback): self.write = write_callback def outReceived(self, data): self.write(\"Begin logger\\n\") data = [line for line in data.split('\\n') if not line.startswith('==')] for d in data: self.write(d + '\\n') self.write(\"End logger\\n\") def processEnded(self, reason): if reason.value.exitCode != 0: log.msg(reason) 循环读取日志文件中每一行并输出信息。 接着在 CmdProtocol 类中加入以下函数: # 根据 cmd 执行相应操作 def processCmd(self, line): if line.startwith('getlog'): tailProtocol = TailProtocol(self.transport.write) # 打印rpcserver.log日志 reactor.spawnProcess(tailProtocol, '/usr/bin/tail', args=['/usr/bin/tail', '-10', '/var/log/rpcserver.log']) 通过获取远程发送来的命令 「getlog」 触发了以下事件 tailProtocol ，并调用 TailProtocol 类中的回调函数 outReceived 来循环读取日志文件中每一行并输出日志信息，返回给客户端。 同理，其余 RPC 远程调用实例也可类似的编写。 注意，这里使用了 Twisted 自带的 spawnProcess() 来处理事件回调，并新建一个线程来执行函数，这就是单个连接中并发的实现。 ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:3:3","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"Step 4 加入 gevent 协程部分 首先我考虑的是使用一个队列来储存每次接收到事件触发的钩子后，把钩子接收的参数存入队列中，再用 gevent 的协程来进行任务的分发。 直接上代码： # server.py import geventfrom gevent.queue import Queue # 任务队列 tasks = Queue() class CmdProtocol(LineReceiver): def worker(self, target): while not tasks.empty(): task = tasks.get() log.msg('User %sgot task %s' % (target, task)) self.processCmd(task) gevent.sleep(0) def dispatch(self, data): tasks.put_nowait(data) def dataReceived(self, data): log.msg('Cmd received from %s: %s' % (self.client_ip, data)) gevent.spawn(self.dispatch, data).join() gevent.spawn(self.worker, self.client_ip) 首先， gevent 的队列 Queue 有两个主要的方法 get() 和 put() 来对队列中的元素进行读和写。put_nowait() 相当于 put() 的无阻塞模式。 在 dispatch() 中，我把每个收到的 data 的 trigger 放入任务队列中，使其进入等待分发的状态。 接着，协程会执行下一步 worker()从任务队列中取出相应的 trigger ，传入 processCmd 中触发回调，执行相应的函数。 执行完后，协程会回到上一步 dispatch() 接着再到 worker() 这样交替轮循，直到任务列表里的任务全部执行完为止，这个过程中，各个任务执行是独立的，不会造成阻塞，吊！ ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:3:4","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"欧勒！ 就酱，我们撸出了一个高性能的、协程的、异步的 RPC 服务器！ ","date":"2016-07-04","objectID":"/zh-cn/rpc-server/:4:0","tags":["Python","Rpc","Twisted"],"title":"Twisted+gevent 异步+协程服务器开发","uri":"/zh-cn/rpc-server/"},{"categories":["Coding"],"content":"关联性和超链接 之前我们的 api 都是用外键关联，然而实际上用超链接的方式更符合 RESTful 的思想。 所以在这一章中我们将要用超链接(代替外键的方式)来提高关联性。 ","date":"2016-04-06","objectID":"/zh-cn/drf-tutorial-5/:1:0","tags":["Python","Django","RESTful"],"title":"Django REST Framework 5-关联性和超链接","uri":"/zh-cn/drf-tutorial-5/"},{"categories":["Coding"],"content":"为 api 提供根路径 由于要采用超链接的方式，而之前我们的 ‘movies’ / ‘directors’ / ‘users’ 虽然有了 endpoints ，但 api 本身却没有一个整体的根路径，所以我们使用 @api_view 装饰器来创建一个根路径。 在 douban/views.py 中添加如下代码: from rest_framework.decorators import api_view from rest_framework.response import Response from rest_framework.reverse import reverse # api 根目录 @api_view(['GET']) def api_root(request, format=None): return Response({ 'user': reverse('user-list', request=request, format=format), 'movies': reverse('movies-list', request=request, format=format), 'director': reverse('director-list', request=request, format=format) }) 在这里需要注意两样东西： 我们用了 DRF 的 reverse 方法而不是 Django 自带的 reverse 方法来返回一个正确的 URLs。 此时如果打开 Web api 界面会报错, 因为我们还没有为 url 进行绑定， 稍后我们会添加。 接着在 doubt/urls.py 中添加对应路径 url(r'^$', views.api_root), ","date":"2016-04-06","objectID":"/zh-cn/drf-tutorial-5/:1:1","tags":["Python","Django","RESTful"],"title":"Django REST Framework 5-关联性和超链接","uri":"/zh-cn/drf-tutorial-5/"},{"categories":["Coding"],"content":"使用炫酷的超链接 DRF 提供了以下几种方式来处理实体间的关系: 主键 超链接 相关项使用单一标识符 相关项默认文本信息 子项在母项中显示出来 其他方式 在这个栗子中我们使用超链接的方式来处理实体关系。 首先在序列器中使用 HyperlinkedModelSerializer 替代 ModelSerializer 注: HyperlinkedModelSerializer 和 ModelSerializer 有以下几点区别: 它没有主键域 ( pk field ) 它默认包含一个 url 域 关联时使用的是 HyperlinkedRelatedField 而不是 PrimaryKeyRelatedField 在 doubt/serializer.py 中进行如下改写: #!/usr/bin/python # -*- coding: utf-8 -*- from django.contrib.auth.models import User from rest_framework import serializers from douban.models import Movies, celebrity, COUNTRY_CHOICES, TYPE_CHOICES class MoviesSerializer(serializers.HyperlinkedModelSerializer): owner = serializers.ReadOnlyField(source='owner.username') director = serializers.HyperlinkedRelatedField(many=False, queryset=celebrity.objects.all(), view_name='director-detail') class Meta: model = Movies fields = ('id', 'title', 'director', 'year', 'country', 'type', 'rating', 'owner') class UserSerializer(serializers.HyperlinkedModelSerializer): movies = serializers.HyperlinkedRelatedField(many=True, view_name='movies-detail', read_only=True) class Meta: model = User fields = ('id', 'username', 'movies') class DirectorSerializer(serializers.HyperlinkedModelSerializer): movies = serializers.HyperlinkedRelatedField(many=True, view_name='movies-detail') class Meta: model = celebrity fields = ('id', 'name', 'age', 'gender', 'movies') ","date":"2016-04-06","objectID":"/zh-cn/drf-tutorial-5/:1:2","tags":["Python","Django","RESTful"],"title":"Django REST Framework 5-关联性和超链接","uri":"/zh-cn/drf-tutorial-5/"},{"categories":["Coding"],"content":"绑定 url 使用超链接 api 有个前提条件，我们需要确保 URL pattern 都已命名。 编辑 douban/urls.py 进行如下修改: #!/usr/bin/python # -*- coding: utf-8 -*- from django.conf.urls import url from rest_framework.urlpatterns import format_suffix_patterns from douban import views urlpatterns = format_suffix_patterns([ url(r'^$', views.api_root), url(r'^movies/$', views.MoviesList.as_view(), name='movies-list'), url(r'^movies/(?P\u003cpk\u003e[0-9]+)/$', views.MoviesDetail.as_view(), name='movies-detail'), url(r'^users/$', views.UserList.as_view(), name='user-list'), url(r'^users/(?P\u003cpk\u003e[0-9]+)/$', views.UserDetail.as_view(), name='user-detail'), url(r'^directors/$', views.DirectorList.as_view(), name='director-list'), url(r'^directors/(?P\u003cpk\u003e[0-9]+)/$', views.DirectorDetail.as_view(), name='director-detail'), ]) 确保每个 URL pattern 都正确的与 views.py 中对应视图的命名进行绑定。 ","date":"2016-04-06","objectID":"/zh-cn/drf-tutorial-5/:1:3","tags":["Python","Django","RESTful"],"title":"Django REST Framework 5-关联性和超链接","uri":"/zh-cn/drf-tutorial-5/"},{"categories":["Coding"],"content":"增加分页 对于大量的数据在单页显示体验很不好，所以要设置分页。 编辑 restapit/settings.py : REST_FRAMEWORK = { 'PAGE_SIZE': 10 } 现在用浏览器访问我们的 api 界面，不断地添数据，就可以看到分页效果辣。 ","date":"2016-04-06","objectID":"/zh-cn/drf-tutorial-5/:1:4","tags":["Python","Django","RESTful"],"title":"Django REST Framework 5-关联性和超链接","uri":"/zh-cn/drf-tutorial-5/"},{"categories":["Coding"],"content":"为什么使用超链接 因为用超链接的方式有个明确的指向，比如该栗子中 movies 的 director 字段由外键变为超链接的关联形式允许直接跳转到 director 的 api 页面。 ","date":"2016-04-06","objectID":"/zh-cn/drf-tutorial-5/:1:5","tags":["Python","Django","RESTful"],"title":"Django REST Framework 5-关联性和超链接","uri":"/zh-cn/drf-tutorial-5/"},{"categories":["Coding"],"content":"验证与授权 目前来看，我们的 API 并没有权限上的限制(即任何人都可以编辑或删除我们的 Movies )，这不是我们想要的。所以我们需要在 API 上做些限制以确保: Movies 与 Users 关联起来。 只有授权了的用户才能创建新的 Movies。 只有 Movies 的创建者才可以更新或删除它。 未授权的用户只能进行查看。 ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:0","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"在 models 中增加以下信息 我们先把之前注释掉的 director = models.ForeignKey('celebrity', related_name='Movies') class celebrity(models.Model): name = models.CharField(max_length=100, blank=True, default='') age = models.IntegerField() gender = models.CharField(choices=GENDER_CHOICES, default='male', max_length=20) 关联导演类的注释解开，来看看多张表在生成的 api 里的关联性。 接着在 models.py 中的 Movies 类中加入以下代码来确定 Movies 的创建者: owner = models.ForeignKey('auth.User', related_name='Movies') 最后 models.py 代码为: #!/usr/bin/python # -*- coding: utf-8 -*- from django.db import models # 举个栗子 COUNTRY_CHOICES = ( ('US', 'US'), ('Asia', 'Asia'), ('CN', 'CN'), ('TW', 'TW'), ) TYPE_CHOICES = ( ('Drama', 'Drama'), ('Thriller', 'Thriller'), ('Sci-Fi', 'Sci-Fi'), ('Romance', 'Romance'), ('Comedy', 'Comedy') ) GENDER_CHOICES = ( ('male', 'male'), ('female', 'female') ) class Movies(models.Model): title = models.CharField(max_length=100, blank=True, default='') year = models.CharField(max_length=20) # 在 director 关联了 Movies 类 和 celecrity 类, 在第4章会用到 celebrity 类 director = models.ForeignKey('celebrity', related_name='movies') # 关联 User 类来确定 Movies 的创建者 owner = models.ForeignKey('auth.User', related_name='movies') country = models.CharField(choices=COUNTRY_CHOICES, default='US', max_length=20) type = models.CharField(choices=TYPE_CHOICES, default='Romance', max_length=20) rating = models.DecimalField(max_digits=3, decimal_places=1) created = models.DateTimeField(auto_now_add=True) class Meta: ordering = ('created',) class celebrity(models.Model): name = models.CharField(max_length=100, blank=True, default='') age = models.IntegerField() gender = models.CharField(choices=GENDER_CHOICES, default='male', max_length=20) 修改完了模型，我们需要更新一下数据表。 通常来讲，我们会创建一个数据库 migration 来更新数据表，但是为了图省事儿，宝宝我索性删了整张 Movies 表直接重建！ 在数据库中删除 douban_movies 表后在终端中执行以下命令: $ python manage.py syncdb 接着我们可能会需要多个 User 来测试 API ，如果之前你没有创建 Django Super User 的话，用以下命令创建: $ python manage.py createsuperuser 然后进入 http://127.0.0.1/admin/ 界面，登录并找到 /user/ 表，然后在里面手动创建 user 并赋予权限。 ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:1","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"为新增的模型增加 endpoints 既然现在我们已经有了 users 模型和 celebrity 模型，那么现在需要做的就是在 serializer.py 中让他们在 API 中展现出来，加入以下代码: class UserSerializer(serializers.ModelSerializer): movies = serializers.PrimaryKeyRelatedField(many=True, queryset=Movies.objects.all()) class Meta: model = User fields = ('id', 'username', 'movies') class DirectorSerializer(serializers.ModelSerializer): movies = serializers.PrimaryKeyRelatedField(many=True, queryset=Movies.objects.all()) class Meta: model = celebrity fields = ('id', 'name', 'age', 'gender', 'movies') 因为我们之前在 models.py 中添加了 owner = models.ForeignKey('auth.User', related_name='movies') 其中 related_name 设置了可以通过 User.movies 来逆向访问到 movies 表。所以在 ModelSerializer 类中我们需要在 fields 中添加一个 movies 来实现逆向访问。同理 DirectorSerializer 类中也进行相应修改。 接着，我们还需要在 views.py 中添加相应的视图。 为 User 添加只读 API ，使用 ListAPIView 和 RetrieveAPIView 为 Director 添加读写 API ，使用 ListCreateAPIView 和 RetrieveUpdateDestroyAPIView class UserList(generics.ListAPIView): queryset = User.objects.all() serializer_class = UserSerializer class UserDetail(generics.RetrieveAPIView): queryset = User.objects.all() serializer_class = UserSerializer class DirectorList(generics.ListCreateAPIView): queryset = celebrity.objects.all() serializer_class = DirectorSerializer class DirectorDetail(generics.RetrieveUpdateDestroyAPIView): queryset = celebrity.objects.all() serializer_class = DirectorSerializer 最后，修改 urls.py 把视图关联起来，在 urlpatterns 中加入以下4个 patterns: urlpatterns = [ url(r'^users/$', views.UserList.as_view()), url(r'^users/(?P\u003cpk\u003e[0-9]+)/$', views.UserDetail.as_view()), url(r'^directors/$', views.DirectorList.as_view()), url(r'^directors/(?P\u003cpk\u003e[0-9]+)/$', views.DirectorDetail.as_view()), ] ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:2","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"把 Movies 和 Director 、 User 关联起来 现在，如果我们新建一部 movie ，那它和 director 还有 user 是没有关联的，因为 director 和 user 信息是通过 request 接收到的，而不是通过序列器接收的，这意味着，数据库中收到 director 和 user 信息是没有(和 movies 存在)外键关系的。 而要让他们发生关系 ，我们的做法是在视图中重写 .perform_create() 方法。 .perform_create() 方法允许我们处理 request 或 requested URL 中的任何信息。 在 MoviesList 和 MoviesDetail 中添加以下代码: def perform_create(self, serializer): serializer.save(owner=self.request.user) 这样 create() 方法就能够在接收到 request.data 时将其传回给序列器里的 owner 和 director 了。 ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:3","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"更新序列器 在视图中重写了 .perform_create() 方法后还需要更新下序列器才能实现他们之间的关联，在 serializer.py 中的 MoviesSerializer 类添加以下代码: owner = serializers.ReadOnlyField(source='owner.username') director = serializers.CharField(source='celebrity.name') 接着在 class Meta 的 fields 中加入 owner 和 director : class Meta: model = Movies fields = ('id', 'title', 'director', 'year', 'country', 'type', 'rating', 'owner') source 关键字负责控制在 fields 中展现的数据的源，它可以指向这个序列器实例的任意一个属性。 对 owner 属性，我们用的是 ReadOnlyField 在确保它始终是只读的，我们也可以用 CharField(read_only=True) 来等效替代，但是我嫌它太长了，其余的 Field 还有诸如 CharField 、 BooleanField 等，你可以在 「这里」查到。 ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:4","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"添加权限 我们希望授权的用户才能新建、更新和删除 movies，所以需要添加权限管理的功能。 DRF 包含了一系列的 permission 类来实现权限管理，你可以在「这里」 查到。 在这个栗子中，我们使用 IsAuthenticatedOrReadOnly 来确保授权的请求得到读写的权限，未授权的请求只有只读权限。 首先，在 views.py 中 import 以下模块: from rest_framework import permissions 接着，在 MoviesList 和 MoviesDetail 中加入以下代码: permission_classes = (permissions.IsAuthenticatedOrReadOnly,) ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:5","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"添加可浏览的授权 api 如果你在浏览器中访问我们的 api Web 界面，你会发现我们没法创建新的 movies 了，因为在上一步我们设置了权限管理。 所以我需要在浏览器中添加用户登录来实现带界面的权限管理。(之所以说带界面是因为可以在终端中直接使用 httpie 来访问 api ) 在 restapi/urls.py 中加入以下代码: urlpatterns += [ url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')), ] 这样通过在浏览器中访问 Web api 界面就能在右上角发现一个登录按钮，进行登录授权了。 ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:6","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"对象级权限 之前提到要使 movies 可以被任何人访问，但是只能被创建者编辑，所以需要赋予其游客访问的权限以及创建者编辑权限。 下面我们新建一个 permissions.py 来详细解决这个权限问题: #!/usr/bin/python # -*- coding: utf-8 -*- from rest_framework import permissions class IsOwnerOrReadOnly(permissions.BasePermission): \"\"\" 游客访问权限及创建者编辑权限 \"\"\" def has_object_permission(self, request, view, obj): # 游客权限 if request.method in permissions.SAFE_METHODS: return True # 编辑权限 return obj.owner == request.user 修改 views.py 中 MoviesDetail 的 permission_class : from douban.permissions import IsOwnerOrReadOnly permission_classes = (permissions.IsAuthenticatedOrReadOnly, IsOwnerOrReadOnly,) 终于，我们完成了整个 api 授权的过程！ ","date":"2016-04-04","objectID":"/zh-cn/drf-tutorial-4/:1:7","tags":["Python","Django","RESTful"],"title":"Django REST Framework 4-验证和授权","uri":"/zh-cn/drf-tutorial-4/"},{"categories":["Coding"],"content":"基于类的视图 基于类的视图比先前基于函数的视图的可重用性更强，可以更多快好省地 ( DRY )地写出简洁的代码。 ","date":"2016-04-03","objectID":"/zh-cn/drf-tutorial-3/:1:0","tags":["Python","Django","RESTful"],"title":"Django REST Framework 3-基于类的视图","uri":"/zh-cn/drf-tutorial-3/"},{"categories":["Coding"],"content":"把 API 用基于类的视图的方式重写 编辑 douban/views.py 进行如下重写 #!/usr/bin/python # -*- coding: utf-8 -*- from douban.models import Movies from douban.serializer import MoviesSerializer from django.http import Http404 from rest_framework.views import APIView from rest_framework.response import Response from rest_framework import status class MoviesList(APIView): \"\"\" 罗列出所有的 Movies 或者 能新建一个 Movies \"\"\" def get(self, request, format=None): movies = Movies.objects.all() serializer = MoviesSerializer(movies, many=True) return Response(serializer.data) def post(self, request, format=None): serializer = MoviesSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) class MoviesDetail(APIView): \"\"\" 展示\\更新或删除一个 Movies \"\"\" def get_object(self, pk): try: return Movies.objects.get(pk=pk) except Movies.DoesNotExist: raise Http404 def get(self, request, pk, format=None): movies = self.get_object(pk) serializer = MoviesSerializer(movies) return Response(serializer.data) def put(self, request, pk, format=None): movies = self.get_object(pk) serializer = MoviesSerializer(movies, data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) def delete(self, request, pk, format=None): movies = self.get_object(pk) movies.delete() return Response(status=status.HTTP_204_NO_CONTENT) 并更新 douban/urls.py #!/usr/bin/python # -*- coding: utf-8 -*- from django.conf.urls import url from rest_framework.urlpatterns import format_suffix_patterns from douban import views urlpatterns = [ url(r'^dbmovies/$', views.MoviesList.as_view()), url(r'^dbmovies/(?P\u003cpk\u003e[0-9]+)/$', views.MoviesDetail.as_view()), ] urlpatterns = format_suffix_patterns(urlpatterns) 重写完毕！ ","date":"2016-04-03","objectID":"/zh-cn/drf-tutorial-3/:1:1","tags":["Python","Django","RESTful"],"title":"Django REST Framework 3-基于类的视图","uri":"/zh-cn/drf-tutorial-3/"},{"categories":["Coding"],"content":"使用 Mixins 使用基于类的视图的一大好处是，我们可以使用各种 mixins DRF 为我们提供了许多现成的 mixins ，方便我们像使用 model-backed API 一样构建 “创建/获取/更新/删除” API. 我们试着使用 Mixins 改写原先的 views GenericAPIView 为我们提供了 views 核心的功能, 而 ListModelMixin 和 CreateModelMixin 为我们提供了 .list() 和 .create() 功能，我们将这些功能与 http 动作的 GET 和 POST 相绑定: from douban.models import Movies from douban.serializer import MoviesSerializer from rest_framework import mixins from rest_framework import generics class MoviesList(mixins.ListModelMixin, mixins.CreateModelMixin, generics.GenericAPIView): queryset = Movies.objects.all() serializer_class = MoviesSerializer def get(self, request, *args, **kwargs): return self.list(request, *args, **kwargs) def post(self, request, *args, **kwargs): return self.create(request, *args, **kwargs) 同样的, 我们使用GenericAPIView, RetrieveModelMixin, UpdateModelMixin和DestroyModelMixin改写views.py: class MoviesDetail(mixins.RetrieveModelMixin, mixins.UpdateModelMixin, mixins.DestroyModelMixin, generics.GenericAPIView): queryset = Movies.objects.all() serializer_class = MoviesSerializer def get(self, request, *args, **kwargs): return self.retrieve(request, *args, **kwargs) def put(self, request, *args, **kwargs): return self.update(request, *args, **kwargs) def delete(self, request, *args, **kwargs): return self.destroy(request, *args, **kwargs) 可看出，这三个 Mixin 分别对应 GET/PUT/DELETE 动作。 ","date":"2016-04-03","objectID":"/zh-cn/drf-tutorial-3/:1:2","tags":["Python","Django","RESTful"],"title":"Django REST Framework 3-基于类的视图","uri":"/zh-cn/drf-tutorial-3/"},{"categories":["Coding"],"content":"使用通用类视图 使用 Mixin 来重写 views 减少了代码量，但是还可以更少！ 那就是使用「通用类视图」—「generic class based views」 同 Django 一样，DRF为我们提供了现成的通用类视图，接下来我们使用这些通用类视图再一次修改原有的 views.py : from douban.models import Movies from douban.serializer import MoviesSerializer from rest_framework import generics class MoviesList(generics.ListCreateAPIView): queryset = Movies.objects.all() serializer_class = MoviesSerializer class MoviesDetail(generics.RetrieveUpdateDestroyAPIView): queryset = Movies.objects.all() serializer_class = MoviesSerializer 这样，代码已经非常的精简了，不过坏处在于，你不知道他具体执行了什么。 ","date":"2016-04-03","objectID":"/zh-cn/drf-tutorial-3/:1:3","tags":["Python","Django","RESTful"],"title":"Django REST Framework 3-基于类的视图","uri":"/zh-cn/drf-tutorial-3/"},{"categories":["Coding"],"content":"请求与响应 ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:0","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"请求对象 DRF 提供了一个 request 对象，它继承自 HttpRequest 并且提供了更丰富的对 request 的解析处理的方法。其中最核心的是 request 对象的 request.data 属性，它看起来和 Django 的request.POST 相似，但是在处理 Web API 上更强大些。 request.POST # Only handles form data. Only works for 'POST' method. request.data # Handles arbitrary data. Works for 'POST', 'PUT' and 'PATCH' methods. request.data 相比于 request.POST 能够处理 api 中的 「POST」、「PUT」、「PATCH」等请求。 ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:1","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"返回对象 DRF 也提供了一个 response 对象，它能把未 render 的对象(数据)通过一定方式转化为正确的数据格式返回给客户端。 return Response(data) # Renders to content type as requested by the client. ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:2","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"状态码 如果单独使用 Http 状态码的话代码会很难度，比如像我这种万年记不住几个很奇怪的状态码的人，在看到它们的时候还要 google 这就很伤！所以 DRF 提供了一个可读性更好的状态码标识，比如 HTTP_400_BAD_REQUEST ，是不是一下就看出来这是 bad request 了。这些状态码都封装在了 status 模块里，使用它们比使用纯数字的 Http 状态码更安逸。 ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:3","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"封装的 API views DRF 提供了两个封装好的 API views @api_view 这个装饰器用于基于函数的视图 APIView 这个类用于基于类的视图 这两个 views 提供了一些函数如确保在视图中接收到 request 实例和自动在 response 对象中添加 context 使其能够被 render 。 ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:4","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"开始撸代码吧 紧接着上节的教程我们要在 views 中添加一些新功能 先把 JSONResponse 扔掉，这东西太难用了，我们不再需要它。 接着在 douban/views.py 中加入以下代码: #!/usr/bin/python # -*- coding: utf-8 -*- from rest_framework import status from rest_framework.decorators import api_view from rest_framework.response import Response from douban.models import Movies from douban.serializer import MoviesSerializer @api_view(['GET', 'POST']) def movies_list(request): \"\"\" 罗列出所有的 Movies 或者 能新建一个 Movies \"\"\" if request.method == 'GET': movies = Movies.objects.all() serializer = MoviesSerializer(movies, many=True) return Response(serializer.data) elif request.method == 'POST': serializer = MoviesSerializer(data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data, status=status.HTTP_201_CREATED) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) @api_view(['GET', 'PUT', 'DELETE']) def movies_detail(request, pk): \"\"\" 展示\\更新或删除一个 Movies \"\"\" try: movies = Movies.objects.get(pk=pk) except Movies.DoesNotExist: return Response(status=status.HTTP_404_NOT_FOUND) if request.method == 'GET': serializer = MoviesSerializer(movies) return Response(serializer.data) elif request.method == 'PUT': serializer = MoviesSerializer(movies, data=request.data) if serializer.is_valid(): serializer.save() return Response(serializer.data) return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST) elif request.method == 'DELETE': movies.delete() return Response(status=status.HTTP_204_NO_CONTENT) 用上了 @api_view 后代码比之前更简洁了。 需要注意的是: 我们不再指明 request 和 response 中的内容类型。 request.DATA 即可用来处理 json 数据类型类型, 也可以处理 yaml 或其他数据类型。我们只需要在 response 中指定要返回的数据， DRF 能根据不同情况，自动在 response 中呈现正确的数据类型。 ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:5","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"在 URLs 中添加可选后缀 现在我们的 response 对象不是像教程1中的对数据类型进行强制要求了。 并且对 url 也不是硬连接的。 那么就可以定制可选的 url 后缀，如: 通过 http://example.com/api/items/4/.json 来访问 Web API。 我们所需做的就是在 views 中添加 format 关键字: def movies_list(request, format=None): 还有 def movies_detail(request, pk, format=None): 然后在 douban/urls.py 中加入 format_suffix_patterns : #!/usr/bin/python # -*- coding: utf-8 -*- from django.conf.urls import url from rest_framework.urlpatterns import format_suffix_patterns from douban import views urlpatterns = [ url(r'^dbmovies/$', views.movies_list), url(r'^dbmovies/(?P\u003cpk\u003e[0-9]+)/$', views.movies_detail), ] urlpatterns = format_suffix_patterns(urlpatterns) 不过，一般情况下，我们耶不会用到那么奇葩的 url 访问方式，以上的例子只是说明了用奇葩的 url 方式也是可以访问的 ：D ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:6","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"再测试下我们的 API 在终端中输入 $ python manage.py runserver 接着来浏览器中访问 http://127.0.0.1/dbmovies/ 如果出现如图所示的 api 则说明 Web api 返回成功。 然后我们可以在这个页面中 POST 一个新的 Movies : 在表单中选择 Media type 为 json 格式并输入 { \"id\": 3, \"title\": \"Carol\", \"year\": \"2015\", \"country\": \"US\", \"type\": \"Romance\", \"rating\": \"8.3\" } 如果返回如下图所示，则说明 POST 成功！ 你或许会注意到，每个访问这个页面的人都能 POST 一个新的 Movies ，这是不合理的，所以需要赋予权限，这个我们日后再说。 ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:7","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"可浏览性 Because the API chooses the content type of the response based on the client request, it will, by default, return an HTML-formatted representation of the resource when that resource is requested by a web browser. This allows for the API to return a fully web-browsable HTML representation. Having a web-browsable API is a huge usability win, and makes developing and using your API much easier. It also dramatically lowers the barrier-to-entry for other developers wanting to inspect and work with your API. See the browsable api topic for more information about the browsable API feature and how to customize it. ","date":"2016-04-02","objectID":"/zh-cn/drf-tutorial-2/:1:8","tags":["Python","Django","RESTful"],"title":"Django REST Framework 2-请求和响应","uri":"/zh-cn/drf-tutorial-2/"},{"categories":["Coding"],"content":"序列化 ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:0","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"环境搭建 首先我们先新建一个 restapi 项目并安装上 django-rest-framework (DRF) 环境 $ pip install djangorestframework $ python manage.py startnewproject restapi $ cd restapi $ python manage.py startnewapp douban 接着，我们需要在 setting.py 里的加入如下代码: INSTALLED_APPS = ( ... 'rest_framework', 'douban', ) ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:1","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"建立模型 由于我炒鸡喜欢看电影，所以仿着 douban-API 来做个简易的豆瓣电影的 rest-api 。 所以我们就用这个「仿豆瓣电影 api 」来作为栗子开始教程吧！ 编辑 douban/models.py 文件并加入以下代码: #!/usr/bin/python # -*- coding: utf-8 -*- from django.db import models # 举个栗子 COUNTRY_CHOICES = ( ('US', 'US'), ('Asia', 'Asia'), ('CN', 'CN'), ('TW', 'TW'), ) TYPE_CHOICES = ( ('Drama', 'Drama'), ('Thriller', 'Thriller'), ('Sci-Fi', 'Sci-Fi'), ('Romance', 'Romance' ), ('Comedy', 'Comedy') ) GENDER_CHOICES = ( ('male', 'male'), ('female', 'female') ) class movies(models.Model): title = models.CharField(max_length=100, blank=True, default='') year = models.CharField(max_length=20) # 在 director 关联了 movies 类 和 celecrity 类, 在第4章会用到 celebrity 类 # director = models.ForeignKey('celebrity', related_name='movies') country = models.CharField(choices=COUNTRY_CHOICES, default='US', max_length=20) type = models.CharField(choices=TYPE_CHOICES, default='Romance', max_length=20) rating = models.DecimalField(max_digits=3, decimal_places=1) created = models.DateTimeField(auto_now_add=True) class Meta: ordering = ('created',) # class celebrity(models.Model): # name = models.CharField(max_length=100, blank=True, default='') # age = models.IntegerField() # gender = models.CharField(choices=GENDER_CHOICES, default='男', max_length=20) 接着在终端中运行: $ python manage.py makemigrations douban $ python manage.py migrate $ python manage.py syncdb 来创建一个新的 migrations 并在数据库中生成表。 ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:2","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"创建序列化类 在开始构建 Web API 时，我们首先要做的就是提供对 movies 实例的序列化和反序列化(即对序列化后的实例进行「解码」)，这样才能生成可供浏览的 json 格式的 api 。我们可以通过声明「序列器」(一个和 Django 表单十分类似的玩意儿)来做到这一点。 在 restapi 目录中创建一个 serializer.py 文件，加入以下代码: #!/usr/bin/python # -*- coding: utf-8 -*- from rest_framework import serializers from douban.models import movies, COUNTRY_CHOICES, TYPE_CHOICES class MoviesSerializer(serializers.Serializer): pk = serializers.IntegerField(read_only=True) title = serializers.CharField(required=False, allow_blank=True, max_length=100) year = serializers.CharField(max_length=20) country = serializers.ChoiceField(choices=COUNTRY_CHOICES, default='US') type = serializers.ChoiceField(choices=TYPE_CHOICES, default='Romance') rating = serializers.DecimalField(max_digits=3, decimal_places=1) def create(self, validated_data): \"\"\" 根据接收到的 validated_data 创建一个 movies 实例 \"\"\" return movies.objects.create(**validated_data) def update(self, instance, validated_data): \"\"\" 根据接收到的 validated_data 更新并返回一个 movies 实例 \"\"\" instance.title = validated_data.get('title', instance.title) instance.year = validated_data.get('year', instance.year) instance.country = validated_data.get('country', instance.country) instance.type = validated_data.get('type', instance.type) instance.rating = validated_data.get('rating', instance.rating) instance.save() return instance 序列器的第一个部分定义了要进行序列化/反序列化的字段。 create() 和 update() 方法定义了符合规范的 movies 实例的创建和更新的方法。 序列器非常类似于 Django Form 表单，它包含了几种对字段常见的验证标识符，如 required 、 max_length 、 default 等。这些标识符实现的功能类似于 Django 表单，就不详细解释了。 所以序列器实现了以下两个功能: 选择相应的模型 选择要展现的字段(验证后的) 我们也可以通过使用 ModelSerializer 多快好省地的构建序列器，这个我们日后再说。 ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:3","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"开始使用序列器 在开始项目之前，我们先熟悉下序列器，在终端中启动 Django shell : $ python manage.py shell 输入以下代码来创建2个 Movies 实例 「荒野猎人」和「蝙蝠侠爱上超人」 from douban.models import Movies from douban.serializer import MoviesSerializer from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser movies = Movies(title='The Revenant', year='2015', country='US', type='Drama', rating=7.9) movies.save() movies = Movies(title='Batman v Superman: Dawn of Justice', year='2016', country='US', type='Romance', rating=6.7) movies.save() 然后将其中一个实例序列化 serializer = MoviesSerializer(movies) serializer.data #{'rating': u'7.9', 'title': u'The Revenant', 'country': 'US', 'year': u'2015', 'pk': None, 'type': 'Drama'} 接着我们将以上数据转换为 JSON 格式，实现序列化 content = JSONRenderer().render(serializer.data) content #{\"pk\":null,\"title\":\"The Revenant\",\"year\":\"2015\",\"country\":\"US\",\"type\":\"Drama\",\"rating\":\"7.9\"}' 反序列化也类似，通过解析 Python 数据流并将数据流\"引入\"实例中即可 from django.utils.six import BytesIO stream = BytesIO(content) data = JSONParser().parse(stream) serializer = MoviesSerializer(data=data) serializer.is_valid() # True serializer.validated_data #OrderedDict([(u'title', u'The Revenant'), (u'year', u'2015'), (u'country', 'US'), (u'type', 'Drama'), (u'rating', Decimal('7.9'))]) 可见, serializer和django form 有多么相似, 当我们写view时, 这一相似性会更加明显. 当我们输入参数many=True时, serializer还能序列化queryset: serializer = MoviesSerializer(Movies.objects.all(), many=True) serializer.data [OrderedDict([('pk', 1), ('title', u'Batman v Superman: Dawn of Justice'), ('year', u'2016'), ('country', 'US'), ('type', 'Romance'), ('rating', u'6.7')]), OrderedDict([('pk', 2), ('title', u'The Revenant'), ('year', u'2015'), ('country', 'US'), ('type', 'Drama'), ('rating', u'7.9')])] ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:4","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"使用更高级的 ModelSerializers 接着如果你按照官网的教程走下去，你会发现上面的 serializer.py 是个代码冗杂的序列器，这不符合 Python 的风格。 所以我们要做的就是简化代码。 DRF 提供了更为简便的 ModelSerializer 类可以解决这个问题。 所以我们修改之前的 serializer.py : class MoviesSerializer(serializers.ModelSerializer): class Meta: model = Movies fields = ('id', 'title', 'year', 'country', 'type', 'rating') 这种模式的序列器可以很方便地检查 fields 中的每个字段 然后在终端中打开 Django shell $ python manage.py shell 输入以下代码 from douban.serializer import MoviesSerializer serializer = MoviesSerializer() print(repr(serializer)) #MoviesSerializer(): id = IntegerField(label='ID', read_only=True) title = CharField(allow_blank=True, max_length=100, required=False) year = CharField(max_length=20) country = ChoiceField(choices=(('US', 'US'), ('Asia', 'Asia'), ('CN', 'CN'), ('TW', 'TW')), required=False) type = ChoiceField(choices=(('Drama', 'Drama'), ('Thriller', 'Thriller'), ('Sci-Fi', 'Sci-Fi'), ('Romance', 'Romance'), ('Comedy', 'Comedy')), required=False) rating = DecimalField(decimal_places=1, max_digits=3) 注: ModelSerializer 类仅仅是创建 serializer 类的一个快捷方法，它除了实现以下两种方法外并没有其余的功能: 声明需要展现的字段 定义 create() 和 update() 方法 ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:5","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"使用 Django views 编写序列器视图 为了更好理解序列器，我们不使用 DRF 的其他特性，仅仅用 Django views 模式来编写序列器的视图。 我们会创建一个 HttpResponse 的子类，这样就能将数据以 json 格式返回。 编辑 douban/views.py 加入以下代码: from django.http import HttpResponse from django.views.decorators.csrf import csrf_exempt from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser from douban.models import Movies from douban.serializer import MoviesSerializer class JSONResponse(HttpResponse): \"\"\" 将数据转为 JSON 格式的 HttpResponse 子类 \"\"\" def __init__(self, data, **kwargs): content = JSONRenderer().render(data) kwargs['content_type'] = 'application/json' super(JSONResponse, self).__init__(content, **kwargs) 讲道理的话，我们 api 的根目录应该能罗列出所有的 Movies 或者 能新建一个 Movies 并且还需要一个用于展示、更新和删除 Movies 的 views 编辑 douban/views.py 加入以下代码: #!/usr/bin/python # -*- coding: utf-8 -*- from django.http import HttpResponse from django.views.decorators.csrf import csrf_exempt from rest_framework.renderers import JSONRenderer from rest_framework.parsers import JSONParser from douban.models import Movies from douban.serializer import MoviesSerializer class JSONResponse(HttpResponse): \"\"\" 将数据转为 JSON 格式的 HttpResponse 子类 \"\"\" def __init__(self, data, **kwargs): content = JSONRenderer().render(data) kwargs['content_type'] = 'application/json' super(JSONResponse, self).__init__(content, **kwargs) @csrf_exempt def movies_list(request): \"\"\" 罗列出所有的 Movies 或者 能新建一个 Movies \"\"\" if request.method == 'GET': movies = Movies.objects.all() serializer = MoviesSerializer(movies, many=True) return JSONResponse(serializer.data) elif request.method == 'POST': data = JSONParser().parse(request) serializer = MoviesSerializer(data=data) if serializer.is_valid(): serializer.save() return JSONResponse(serializer.data, status=201) return JSONResponse(serializer.errors, status=400) @csrf_exempt def movies_detail(request, pk): \"\"\" 展示\\更新或删除一个 Movies \"\"\" try: movies = Movies.objects.get(pk=pk) except Movies.DoesNotExist: return HttpResponse(status=404) if request.method == 'GET': serializer = MoviesSerializer(movies) return JSONResponse(serializer.data) elif request.method == 'PUT': data = JSONParser().parse(request) serializer = MoviesSerializer(snippet, data=data) if serializer.is_valid(): serializer.save() return JSONResponse(serializer.data) return JSONResponse(serializer.errors, status=400) elif request.method == 'DELETE': movies.delete() return HttpResponse(status=204) 我不是很弄明白这里关掉 csrf 的意义，那不如直接就不用 csrf 不就好了？ 不管了，先放着，以后回来看 ( 吐舌头 最后修改 douban/url.py 导入相应的视图 from django.conf.urls import url from douban import views urlpatterns = [ url(r'^dbmovies/$', views.movies_list), url(r'^dbmovies/(?P\u003cpk\u003e[0-9]+)/$', views.movies_detail), ] 并在 restapi/url.py 中 include 一下 from django.conf.urls import url, include urlpatterns = [ url(r'^', include('douban.urls')), ] 这样 url 和 views 就绑定好了。 ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:6","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"测试 Web API 在终端中输入 $ python manage.py runserver 接着来浏览器中访问 http://127.0.0.1/dbmovies/ 如果出现如图所示的 api 则说明 Web api 返回成功。 (顺便安利一个 chrome 插件 — FeHelper 可以自动格式化 JSON 代码) ","date":"2016-04-01","objectID":"/zh-cn/drf-tutorial-1/:1:7","tags":["Python","Django","RESTful"],"title":"Django REST Framework 1-序列化","uri":"/zh-cn/drf-tutorial-1/"},{"categories":["Coding"],"content":"Django REST Framework 快速上手 ","date":"2016-03-29","objectID":"/zh-cn/django-rest-framework/:1:0","tags":["Python","Django","RESTful"],"title":"Django REST Framework 快速上手","uri":"/zh-cn/django-rest-framework/"},{"categories":["Coding"],"content":"背景 这几天正好在研究 RESTful 的方式来写 API，然后上手 Django REST 框架。 Django REST Framework (以下简称 DRF )是一个轻量级的库，熟悉 Django 的话可以很容易的用它来构建 Web API。 ","date":"2016-03-29","objectID":"/zh-cn/django-rest-framework/:1:1","tags":["Python","Django","RESTful"],"title":"Django REST Framework 快速上手","uri":"/zh-cn/django-rest-framework/"},{"categories":["Coding"],"content":"安装前提 Django REST Framework 安装需要以下前提: Python (2.7, 3.2, 3.3, 3.4, 3.5) Django (1.7+, 1.8, 1.9) 我自己的环境是: Python 2.7.10 Django 1.8.2 ","date":"2016-03-29","objectID":"/zh-cn/django-rest-framework/:1:2","tags":["Python","Django","RESTful"],"title":"Django REST Framework 快速上手","uri":"/zh-cn/django-rest-framework/"},{"categories":["Coding"],"content":"安装配置 安装 DRF 需要用到 pip 命令 pip install djangorestframework pip install markdown # Markdown support for the browsable API. pip install django-filter # Filtering support 或者在 GitHub 上 clone 它 git clone git@github.com:tomchristie/django-rest-framework.git 接着在 Django Project 根目录的 setting.py 文件中的 INSTALLED_APPS 加入 'rest_framework' INSTALLED_APPS = ( ... 'rest_framework', ) 如果你要使用 DRF 的 browsable API 的话，你可能还需要添加 REST 框架的登录登出视图 ( views )，辣么需要在 url.py 文件中加入以下代码: urlpatterns = [ ... url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')) ] 注: 这个 URL 地址可以是任意的，但是必须 include 'rest_framework.urls' 和 namespace='rest_framework' 。 ","date":"2016-03-29","objectID":"/zh-cn/django-rest-framework/:1:3","tags":["Python","Django","RESTful"],"title":"Django REST Framework 快速上手","uri":"/zh-cn/django-rest-framework/"},{"categories":["Coding"],"content":"举个栗子 现在我们来看一下一个简单的用 DRF 来构建一个模型支持较好的 API 的栗子。 任何一个对 REST 框架的全局设置都被放在 REST_FRAMEWORK 的模块内，所以你需要在 settings.py 文件中添加以下代码来通过 REST_FRAMEWORK 入口进行全局设置: REST_FRAMEWORK = { # Use Django's standard `django.contrib.auth` permissions, # or allow read-only access for unauthenticated users. 'DEFAULT_PERMISSION_CLASSES': [ 'rest_framework.permissions.DjangoModelPermissionsOrAnonReadOnly' ] } 现在我们可以构建 API 了，编辑 Django 项目根目录的 url.py 文件: from django.conf.urls import url, include from django.contrib.auth.models import User from rest_framework import routers, serializers, viewsets # Serializers define the API representation. class UserSerializer(serializers.HyperlinkedModelSerializer): class Meta: model = User fields = ('url', 'username', 'email', 'is_staff') # ViewSets define the view behavior. class UserViewSet(viewsets.ModelViewSet): queryset = User.objects.all() serializer_class = UserSerializer # Routers provide an easy way of automatically determining the URL conf. router = routers.DefaultRouter() router.register(r'users', UserViewSet) # Wire up our API using automatic URL routing. # Additionally, we include login URLs for the browsable API. urlpatterns = [ url(r'^', include(router.urls)), url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')) ] 解释一下， 每个 xxxSerializer 都要继承 ModelSerializer 来选择模型和模型字段。 UserSerializer 类继承了更符合 RESTful 设计的 HyperlinkedModelSerializer 超链接模型 Serializer 类，它和普通的 ModelSerializer 类有以下区别: 缺省状态下不包含 pk 字段 具有一个 url 字段，即HyperlinkedIdentityField类型 用HyperlinkedRelatedField表示关系，而非PrimaryKeyRelatedField 然后在 class Meta 中选择模型和要展现的模型元素 ViewSet 用来定义 View 的行为，和 Django 的 views 类似，用来处理 API 的 read 、write、 update 等方法(而 Django views 则处理 http 的 GET 和 POST ) 在 ViewSet 实例化之后，通过 Router 类，最终将 URL 和 ViewSet 方法绑定起来。 ok，现在你可以通过在浏览器中访问 http://127.0.0.1:8000/ 来查看你的 ‘users’ API 了。 ","date":"2016-03-29","objectID":"/zh-cn/django-rest-framework/:1:4","tags":["Python","Django","RESTful"],"title":"Django REST Framework 快速上手","uri":"/zh-cn/django-rest-framework/"},{"categories":["Coding"],"content":"背景 : 公司要求用 Django 做些项目，之前按网上教程搭环境的时候就遇到很多问题，感觉有些教程都是有误的，今天用 uWSGI 开多线程的时候服务器报了 HTTP 500 的错( Internal Server Error )，然后就一直连不上去了。所以按官网的教程重新配置一遍，把出现的问题记录下来。 ","date":"2016-02-27","objectID":"/zh-cn/nginx-uwsgi-django/:0:1","tags":["Python","Django","Nginx"],"title":"在 Ubuntu 下搭建 uWSGI + nginx + Django","uri":"/zh-cn/nginx-uwsgi-django/"},{"categories":["Coding"],"content":"准备工作 概况 一个 Web 服务器能加载 ( Html , images , CSS 等静态文件)，但是它不能直接跑 Django 应用 (对于动态的请求无法处理) ，它需要某些工具来支持 Django 应用的运行，从而使 服务器能够接受客户端的请求，处理，并返回请求。s 这时，我们就需要一个服务器网关接口 – WSGI ! WSGI 是一种Web服务器网关接口。它是一个 Web 服务器（如 nginx）与应用服务器（如 uWSGI 服务器）通信的一种规范 而 uWSGI 是一个Web服务器，它实现了 WSGI 协议、 uwsgi 、 http 等协议。 nginx 中 HttpUwsgiModule 的作用是与 uWSGI 服务器进行数据交换。 所以这套配置的实现原理是将 nginx 作为服务器最前端，它将接收 Web 的所有请求，统一管理请求。nginx 把所有静态请求自己来处理（这是 Nginx 的强项）。然后，Nginx 将所有非静态请求通过 uwsgi 传递给 Django ，由 Django 来进行处理，从而完成一次 Web 请求。 配置 uWSGI + nginx + Django 即实现以下4个链接，在下文，我们会一步步进行链接。 the web client \u003c-\u003e the web server \u003c-\u003e the socket \u003c-\u003e uwsgi \u003c-\u003e Django Python Ubuntu 14.04 自带了 Python2.7.6 你也可以通过 sudo apt-get install python2.7 python2.7-dev 来安装最新版本的 Python2.7.11 Python-pip pip 是 Python 的包管理工具，建议 Python 的包都用 pip 进行管理。 通过以下命令安装 pip : sudo apt-get install python-pip Django 通过以下命令安装 Django 并创建一个新的项目，然后进入到项目根目录 : sudo pip install Django django-admin.py startproject mysite cd mysite 关于域名和端口 在这篇 blog 中，我们把调试域名定为 127.0.0.1，你可以用自己的域名或本机 ip 地址来替代它。 并且，我们用 8000 端口作为 web 调试地址端口，这个端口与大部分 web 服务器的端口不重叠，当然你也可以自行修改调试地址的端口。 ","date":"2016-02-27","objectID":"/zh-cn/nginx-uwsgi-django/:0:2","tags":["Python","Django","Nginx"],"title":"在 Ubuntu 下搭建 uWSGI + nginx + Django","uri":"/zh-cn/nginx-uwsgi-django/"},{"categories":["Coding"],"content":"安装配置 uWSGI 安装 sudo pip install uwsgi 用 pip 安装 uwsgi 最为方便，因为如果你用 apt-get install 来安装 uwsgi 的话，你需要在 Python 搜索路径中添加入 uwsgi 模块。 测试 uwsgi 在刚刚的 mysite 目录下新建一个 Python 文件 test.py : # test.py def application(env, start_response): start_response('200 OK', [('Content-Type','text/html')]) return [b\"Hello World\"] # python3 #return [\"Hello World\"] # python2 接着运行 uWSGI 命令: uwsgi --http :8000 --wsgi-file test.py 注意：在 http 与 :8000 之间有一个空格！ 参数含义： http :8000：使用 http 协议，8000端口 wsgi-file test.py : 加载指定文件 test.py 接着在浏览器中输入以下 url : http://127.0.0.1:8000 如果出现了 ‘Hello World!’ 那说明 uWSGI 安装成功，以下链接是成功的 : the web client \u003c-\u003e uWSGI \u003c-\u003e Django 测试 Django 项目 现在我们用 uWSGI 来跑 Django 网站试试。 首先进入 Django 项目根目录，即之前的 /mysite/ 运行 python manage.py runserver 127.0.0.1:8888 在浏览器中访问该 url ，如果出现如下界面则说明你的 mysite 项目是可运行的 : 接着运行 uWSGI : uwsgi --http :8000 --module mysite.wsgi module mysite.wsgi : 读取特定的 wsgi 模块 如果出现同样的界面，说明你的 uWSGI 已经可以搭载你的 Django 应用惹，所以以下的链接是成功的 : the web client \u003c-\u003e uWSGI \u003c-\u003e Django ","date":"2016-02-27","objectID":"/zh-cn/nginx-uwsgi-django/:0:3","tags":["Python","Django","Nginx"],"title":"在 Ubuntu 下搭建 uWSGI + nginx + Django","uri":"/zh-cn/nginx-uwsgi-django/"},{"categories":["Coding"],"content":"安装配置 nginx 安装 nginx sudo apt-get install nginx sudo /etc/init.d/nginx start # 开启 nginx 服务 现在到浏览器中输入 http://127.0.0.1 ，如果你看到以下信息 : “Welcome to nginx!”那么说明 nginx 服务器运行成功，以下链接成功 : the web client \u003c-\u003e the web server 配置 nginx 首先，你需要一个 uwsgi_params 文件。 将 uwsgi_params 文件拷贝到项目文件夹下(即 /mysite/ )。uwsgi_params文件在/etc/nginx/目录下，也可以从这个页面下载 在项目文件夹下创建文件 mysite_nginx.conf ,填入并修改下面内容： # mysite_nginx.conf # the upstream component nginx needs to connect to upstream django { # server unix:///path/to/your/mysite/mysite.sock; # for a file socket server 127.0.0.1:8001; # for a web port socket (we'll use this first) } # configuration of the server server { # the port your site will be served on listen 8000; # the domain name it will serve for server_name 127.0.0.1; # substitute your machine's IP address or FQDN charset utf-8; # max upload size client_max_body_size 75M; # adjust to taste # Django media location /media { alias /path/to/your/mysite/media; # your Django project's media files - amend as required } location /static { alias /path/to/your/mysite/static; # your Django project's static files - amend as required } # Finally, send all non-media requests to the Django server. location / { uwsgi_pass django; include /path/to/your/mysite/uwsgi_params; # the uwsgi_params file you installed } } 在终端中进入之前的 /mysite/ 项目文件夹，输入 ``pwd``` ，复制下该路径，将 mysite_nginx.conf 中的 /path/to/your/mysite 全部替换掉。 在/etc/nginx/sites-enabled目录下创建本文件的连接，使nginx能够使用它 : sudo ln -s ~/path/to/your/mysite/mysite_nginx.conf /etc/nginx/sites-enabled/ 注意：记得检查 /etc/nginx/sites-enabled/ 下的软链接是否成功，因为之前我就遇到了路径出错的问题。 部署静态文件 在运行 nginx 前，你还需要把 Django 的所有静态文件全部整理到之前的 static 文件夹里，在 /mysite/mysite/settings.py 中添加以下内容 : STATIC_ROOT = os.path.join(BASE_DIR, \"static/\") 接着运行 python manage.py collectstatic 现在你发现 Django 所有的静态文件都被整理到了 /mysite/static/ 文件夹里了。 测试 nginx 首先重启 ngxin 服务 : sudo /etc/init.d/nginx restart 在 /mysite/mysite/media/ 文件夹中添加一个 media.png 文件。 在浏览器中打开 :http://127.0.0.1:8000/media/media.png 如果显示出了图片，说明 nginx 服务已经正确运行惹。 注意在从浏览器中请求图片信息时，在 uwsgi 里是没有输出信息的，而请求一个其他的动态网页时，则会输出类似 [pid: 1952|app: 0|req: 3/3] 127.0.0.1 () {36 vars in 599 bytes} [Wed Mar 18 08:43:27 2015] GET /time/ =\u003e generated 63 bytes in 1 msecs (HTTP/1.1 200) 2 headers in 88 bytes (1 switches on core 0) 这样的信息。 也就是缩，当你在浏览器中请求 media.png 时， nginx 会检查这个地址 /media/ ，接着它会在 mysite_nginx.conf 文件中看到这段代码: location /media { alias /home/thehackercat/Dev/mysite/mysite/media; # your Django project's media files - amend as required 它会直接从这个路径下去寻找图片，找到了就显示粗来，没找着就报 404 错误。 nginx and uWSGI and test.py 现在进入 /mysite/ 文件夹 输入以下命令 ： uwsgi --socket :8001 --wsgi-file test.py 在浏览器中访问 http://127.0.0.1:8000 如果出现 ‘Hello World!’ 则说明以下链接是成功的 : the web client \u003c-\u003e the web server \u003c-\u003e the socket \u003c-\u003e uWSGI \u003c-\u003e Python ","date":"2016-02-27","objectID":"/zh-cn/nginx-uwsgi-django/:0:4","tags":["Python","Django","Nginx"],"title":"在 Ubuntu 下搭建 uWSGI + nginx + Django","uri":"/zh-cn/nginx-uwsgi-django/"},{"categories":["Coding"],"content":"用 UNIX socket 取代 TCP port 在 mysite/ 文件夹下创建一个新文件 mysite.sock （空文本文档即可）。 然后对 mysite_nginx.conf 做以下修改 : server unix:///path/to/your/mysite/mysite.sock; # for a file socket # server 127.0.0.1:8001; # for a web port socket (we'll use this first) 运行 : sudo /etc/init.d/nginx restart uwsgi --socket mysite.sock --wsgi-file test.py 打开 http://127.0.0.1:8000 结果报错了，出现了这个错误 [crit] 4133#0: *1 connect() to unix:/home/thehackercat/Dev/mysite/mysite.sock failed (13: Permission denied) while connecting to upstream, client: 127.0.0.1, server: 127.0.0.1, request: \"GET / HTTP/1.1\", upstream: \"uwsgi://unix:/home/thehackercat/Dev/mysite/mysite.sock:\", host: \"127.0.0.1:8000\" 发现原来是权限的问题，于是在命令中加入这一段 : uwsgi --socket mysite.sock --wsgi-file test.py --chmod-socket=666 然后就成功了！ 用 uswgi 和 nginx 跑 Django 应用 如果上面一切都运行正常，则输入下面命令可以跑 Django 应用 : uwsgi --socket mysite.sock --module mysite.wsgi --chmod-socket=666 ","date":"2016-02-27","objectID":"/zh-cn/nginx-uwsgi-django/:0:5","tags":["Python","Django","Nginx"],"title":"在 Ubuntu 下搭建 uWSGI + nginx + Django","uri":"/zh-cn/nginx-uwsgi-django/"},{"categories":["Coding"],"content":"配置 uWSGI 便捷开启服务器 如果每次都按上述命令来跑 Django 应用实在麻烦，所以使用 .ini 文件来简化工作，便捷开启服务器，方法如下 : 在 /mysite/ 文件夹下创建文件 mysite_uwsgi.ini ，并填写修改下面内容 : mysite_uwsgi.ini file [uwsgi] # Django-related settings # the base directory (full path) chdir = /home/thehackercat/Dev/mysite # Django's wsgi file module = mysite.wsgi # the virtualenv (full path) 如果你没有装 virtualenv 就把下面这行用注释掉 home = /usr/bin/virtualenv # process-related settings # master master = true # maximum number of worker processes processes = 10 # the socket (use the full path to be safe socket = /home/thehackercat/Dev/mysite/mysite.sock # ... with appropriate permissions - may be needed chmod-socket = 666 # clear environment on exit vacuum = true 现在，只要运行以下命令，就可以跑 Django 应用了 : uwsgi --ini mysite_uwsgi.ini 到这里，如果你在浏览器中访问http://127.0.0.1:8000 可以看到正常的 Django 页面，则说明 uWSGI + nginx + Django 配置成功！ 参考文档 nginx与Django不可不说的秘密 Setting up Django and your web server with uWSGI and nginx 有些同学一定会被网上各种教程的 Django 目录结构搞得头大，其实这个目录是可自定义的，下面是我的目录结构 : ","date":"2016-02-27","objectID":"/zh-cn/nginx-uwsgi-django/:0:6","tags":["Python","Django","Nginx"],"title":"在 Ubuntu 下搭建 uWSGI + nginx + Django","uri":"/zh-cn/nginx-uwsgi-django/"},{"categories":["Interview"],"content":"背景 12月23号下午2：00参加了绿盟的 Web 后端开发实习生的面试。考官是个胖哥哥，也是科大的，人很温柔和蔼。先问了一些数据结构与算法的问题，接着问了计算机网络的一些基础问题，最后考察了下 Web 开发的一些知识。总得来说题目不难，但是自己也发挥不好，原来以为有了几次面经，但是在现场还是紧张得不行。 (真是给自己的心理素质跪了 ：P) ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:1:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"(数据结构与算法)图的遍历 我怕出错就写了5个结点的无向图，如下： 然后写了广度优先遍历： 1-\u003e2-\u003e3-\u003e4-\u003e5 深度优先遍历： 1-\u003e2-\u003e5-\u003e4-\u003e3 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:2:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"(数据结构与算法)写个排序算法求列表中倒数第二大的元素 我用 Python 写了个冒泡排序来处理： # 冒泡排序 def bubbleSort(L): for passnum in range(len(L)-1,0,-1): for i in range(passnum): if L[i] \u003e L[i+1]: L[i],L[i+1] = L[i+1],L[i] return L[-2] ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:3:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"(数据结构与算法)去重的优化算法 接着不造为什么就谈到了之前在海豚面试的时候对算法时间复杂度进行优化的问题，然后考官问了我一个去除一个列表中重复元素的算法。 # 去重 def induplicate(L): L1 = [] return [L2.append(i) for i in L if not i in L2] 这样通过增加空间复杂度来降低时间复杂度 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:4:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"Http 状态码 这个我当时说错了 我说的是 2 开头的是成功 3 开头的是需要等待 4 开头的通常是请求出错 5 开头的是服务器问题 后来回来查了下 3 开头的标识重定向 5 开头的表示服务不可用 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:5:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"TCP 3次握手连接和4次握手断开连接的过程 这个不能更经典了。 就不详细列出了，可以参见这个详解 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:6:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"设计一个产品参数配置页面布局 我本来打算多扯一些的，因为最近正好在看的《写给大家看的设计书》，但是词穷了，就画了个抽屉菜单的布局,但是感觉还有很多交互设计的地方我欠考虑。 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:7:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"Http 和 Https 的区别 这个我没答出来，我只知道 Https 是经过一定手段加密使得 Http 传输的数据包中一些明文数据变得\"隐晦”，但是具体的实现方法不太清楚。 后来我看了一篇 Blog，主要是用 SSL/TLS 来对数据包进行加密。 这样经过 SSL/TLS 协议加密后，当客户端收到服务器的 Https 请求后，会查询本机所支持的加密算法，并通过该算法来解密 Https 请求。 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:8:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"总结 这次面试总的来说题目相对简单。 面试官也教了我很多网安方面的知识，比如12306的签名协议和网关安全，虽然我是网络安全方面的小白，但是我觉得 Web 安全很炫酷。 ","date":"2015-12-23","objectID":"/zh-cn/nsfocus-interview/:9:0","tags":["Interview"],"title":"绿盟 Web 后端实习面试心得","uri":"/zh-cn/nsfocus-interview/"},{"categories":["Interview"],"content":"背景 12月11号下午4：30参加了海豚浏览器的 Python 后台开发实习生的电面，考官一开始先问了我一些 Python 基础的问题，接着问了些计网的经典面试题，最后考了2道算法题，然后开始扯皮一些之前做过的项目中的问题等，最后总结心得如下： ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:1:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"Python 的 List 能不能作为字典的 key 传入？ 我回答的是不能，因为字典的 key 值必须是不变的，而 List 的值是可变的。 之后我上网查了下，更标准的说法是，Python Dict 的 key 值是 hashable 的，即 这个 key 值在其生命周期内是不变的。 并且可以和其他对象进行比较。 以下是官方对于 hashable 给出的解释： An object is hashable if it has a hash value which never changes during its lifetime (it needs a hash() method), and can be compared to other objects (it needs an eq() or cmp() method). Hashable objects which compare equal must have the same hash value. Hashability makes an object usable as a dictionary key and a set member, because these data structures use the hash value internally. All of Python’s immutable built-in objects are hashable, while no mutable containers (such as lists or dictionaries) are. Objects which are instances of user-defined classes are hashable by default; they all compare unequal (except with themselves), and their hash value is their id(). 所以得出，Python 中所有不变的内奸对象都是 hashable 的，所有可变的容器(比如，list or dict)都不是 hashable 的，故不能作为字典的 key。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:2:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"Python 装饰器是什么，有什么看法？ 正好之前我写了一篇深入理解 Python 装饰器的 blog 我就向他解释了下，装饰器是在不修改原先代码块的情况下，为其加上一些装饰。 然后我扯了一些装饰器所使用的 Python 语言的几个特性 闭包 把函数作为参数传递 装饰器的迭代 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:3:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"Python 的 yield() 函数的看法？ 我想起来之前有看过一篇 blog 正好讲过。 yield 函数是 Python 在进行迭代时，函数内部的代码并不立刻执行，而是返回一个 generator 对象，接着每次迭代时，再读取下一个元素。 这样的好处在于，不需要一次性读取全部对象，二是实时地读取生成数据，减少了内存的开支。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:4:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"解释下 Django 的 MVC 模式，其中那一部分充当的是 controller 的部分？ 我解释了下，其实 Django 是一个 MTV 模式的框架, MTV 三个部分如下， 模型( Model )，数据存取层：处理与数据相关的所有事务，即如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。 模板( Template )，表现层：处理与表现相关的决定，即如何在页面或其他类型文档中进行显示。 视图( View )，业务逻辑层：存取模型及调取恰当模板的相关逻辑。模型与模板之间的桥梁。 而其中作为 controller 的部分是 Django 的 URLconf。 它获取用户在地址栏中输入的 URL 并将其路由到 views 模块对应的各个函数，并调用他们。实现了相应的视图函数路由到相应界面的映射功能。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:5:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"Django 中的缓存用过吗？看法是？ 正好之前在写一个 Django 练手的图书馆项目中试过 Django 的缓存机制，所以就以那个例子介绍了下。 Django 的缓存系统让开发者能够缓存某个视图的输出。这个缓存是无法在浏览器缓存中控制的，因为它并不包含在 http 头部内。 我用的是 Django 缓存系统的 memcached。 memcached 作为一个后台进程运行，并分配一个指定的内存量，它所实现的功能是提供一个添加、检索和删除缓存中任意数据的快速接口，所有的数据是直接存储在内存中的，所以没有用到数据库或者文件系统，减少了额外开销。 但是 memcached 有一个缺点是，它的缓存是完全存在内存中的，一旦服务器崩溃，辣么所有缓存的数据就丢失了。 其他的缓存机制偶没有用过，所以就没有谈。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:6:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"用户在浏览器中输入一个网址，到 Django 后台捕捉到请求其中的过程？ 这个我当时貌似讲偏题了，我说的是 用户输入一个网址后 浏览器先检查缓存，如果有缓存，就从缓存中获得资源文件并加载，如果木有缓存，则执行下一步。 进行 DNS 域名解析，将域名解析成 ip 地址。 与 ip 地址对影的服务器进行 TCP 连接。 接着经历 TCP 3次握手过程。 一旦连接建立后，开始发送 Http 请求。 服务器获得 Http 请求后，将该请求打包成 HttpRequest 对象。 接着检查 Request 中是否需要 Django 中间件的方法，如果没有则执行下一步。 判断 Request 中的各种信息，诸如 user_agent、GET/POST 等，并在 URLconf 中进行匹配路由到对应的 views 视图函数中。 返回一个 Response 对象，并调用相应的 views 视图函数。 最后返回一个 Http 相应，并加载页面。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:7:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"(数据结构与算法)获得两个列表的交集 我第一次写的是 def intset(L1,L2): L = [] for i in L1: if i not in L2: L.append(i) return L 接着考官问我，这个时间复杂度是多少，很明显是O(n^2)，他又问我有没有更好的方法， 于是我写了第二种方法 def intset2(L1,L2): L = [set(L1)^set(L2)] return L 这样先把L1、L2列表中重复的元素删除了，接着再用异或符来取得他们的交集。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:8:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"(数据结构与算法)一个人一次可以爬3级或5级的台阶，请问他爬到第m层时，有n种解法，求解 这个我当时没写出来，我第一眼感觉是递归的题，后来室友告诉我是线性规划的题。之后我在 leetcode 上也看到了相应的解法，真是太蠢了我！ leetcode 解法如下： class Solution: # @param {integer} n # @return {integer} def climbStairs(self, n): if n==1 or n==2: return n a=1;b=2;c=3 for i in range(3,n+1): c=a+b;a=b;b=c return c ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:9:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Interview"],"content":"总结 以上就是我这次 Python 实习面试的大部分考题，面试完之后感觉自己基础还是不扎实，对于性能优化的理解还有缺陷，代码写得不够漂亮，算法方面很薄弱。故决定刷一下 Python 文档和 Leetcode。 而且这次面试感觉要黄，因为都一星期了，HR 还是木有给我打电话 T.T 不过，我有了其他的考虑了，心理也安定了许多。 现在，我只想去睡个十除以三的懒觉。 ","date":"2015-12-18","objectID":"/zh-cn/dolphin-interview/:10:0","tags":["Interview"],"title":"海豚浏览器 Python 实习面试心得","uri":"/zh-cn/dolphin-interview/"},{"categories":["Coding"],"content":"由于官网教程讲得迷迷糊糊的，所以我提炼了下代码，发现便于理解很多。 ","date":"2015-12-08","objectID":"/zh-cn/django-learning4/:0:0","tags":["Python","Django"],"title":"Django 高级 views 和 URLconf 配置","uri":"/zh-cn/django-learning4/"},{"categories":["Coding"],"content":"URLconf 技巧 # 在模块开始导入关联的视图函数，直接传递函数对象 from django.conf.urls import include, url from django.contrib import admin from mysite.views import hello, current_datetime, hours_ahead urlpatterns = [ url(r'^admin/', include(admin.site.urls)), url(r'^hello/$', hello), url(r'^time/$', current_datetime), url(r'^time/plus/(\\d{1,2})/$', hours_ahead), ] # 在模块开始导入 views 模块，传递 views.视图函数 from django.conf.urls import include, url from django.contrib import admin from mysite import views urlpatterns = [ url(r'^admin/', include(admin.site.urls)), url(r'^hello/$', views.hello), url(r'^time/$', views.current_datetime), url(r'^time/plus/(\\d{1,2})/$', views.hours_ahead), ] # 传入一个包含模块名+函数名的对象 from django.conf.urls import include, url from django.contrib import admin urlpatterns = [ url(r'^admin/', include(admin.site.urls)), url(r'^hello/$', 'mysite.views.hello'), url(r'^time/$', 'mysite.views.current_datetime'), url(r'^time/plus/(\\d{1,2})/$', 'mysite.views.hours_ahead'), ] # 开启 URLconf 调试模式 from django.conf.urls import include, url from django.contrib import admin from mysite import views urlpatterns = [ url(r'^admin/', include(admin.site.urls)), url(r'^hello/$', views.hello), url(r'^time/$', views.current_datetime), url(r'^time/plus/(\\d{1,2})/$', views.hours_ahead), ] if settings.DEBUG: urlpatterns += url(r'^debuginfo/$', views.debug), ) ","date":"2015-12-08","objectID":"/zh-cn/django-learning4/:1:0","tags":["Python","Django"],"title":"Django 高级 views 和 URLconf 配置","uri":"/zh-cn/django-learning4/"},{"categories":["Coding"],"content":"命名组 我觉得命名组的模式增加了代码冗余度，且语义化也不好。对于我这种懒人完全不需要 ：D (其实就是我懒的借口) 而它的目的在于，将变量以位置参数的方式传递给视图函数变为以关键字参数的方式传递。 # 无名组，以位置参数传递变量 from django.conf.urls import include, url from django.contrib import admin from mysite import views urlpatterns = [ url(r'^admin/', include(admin.site.urls)), url(r'^articles/(\\d{4})/$'， view.year_archive), url(r'^articles/(\\d{4})/(\\d{2})/$', views.month_archive), ] # 命名组，以关键字参数传递变量 from django.conf.urls import include, url from django.contrib import admin from mysite import views urlpatterns = [ url(r'^admin/', include(admin.site.urls)), url(r'^articles/(?P\u003cyear\u003e\\d{4})/$'， view.year_archive), url(r'^articles/(?P\u003cyear\u003e\\d{4})/(?\u003cmonth\u003e\\d{2})/$', views.month_archive), ] ","date":"2015-12-08","objectID":"/zh-cn/django-learning4/:2:0","tags":["Python","Django"],"title":"Django 高级 views 和 URLconf 配置","uri":"/zh-cn/django-learning4/"},{"categories":["Coding"],"content":"最近在写 Python+Django 的时候发现，有时候封装 API 的时候经常会遗失一些重复的装饰信息，但是直接封装到方法里是比较差劲的写法，因为有多个模块可能同时需要这些装饰信息，所以我希望使用一种可以迭代的装饰器。于是我在 Stack Overflow 上找到了相应的解答。下面以这篇解答为引写下我理解 Python decorator 的思路过程。 ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:0:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"装饰器是做什么用的？ 装饰器实现对一个已有的模块做一些“修饰工作”，所谓修饰工作就是想给现有的模块加上一些小装饰（一些小功能，这些小功能可能好多模块都会用到），但又不让这个小装饰（小功能）侵入到原有的模块中的代码里去。 ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:1:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"装饰器的定义 首先，你需要知道 Python 的闭包，接着发现3点 Python 的特性在装饰器中运用： 函数可以赋值给一个变量。 函数可以定义在另一个函数内部。 函数名可以作为函数返回值。 辣么，先来看一段代码: def getTalk(type=\"shout\"): # 定义函数 def shout(word=\"yes\"): return word.capitalize()+\"!\" def whisper(word=\"yes\") : return word.lower()+\"...\"; # 返回函数 if type == \"shout\": # 没有使用\"()\", 并不是要调用函数，而是要返回函数对象 return shout else: return whisper # 如何使用？ # 将函数返回值赋值给一个变量 talk = getTalk() # 我们可以打印下这个函数对象 print talk #outputs : \u003cfunction shout at 0xb7ea817c\u003e # 这个对象是函数的返回值 print talk() #outputs : Yes! # 不仅如此，你还可以直接使用之 print getTalk(\"whisper\")() #outputs : yes... 既然函数可以作为返回值，是不是函数也可以作为参数传递呢 def doSomethingBefore(func): print \"I do something before then I call the function you gave me\" print func() doSomethingBefore(scream) #outputs: #I do something before then I call the function you gave me #Yes! 所以看过这两段代码，你一定明白了，装饰器的定义。 装饰器就是封装器，可以让你在被装饰函数之前或之后执行代码，而不必修改函数本身代码。 ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:2:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"怎么写封装器： 首先，我们来手写一个封装器： def new_decorator(func): def wrapper(): print(\"before the function runs\") func() print(\"after the function runs\") return wrapper def along_func(): print(\"I am a alone function\") decorated_along_func = new_decorator(along_func) decorated_along_func() #outputs: #before the function runs #I am a alone function #after the function runs 这里每次调用 decorated_along_func 函数时，都会将 along_func 函数传入到装饰函数 new_decorator 中，完成封装。 ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:3:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"怎么写装饰器： 那将上例代码稍微进行修改： def new_decorator(func): def wrapper(): print(\"before the function runs\") func() print(\"after the function runs\") return wrapper @new_decorator def along_func(): print(\"I am a alone function\") along_func() 就会发现会得到相同的结果，这就是装饰器！ 那么回到我最初的问题，装饰器能否迭代呢？ 可以！ def decorator1(func): def wrapper(): print(\"before the function runs\") func() print(\"after the function runs\") return wrapper def decorator2(func): def wrapper(): print(\"before the decorator1 runs\") func() print(\"after the decorator1 runs\") return wrapper @decorator2 @decorator1 def along_func(): print(\"I am a alone function\") along_func() #outpus: #before the decorator1 runs #before the function runs #I am a alone function #after the function runs #after the decorator1 runs 这种特性十分的便捷，但是必须注意装饰器的顺序。 如果上例代码写成： @decorator1 @decorator2 def along_func(): print(\"I am a alone function\") 那么结果将变为 before the function runs before the decorator1 runs I am a alone function after the decorator1 runs after the function runs ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:4:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"一些迭代装饰器的用法 # bold装饰器 def makebold(fn): def wrapper(): # 在前后加入标签 return \"\u003cb\u003e\" + fn() + \"\u003c/b\u003e\" return wrapper # italic装饰器 def makeitalic(fn): def wrapper(): # 加入标签 return \"\u003ci\u003e\" + fn() + \"\u003c/i\u003e\" return wrapper @makebold @makeitalic def say(): return \"hello\" print say() #outputs: \u003cb\u003e\u003ci\u003ehello\u003c/i\u003e\u003c/b\u003e # 等价的代码 def say(): return \"hello\" say = makebold(makeitalic(say)) print say() #outputs: \u003cb\u003e\u003ci\u003ehello\u003c/i\u003e\u003c/b\u003e 是不是灰常炫酷。 ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:5:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"高级用法 关于更多装饰器的高级用法，你可以戳以下链接： 戳我 关于 Python Decroator 的各种提案，可以参看： Python Decorator Proposals ","date":"2015-12-07","objectID":"/zh-cn/python-decorator-learning/:6:0","tags":["Python"],"title":"深入理解 Python 装饰器","uri":"/zh-cn/python-decorator-learning/"},{"categories":["Coding"],"content":"MTV vs MVC 正如在之前这篇文章所提到的， 把数据存取逻辑、业务逻辑和表现逻辑组合在一起的概念有时被称为软件架构的 Model-View-Controller ( MVC )模式。 在这个模式中， Model 代表数据存取层，View 代表的是系统中选择显示什么和怎么显示的部分，Controller 指的是系统中根据用户输入并视需要访问模型，以决定使用哪个视图的那部分。 而 Django 使用的更多的则是模型( Model )、模板( Template )和视图( Views )的软件设计模式，称为 MTV 模式。我在 Stack Overflow 的这个回答里找到了对于 MTV vs MVC 两种设计模式间的微妙的差别。 其中提到，不能简单的把 Django 视图认为是 MVC 控制器，把 Django 模板认为是 MVC 视图。 两者之间的差别在于，在 Django 中，视图( Views )不处理用户输入，而是用来选择要展示的哪些数据，而不是要如何展示数据。而 Django 模板 仅仅决定如何展现Django视图指定的数据。 或者说, Django 将 MVC 中的视图进一步分解为 Django 视图 和 Django 模板两个部分，分别决定 “展现哪些数据” 和 “如何展现”，使得 Django 的模板可以根据需要随时替换，而不仅仅限制于内置的模板。至于 MVC 控制器部分，由 Django 框架的 URLconf 来实现。 ","date":"2015-11-21","objectID":"/zh-cn/django-learning3/:1:0","tags":["Python","Django"],"title":" Django 学习笔记3-- Models ","uri":"/zh-cn/django-learning3/"},{"categories":["Coding"],"content":"模型练手 为了深入了解 Django Models 对数据的操作，我写了一个简单的博客模型作为练手。 在新建模型时遇到了一个 App migrations 问题如下： 后来发现是由于 Django 版本问题，在最近版本把 migrations 移出了所创建的 App 的根目录，只需要执行python manage.py makemigration接着再执行python manage.py migrate即可解决。 写了个简单的博客的增删改查，代码如下： #!/usr/bin/python #-*-coding:utf-8 -*- from django.shortcuts import render from django.http import HttpResponseRedirect from blog.models import Blog from blog import forms from django.template import RequestContext def blog_list(request): blog_list = Blog.objects.all() return render(request,\"blog_list.html\",{'blog_list':blog_list}) def blog_form(request): if request.method == 'POST': form = forms.BlogForm(request.POST) if form.is_valid(): data = form.cleaned_data if 'id' not in data: blog = Blog(title=data['title'],author=data['author'],content=data['content']) blog.save() else: blog = Blog.object.get(id=data.id) blog.title = data['title'] blog.author = date['author'] blog.content = data['content'] blog.save() return HttpResponseRedirect('/blog/list') else: form = forms.BlogForm() return render(request,\"blog_form.html\",context_instance=RequestContext(request)) def blog_del(request): errors = [] if 'id' in request.GET: bid_ = request.GET['id'] Blog.objects.fileter(id=bid_).delete() return HttpResponseRedirect('/blog/list') def blog_view(request): if 'id' in request.GET: bid_ = request.GET['id'] blog = Blog.object.get(id=bid_) form = forms.BlogForm( initial = {'id':blog.id,'title':blog.title,'author':blog.author,'content':blog.content} ) return render(request,\"blog_form.html\",{'form':form},context_instance=RequestContext(request)) else: errors.append(\"参数异常请刷新后重试\") return render(request,\"blog_list.html\",{'errors':errors}) def blog_edit(request): errors = [] if 'id' in request.GET: bid_ = request.GET['id'] blog = Blog.objects.get(id=bid_) form = forms.BlogForm( initial = {'id':blog.id,'title':blog.title,'author':blog.author,'content':blog.content} ) return render_to_response(\"blog_form.html\",{'form':form},context_instance=RequestContext(request)) else: errors.append(\"参数异常请刷新后重试\") return render(request,\"blog_list.html\",{'errors':errors}) #!/usr/bin/python #-*-coding:utf-8 -*- from django import forms class BlogForm(forms.Form): title = forms.CharField(label='标题') author = forms.CharField(label='作者') content = forms.CharField(label='正文',widget=forms.Textarea) 其中 CharField() 相当于赋予了 title 表段 varchar 的属性。 object.all() 相当于执行了一条select * from blog的 sql 语句。 object.get() 相当于执行了一条select * from blog where id='bid_'的获取单个对象的 sql 语句。 object.save() 相当于执行了UPDATE blog SET ... 的 sql 语句。 并用 errors[] 列表来捕捉错误信息，一般防止出现错误的 sql 语句时增加了 blog 表段中的 id 号而其余属性值为空的情况。 感觉相比于 ThinkinPHP 操作表单 GET/POST 请求以及处理数据库方面要方便得多。 ","date":"2015-11-21","objectID":"/zh-cn/django-learning3/:2:0","tags":["Python","Django"],"title":" Django 学习笔记3-- Models ","uri":"/zh-cn/django-learning3/"},{"categories":["Coding"],"content":"疑惑 MVC 框架大大缩小了开发者对数据存储的直接操作，框架自动生成 sql 语句并空值数据的存取等。以后写 sql 感觉就跟 Excel 一样了，那应该怎么优化 sql 呢。 顺便吐槽一下，最近 GitHub Repositorie 换新的布局，天热噜，怎么能这么丑！ ","date":"2015-11-21","objectID":"/zh-cn/django-learning3/:3:0","tags":["Python","Django"],"title":" Django 学习笔记3-- Models ","uri":"/zh-cn/django-learning3/"},{"categories":["Coding"],"content":"背景 由于之前写 Django – Templates 篇时要用到包含 Liquid 语法的示例代码，而 Octopress (Jekyll) 在后端使用 Liquid 来处理生成 Web Pages ，对于文章内部插入的原本用来作示例的 Liquid 代码会被解析成 Web Pages 生成语句而不是原本的内容。故苦恼了我一会儿 Q.Q 不过这都不是事儿 ","date":"2015-11-20","objectID":"/zh-cn/blog-build-with-liquid/:1:0","tags":["Octopress"],"title":"在 Octopress 中生成包含 liquid 语句的代码","uri":"/zh-cn/blog-build-with-liquid/"},{"categories":["Coding"],"content":"解决方法 比如，我之前写的 {% raw %} {% if variable %} {% else %} {% endif %} {% endraw %} 就会因为包含了 {% raw %}{% ... %}{% endraw %} 解决方法是： 在每一块包含 Liquid 语句的代码快前后用 {{ \"{% raw \" }}%} 和 {{ \"{% endraw \" }}% 包括起来。 这样就能确保示例代码不会被错误的解析成 Jekyll Web Pages 生成语句。 但是如果我要显示 {{ \"{% raw \" }}%} 和 {{ \"{% endraw \" }}%} 怎么办呢 ？ 我试着使用使用如下方法： {{ \"{% raw \" }}%} {{ \"{% raw \" }}%} {{ \"{% endraw \" }}%} {{ \"{% endraw \" }}%} 来显示一个 {{ \"{% raw \" }}%} 和 {{ \"{% endraw \" }}%} 后来我在 Stack Overflow 找到了一个回答： 使用 {% raw %}{{ \"{% raw \" }}%}{% endraw %} 就可以得到 {{ \"{% raw \" }}%} 这种方法是正确的。 ","date":"2015-11-20","objectID":"/zh-cn/blog-build-with-liquid/:2:0","tags":["Octopress"],"title":"在 Octopress 中生成包含 liquid 语句的代码","uri":"/zh-cn/blog-build-with-liquid/"},{"categories":["Coding"],"content":"虽然 Django 中 Html 可以直接硬编码到 Python 中，但是这种行为并不利于前端开发人员进行维护。所以 Django 有了流模板 ( Liquid Templates )。 ","date":"2015-11-16","objectID":"/zh-cn/django-learning2/:0:0","tags":["Python","Django"],"title":" Django 学习笔记2-- Templates ","uri":"/zh-cn/django-learning2/"},{"categories":["Coding"],"content":"流模板基础 举个例子，下面这个模板大致含括了 Django 模板的几个特性。 {% raw %} {% load staticfiles %} \u003c!DOCTYPE html\u003e \u003chtml lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"UTF-8\"\u003e \u003ctitle\u003e{% block title %}{% endblock %}\u003c/title\u003e \u003c/head\u003e \u003cbody\u003e \u003cp\u003eDear {{ person_name }},\u003c/p\u003e \u003cp\u003eThanks for placing an order from {{ company }}. It's scheduled to ship on {{ ship_date|date:\"F J,Y \" }}.\u003c/p\u003e {% if ordered_warranty %} \u003cp\u003eYour warranty information will be included in the packaging.\u003c/p\u003e \u003cp\u003eHere are the items you've ordered: \u003c/p\u003e \u003cul\u003e {% for item in item_list %} \u003cli\u003e {{ item }} \u003c/li\u003e {% endfor %} \u003c/ul\u003e {% else %} \u003cp\u003eYou didnt order a warranty, so you're on your own when the products inevitably stop working. \u003c/p\u003e {% endif %} \u003cp\u003eSincerely,\u003cbr /\u003e{{ company }}\u003c/p\u003e \u003cp\u003efooter\u003c/p\u003e {% block footer %} \u003chr\u003e \u003cp\u003eThanks for visiting my site.\u003c/p\u003e {% endblock %} \u003c/body\u003e \u003c/html\u003e {% endraw %} 看出，模板是基于 Html 的，事实上它就是保存成一个 .html 文件，它跟我们所看到的 html 的区别就在于多了一些由 {% raw %}{{ }}{% endraw %}括起来的变量以及由{% raw %}{% %}{% endraw %}括起来的模板标签，此外变量还通过过滤器 |来对文本输出格式进行转换。而这里 {% raw %}{{ }}{% endraw %} 里的变量相当于一个形参，真正显示出来的是在我们渲染模板的 Python 文件里所传给它的值。 比如在下面的模板渲染代码里 c = Context({'person_name':'LexusLee', 'company': 'UESTC', 'ship_date':datetime.date(2015,09,24), 'ordered_warranty': False}) 那么模板中的 person_name 最终显示的就是 LexusLee 。 ","date":"2015-11-16","objectID":"/zh-cn/django-learning2/:1:0","tags":["Python","Django"],"title":" Django 学习笔记2-- Templates ","uri":"/zh-cn/django-learning2/"},{"categories":["Coding"],"content":"模板标签 {% raw %}{% if variable %} {% else %} {% endif %}{% endraw %}用于判断变量 variable 是否为真，为真则执行 else 标签前的内容，否则执行 else 便签内的内容，跟大部分编程语言中的条件语句用法一致。 同理{% raw %}{% for %} {% endfor %}{% endraw %}的用法也和大部分编程语言中循环语句的用法一致。需要注意的是，每个 for 循环中还有一个成为{% raw %}{% forloop %}{% endraw %}的模板变量，这个变量能提示一些循环进度信息相关的属性，关于这个变量的详细统发可以参照这一节。 {% raw %}{% block content %} {% endblock %}{% endraw %}是用来处理模板继承和重载的标签，来避免重复和冗余的代码。比如上述的实例模板( base.html )中，我希望在多个文件中都能显示 footer ，而不需要重复编码，故在该模板中写了{% raw %}{% block footer %}{% endraw %},而在另一个文件中只需要写 {% raw %} {% extends \"base.html\" %} {% block footer %} \u003ca href=\"https://github.com/thehackercat\"\u003eGithub\u003c/a\u003e {% endblock %} {% endraw %} 这样所有的 footer 中都会有 \u003ca href=\"https://github.com/thehackercat\"\u003eGithub\u003c/a\u003e 这行代码。而之前{% raw %}{% block footer %} {% endblock %}{% endraw %}框中的代码将会被 overwrite ，也就是说对于重载模块，子模板可以重载这些部分，如果子模板不重载这些部分，则会按照默认的内容显示。 4.{% raw %}{% load staticfiles %}{% endraw %}用来加载静态资源，比如加载 CSS 、 JS 等静态文件时会用到。 5.{% raw %}{# #}{% endraw %} 用于注释。 ","date":"2015-11-16","objectID":"/zh-cn/django-learning2/:2:0","tags":["Python","Django"],"title":" Django 学习笔记2-- Templates ","uri":"/zh-cn/django-learning2/"},{"categories":["Coding"],"content":"今天好像巴黎有点乱，希望明天太阳还会照常升起。 ","date":"2015-11-14","objectID":"/zh-cn/django-learning1/:0:0","tags":["Python","Django"],"title":" Django 学习笔记1-- URLconf ","uri":"/zh-cn/django-learning1/"},{"categories":["Coding"],"content":"简介 Django 是一个由 Python 编写、开源并采用经典的 MVC 设计模式的 Web Full Stack 应用框架。 在 Django 中，控制器接受用户输入的部分由框架自行处理，所以 Django 里关注更多在模型( Model )、模板( Template )和视图( Views )，称为 MTV 模式。他们各自的职责如下： 模型( Model )，数据存取层：处理与数据相关的所有事务，即如何存取、如何验证有效性、包含哪些行为以及数据之间的关系等。 模板( Template )，表现层：处理与表现相关的决定，即如何在页面或其他类型文档中进行显示。 视图( View )，业务逻辑层：存取模型及调取恰当模板的相关逻辑。模型与模板之间的桥梁。 而 Django 的编译方式比较特别，他的 MVC 控制器部分由 URLconf 来实现。 ","date":"2015-11-14","objectID":"/zh-cn/django-learning1/:1:0","tags":["Python","Django"],"title":" Django 学习笔记1-- URLconf ","uri":"/zh-cn/django-learning1/"},{"categories":["Coding"],"content":"URLconf 当我在 Django 中编写完视图要想将其路由要页面上时，我发现了 Django 的 URLconf 路由机制，他实现了为相应的视图函数路由到相应界面的映射功能，也就是说，当用户访问了 http://127.0.0.1:8000/hello/ 时， Django 调用了视图 views.py 中的 hello () 函数。 from django.conf.urls import include, url from mysite.views import hello,current_datetime,hours_ahead,letter urlpatterns = [ url(r'^hello/$', hello), url(r'^time/$', current_datetime), url(r'^time/plus/(\\d{1,2})/ $',hours_ahead), ] 可以看出， URLconf 的路由是通过正则表达式来匹配一个完整的 hello 的 URL ，这样的话就可以保证 诸如 /hello/foo/ 等 URL 不会被匹配到。 为了更深入了解 URLconf 路由的机制，我找到了类似的 tornado 框架来对比。 注意到在其中 web.py 文件中的第2964行开始的如下代码： application = tornado.web.Application([ (r\"/\", MainHandler), ]) http_server = tornado.httpserver.HTTPServer(application) http_server.listen(options.port) tornado.ioloop.IOLoop.current( ).start( ) 可以看出 torando 现把一个路由表作为一个参数，传给 Application 类的构造函数，接着创建了一个实例，然后再把这个实例传递给 http_server 。那么当客户端发起get /请求的时候, http server 接收到这个请求，在路由表中匹配 url pattern ，最后交给 MainHandler 去处理。 这个机制跟 Django 的 URLconf 是类似的，都是通过在 pattern 中匹配好对应的 url 接着传给处理器来负责从路由表中检索并路由。 这种方法松耦合了 http server 层和 web application 层，从而让开发者可以专注于 web 应用的逻辑层，很好！ ：D ","date":"2015-11-14","objectID":"/zh-cn/django-learning1/:2:0","tags":["Python","Django"],"title":" Django 学习笔记1-- URLconf ","uri":"/zh-cn/django-learning1/"},{"categories":["Coding"],"content":"Django 如何处理请求 所以了解过了 Django 的 URLconf 机制后，我开始思考他是如何处理请求的。 我开启服务器后在地址栏中输入 http://127.0.0.1:8000/time/plus/20/ 然后花现处理路线如下： 进来的请求转入 /time/plus/20/ . Django 通过在 ROOT_URLCONF 配置来决定根 URLconf . Django 在 URLconf 中的所有 URL 模式中，查找第一个匹配 /time/plus/20/ 的条目。 如果找到匹配，将调用相应的视图函数 如果没找到匹配，则返回相应的 Http 状态码 (如图) 视图函数返回一个HttpResponse Django 转换 HttpResponse 为一个适合的 HTTP response ，以 Web page 显示出来 ","date":"2015-11-14","objectID":"/zh-cn/django-learning1/:3:0","tags":["Python","Django"],"title":" Django 学习笔记1-- URLconf ","uri":"/zh-cn/django-learning1/"},{"categories":["Coding"],"content":"最近把个人博客搭好了，用了Octopress,一个基于 Jekyll 的集成开发工具。 原来 CSDN 的那个『骇客猫』弃坑了。 ","date":"2015-11-09","objectID":"/zh-cn/fix-the-datetime-bug/:0:0","tags":["Octopress"],"title":"使用Octopress搭建静态博客","uri":"/zh-cn/fix-the-datetime-bug/"},{"categories":["Coding"],"content":"安装和配置. Octopress 的安装配置比较简单，是需要按照官网或者网上一些教程一步步走即可。 由于我在2015年10月1日更新了 OS X EI Capitan，新系统在权限设置上增加了 System Integrity Protection (SIP) 来提高系统安全性并且在 System Library 的路径上作了修改，导致了一些安装 Jekyll 时出现的异常，罗列如下： 如果你使用命令行 $ gem install jekyll 安装 Jekyll 时 遇到了如下问题： ERROR: While executing gem ... (Errno::EPERM) Operation not permitted - /usr/bin/jekyll 辣么尝试使用 $ sudo gem install -n /usr/local/bin/ jekyll 从而有效地避开 EI Captian 中 rootless 用户的权限问题。 或者有更彻底的办法，在终端输入 $ export PATH=/usr/local/bin:$PATH 这样会将原来 /usr/bin 的路径更改为 /usr/local/bin ，然后再进行安装，一劳永逸，但我不建议这么做。 如果你在进行上述操作时遇见了如下问题： 在命令行中输入 ```$ xcode-select —install```就可以安装了。 辣么你应该没有安装 OS X developer tools ，安装后才能编译一些 ruby 的原生的拓展插件。 在命令行中输入 $ xcode-select —install就可以安装了。 如果你遇到了任何 Permission denied 的问题： ERROR: While executing gem ... (Errno::EACCES) Permission denied 辣么在命令行之前加上 $ sudo 。 ","date":"2015-11-09","objectID":"/zh-cn/fix-the-datetime-bug/:1:0","tags":["Octopress"],"title":"使用Octopress搭建静态博客","uri":"/zh-cn/fix-the-datetime-bug/"},{"categories":["Coding"],"content":"个性化修改 对于我的博客的个性化修改我主要做了以下三个： 第三方主题：Octopress有很多第三方主题安装也很便捷。 插件安装：可以在 /plugins 目录下安装一些第三方插件，诸如 Disqus 评论系统、 Twitter 的时间线等。 样式修改：我在 /sass/custom/_styles.scss 中修改了字体、 blog 的行间距以及一些边边角角的地方。 我使用了第三方主题 cleanpress 她极简的风格很吸引我，但是这个主题有蛮多 bug 的。 比如，在首页会遇见一个博客的 post 时间无法显示导致日历图标和目录图标重合的问题，如下所示： 经过一上午的 debug ，我发现了在 /source/_includes/post/date.html 第11行 date_formatted 是没有声明 formatted 的格式的从而导致了无法显示。 故我将其替换成了 date | date: \"%b %e, %Y\" 然后就可以显示出 format 后的时间了。 还有在发布超过20字的标题的博客时，首页的相应博客处会出现样式错误， date_line 会与标题重叠在一起。 这两个 bug 我都已修复并提交了，在这里可以查看并修改。 我使用了第三方插件 Disqus 非常棒的评论系统，以及 Twitter Timeline 也是非常棒的时光机插件。根据官网的教程很容易安装并使用。 接着我还修改了样式，其中把全局字体改成了谷歌和 Adobe 联合发布的 思源黑体 ，漂亮得不像实力派。修改过程主要参考了这篇文章。其中每个不同的 Adobe 账户需要插入的是不同的 Typekit 代码( Adobe 会帮你自动生成代码)。但需要注意的是 Adobe Typekit 虽然不是免费服务，但也有免费方案可以选择，注册后有每月 25,000 次的浏览次数额度，对于一般个人 blog 或小型网站来说其实还算充裕（当然你也可以考虑付费升级，价格并不高 ：P ）。 ","date":"2015-11-09","objectID":"/zh-cn/fix-the-datetime-bug/:2:0","tags":["Octopress"],"title":"使用Octopress搭建静态博客","uri":"/zh-cn/fix-the-datetime-bug/"},{"categories":["Coding"],"content":"为什么是蜘蛛侠 ？ 嗷，其实是这样的，熟悉我的人就知道，我个人是漫威巨粉，而先前我看到 cleanpress 的 demo 页是酱的图片我很喜欢，并且想起来蜘蛛侠要回归漫威了, 再看绿箭我就节食5分钟！ 这就像苯宝宝又要回归已经弃坑的绿箭侠一样鸡冻。 ","date":"2015-11-09","objectID":"/zh-cn/fix-the-datetime-bug/:3:0","tags":["Octopress"],"title":"使用Octopress搭建静态博客","uri":"/zh-cn/fix-the-datetime-bug/"},{"categories":["Gossip"],"content":"从小，我就想做英雄大侠。 长着翅膀，穿梭云雾间如探囊取物的那种。 我幻想着， 有一天我能够被基因拼接后的老鹰咬一口， 白天老老实实上班， 晚上却化身老鹰侠。 戴上面具， 展开翅膀， 去消灭世界上所有的坏蛋。 于是老鹰侠兴高采烈地跑去告诉老爸这个想法。 老爸沉默了好久，才如梦初醒， 终于做出了回应。 他十分激动地顺手拿起了手边的书， 劈头盖脸地向老鹰侠打去， 边打还边说： “你爹我文曲星转世，学没富五车，富辆皮卡还是有的， 靠着这张嘴打架也没输过谁，上能识天象闻天下，下能解鸡兔同笼奥数题， 不说是什么千秋万代的伟人，好歹也是黑夜中最耀眼的那颗星。 你老子我尚且不能当什么大侠，你小子跟我装什么逼！” 老爸一套招式行云流水不带喘精准无误地命中了我瘦弱的身躯。 当时我就懵逼了。 “你儿子我一颗红心向祖国，先不说牺牲了写作业的时间去打坏蛋，好歹也是单纯地希望着这个世界好人能得到褒 赏，坏人能受到仲裁。法律做不到的我来做。这种跨时代的vigilante思想，却受到了守旧派文曲星转世的打 压。悲哀！ 哼╭(╯^╰)╮” 当然了，想归想。 在强权面前， 羁傲不逊的老鹰侠还是选择了低头。 带着书卷气的这顿揍最终还是抑制住了大脑的气血上涌。 然而这种气血上涌， 随着我年龄渐长， 越发演绎到极致。 我开始怀疑， 为什么有人会利用人类的善良来作为谋利的工具。 为什么有人会去折磨一些比自己弱小的生灵。 为什么有人能从看着别人受难中取乐。 我很不开心。 为什么善良的人应该承受比他人更多的痛楚， 而邪恶的人却可以嚣张跋扈逍遥自得。 我学会暴怒。 眼眶发红，咬牙切齿。 我想把坏人们都撕碎。 我要他们都知道痛苦总是平等传递的， 欺负任何生命都是不被允许的。 至少， 不被我允许！ 我还是长大了啊。 我开始明白，做个基因拼接的超人，是比较不现实的 T.T 我开始偷偷学电工，画铠甲， 试图做个像钢铁侠那样的英雄。（是的，在我还不认识Tony Stark的时候，我就先有了做钢铁侠的想法了。） 刀枪不入，所向披靡。 任何装备的坏人都妄想阻止神装的我。 然而很不辛， 我的手稿最终还是被名侦探文曲星转世找着了。 我爸很快就明白了我的意图， 而我也直言不讳， “我想制造个世界，那里只有好人活着，大家见面时点头微笑，不用武装，不用伪装。而我可以从人们清澈的眼神里看到折射的太阳光。他们偶尔抬头，发现这个世界依旧温软善良。” 我估计当时我爸听完肠子都悔青了， “完了完了，一世英名就要毁在这臭小子手里了。” 因为当时老爸并没有再沉默， 他直接用数学卷抽了我一耳光。 “作业写完了吗？没写完画什么鸟人！” 我想，大概是他把我精心设计的 「宇宙无敌钛合金钢铁老鹰侠小虚大魔王一号」 当做了鸟人吧。 羞辱！大大的羞辱！ 不能忍了！ 于是，我奋起反抗， 强烈谴责老爸侮辱艺术，侮辱科技，侮辱未来。 而我老爸也是个精干的人，不逼逼，直接劈头盖脸一顿揍。 “你小子数学考成这样，你跟我谈科技，跟我谈教育面向现代化，面向世界，面向未来？！” 那一刻， 我突然获得了一种神祗般的平静。 我明白自己太弱了， 这个世界过分强势得太多了， 聪明人不该反复迎头而上。 至少， 不该再挨第三顿揍了。 所以， 我尝试慢慢把愤怒变为理解。 我理解这个世界， 理解一切循规蹈矩和荒腔走板， 一切汲汲营营和独辟蹊径。 如果可以的话， 也想跟他们称兄道弟， 这是最敏感的人的生存方式， 我知道它想要什么， 它想要一个识时务的俊杰， 可是我真的不想做。 我想做一个大侠， 像一阵风一样的大侠。 我们坚持一件事情，并不是因为这样做了会有效果，而是坚信，这样做是对的。 ——哈维尔 这个答案在知乎上 获得了6928个赞。 而我曾经给这个回答点了个向下的箭头。 大概是我曾觉得， 有时候最能解决问题的方法， 不一定是对的，却是最有效的。 Vigilante 是不对的，但可能是最有效的。 如果能做一个绝望但细润无声的生命体， 那我愿意接盘背锅。 我想，大概是因为 我是一个间距很大的人， 永远无法在生活的漫漫长河里筛出那些温情闪光的瞬间来普渡众人。 我更像是行走的钢丝， 所到之处都是血案。 这种偏执一直持续到我前几天看了一个视频。 有那么一刻， 从一个男人的角度。 我觉得这个人帅毙了。 比蝙蝠侠，钢铁侠，蜘蛛侠，绿箭侠，老鹰侠都帅！ 我突然找到了一种， 作为一个全然的自己， 去和这个世界平等的相处。 我既不能仲裁别人的生死， 别人也不能干扰我的心境。 我要像这个男人一样， 我暂时不想去当什么英雄大侠了。 我只想做个「鹿尤」一样的长颈鹿侠。 把阳光，微笑和温柔善良带给身边的人。 蝙蝠侠，钢铁侠，蜘蛛侠，绿箭侠，老鹰侠 他们也许能穿梭云雾间， 窥探云层中的奥秘。 他们有自己黑暗骑士的信条， 而我也有自己的理解和克制。 人们会欢呼于他们的招招致命的凌厉，轰隆作响的正义。 我却折服于他们对生命的谦卑的那一瞬。 他们公演一个神话， 而我，书写一个笑话， 奢求这个世界的微笑。 挺好。 ","date":"2015-05-16","objectID":"/zh-cn/giraffe-man/:0:0","tags":["Gossip","Superhero"],"title":"长颈鹿侠","uri":"/zh-cn/giraffe-man/"},{"categories":["Gossip"],"content":"小王子的星球上忽然绽放了一朵娇艳的玫瑰花。 在我没长牙前，我就知道，草是绿的，花是红的，眼泪是咸的，口水是黏的，漂亮阿姨上厕所是不能偷看的。 在我长牙约莫拾几朵玫瑰花之后，我开始明白，玫瑰是不能摘的，春天是追不到的。 只能追，不能到！ 成都的春天鲜艳得很。 仿佛向整个天空泼上绿色的漆。 这也使玫瑰显得更加耀眼。 所以，小王子爱上这朵玫瑰，细心地呵护它。他以为，这是一朵唯一的花，只有他的星球上才有，其他的地方都不存在。 然而，玫瑰有刺，香水有毒。 当所有的女青年们一层腻子再一层粉盖住玻尿酸和肉毒杆菌微雕后的粉脸，闷几行code，写几首诗，拜读下冯 唐，再上微博刷个韩寒。 我开始发现，原来世界上有数百万朵这么完全一样的花儿。 这时，他才知道，他有的只是一朵普通的花。 然而，小王子并没有停止浇灌他的玫瑰花。 因为他知道，他星球上的那朵，仍然是独一无二的，他浇灌过它，用屏风保护过它，还倾听过它的哀怨自诩、它的孤单寂寞。 It is the time you have wasted for your rose that makes your rose so important. —- 小王子。 小王子注定不是个沾满胭脂味的人。 我也不是。 因为我还要纠结preg_match_all($rose，$spring,$you); 会不会报错。 //『从 ‘春天’ 里抓取 一朵’玫瑰’，传回给 ‘你’；』 我还要担心明天自己能否调整呼吸气定神闲地跟食堂阿姨要个豆浆鸡蛋饼。 然后默默保佑自己 听到闹钟就起床，上课不睡觉，代码一遍过，不得颈椎病。 我远不如小王子那样潇洒， 如果不能活得洒脱，那我选择活得机灵。 —- 小虚大魔王。 If I cant be dissolute, I prefer to die. —- 不要碧莲的小虚大魔王。 所以我想到多年前认识的一个机灵的朋友。 他是一只熊。 一只春天里的熊。 “最最喜欢你，绿子。” “什么程度？” “像喜欢春天的熊一样。” “春天的熊？什么春天的熊？” “春天的原野里，你，一个人正走着，对面走来一只可爱的小熊，浑身的毛活像天鹅绒，眼睛圆鼓鼓的。 它这么对你说道: ‘你好，小姐，和我一块儿打滚玩好么？’ 接着，你就和小熊抱在一起，顺着长满三叶草的山坡咕噜噜滚下来，整整玩了一大天，你说棒不棒？” “太棒了！” “我就是这么喜欢你！” 春天的熊哥注定是个极讨女孩子欢心的熊。 我却不是。 因为我既没有他那绒厚的脂肪来缓冲从山顶滚到山脚的摩擦，也没有他那清澈的晶状体房水来倒映出整个湛蓝的天。 然而，我还是很高兴能结识他们两个碧池碧莲。 他们鲜衣怒马，少年才俊。 他们就是牛逼， 新鲜的牛逼。 有空时， 我们聊聊教育面向现代化面向世界面向未来， 再吹逼一下世界霍乱时我们如何去拯救别人的爱情。 最后总结一下把整个春天都拥入怀里的250种方法。 看来所有不要脸的人都注定要相遇的。 成都的春天真绿啊。 阳光透过叶的缝隙小孔成像。 不规则地映射了一个季节。 不知道厦门的太阳是否温软？ 不知道可否有一天，我能够像小王子和春天的熊一样， 以真正牛逼的姿态再次相见。 大言不惭地说自己已经足够帅气足够有钱任性， 能依然对汲汲营营的名誉、条条框框的规则、战战兢兢的人情 不屑一顾， 对嘲讽和贬低的声音置之不理，保有与物质世界隔开距离的独到审美， 把所有的目光都留给美好、聪明、温柔的所在。 毕竟， 这才是我们和这个世界结交的缘由。 小王子走近这朵不期而至的玫瑰花。 风却把这朵玫瑰吹散。 花瓣迎着风， 在最高点乘着叶片向前飞。 小王子也迎着风向前追。 然而，扬起的风反而使玫瑰渐行渐远。 如果我不能像一阵风， 那么， 请让我追逐你风中飘忽不定的脸。 ","date":"2015-04-02","objectID":"/zh-cn/little-prince-and-bear/:0:0","tags":["Gossip","Bong"],"title":"小王子与春天的熊","uri":"/zh-cn/little-prince-and-bear/"},{"categories":null,"content":" 是清晨轻抚你脸庞的清风， 是夜里执剑走江湖的熊猫侠。 简述 我，是一个比较中二的程序员。 平时喜欢逛技术论坛 HackerNews、乌云 、InfoQ 等，同时还是 sae 认证的初级独立开发者. 早些年在乌云上提过漏洞, 折腾过网络安全. 目前技术栈集中在云平台开发，主要用到的语言是 Python/Golang，Python 吸引我在于他语法简洁语法糖易用，常被诟病的Python速度慢也在 Python3.6 之后的async出现后逐渐改善.不过在处理 CPU bound 任务时表现比较差, 这部分 Golang 则更好地解决了. 近来对 CloudNative/Kubernetes/Service Mesh 感兴趣, 尝试 Dive into it. 在编程之外，我还喜欢刷些 Dribbble 、 Pinterest 等设计师论坛，对其中精妙的设计十分着迷，对交互设计也有极大的兴趣，感觉一个 app 是为人服务的，应该贴切人类的使用习惯并遵循人类优质的审美风格。 在逛论坛之外，我还是这个星球上 🪐 为数不多会说  小黄人语的少年，喜欢荒诞却有逻辑的事务，眼里所有的风车，都是条巨龙。 同时，我还是漫威巨粉，对一切超级英雄有着不切实际的幻想，喜欢追逐他们风中飘忽不定的脸。 为什么写博客 写博客一是记录我的生活, 等老去时能留下一些念想 二是技术上记录下我踩过的一些坑, 防止未来第二次掉入 希望这些我自己揣摩的东西能够真正对你产生帮助, 我也以此获取肯定, 一定会非常开心 当然了, 写的文章受限于时间和个人理解, 难免有错, 也望指正 欢迎在评论中交流, 或通过  邮件,  LexusLee@Twitter 我. 其他博客地址 微信公众号: lexusscofieldlee 掘金: https://juejin.cn/user/2612095360185543 简书: https://www.jianshu.com/u/429597c212e8 近期想做的事 30 岁前看过 \u003e 2000 部电影 当一次超级英雄 去过一趟西西里 听一场 Adele 的 Live 接触过一次 👽 看一次极光 🌟 ","date":"2021-10-16","objectID":"/zh-cn/about/:0:0","tags":null,"title":"关于我","uri":"/zh-cn/about/"}]