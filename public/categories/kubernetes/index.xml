<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Kubernetes - Category - LexusLee&#39;s blog</title>
        <link>https://example.com/categories/kubernetes/</link>
        <description>Kubernetes - Category - LexusLee&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>lexuscyborg103@gmail.com (LexusLee)</managingEditor>
            <webMaster>lexuscyborg103@gmail.com (LexusLee)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 10 Jul 2020 19:10:46 &#43;0000</lastBuildDate><atom:link href="https://example.com/categories/kubernetes/" rel="self" type="application/rss+xml" /><item>
    <title>升级 k8s 集群 docker</title>
    <link>https://example.com/upgrade-dockerd-in-k8s/</link>
    <pubDate>Fri, 10 Jul 2020 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/upgrade-dockerd-in-k8s/</guid>
    <description><![CDATA[背景 最近需要给 k8s 集群升级 docker, 预期升到 19.03.x.
遇到一些问题, 记录下
调试期间遇到的问题: 集群中的 ingress 是以 daemonsets 方式部署, 通过 node-selector 选择节点定死.
而 kubectl drain node 并不 evict daemonset pods. 故在升级/重启 dockerd 期间会造成 ingress 短暂不可用. 而现有的 lb 的 health check 不能 cover 这一点, 仍会有流量打入.会导致升级期间 ingress 流量黑洞问题.
并且 dockerd 拉起来后, 有些 daemonsets 由于 ingress 自身 livenessProbe 等原因在 dockerd 升级期间持续 crashLoopbackoff 了, 一个原因是仍然 mount 一份旧的 docker overlay. 在 delete pod 重启后恢复. 故需要一个手段在升级后重启 ingress pods.
Ref https://github.com/kubernetes/kubernetes/issues/75482#issuecomment-511476698]]></description>
</item><item>
    <title>Talk about Kubernetes cronJob controller</title>
    <link>https://example.com/talk-about-k8s-cronjob/</link>
    <pubDate>Sat, 14 Dec 2019 13:57:32 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/talk-about-k8s-cronjob/</guid>
    <description><![CDATA[背景 之前一段时间正好接触到 kubernetes cronjob, 在接入时遇上了在一定量级下 cronjob schedule delay 的问题, 故开始读了下代码, 发现了一些问题并试着调优了下
存在的问题 按生产环境实际测试来看约 250-375 个 */1 * * * * 每分钟 interval 的 cronjob 就会产生 delay, cronjob 和 controller manager 没有异常 event 但新产生的 job 出现了延迟, 由于我们设置了 startingDeadlineSeconds 故累加起来的 delay 最终导致了 cron 任务严重滞后
代码解读 出于分析上述问题的目的, 读了下 cronjob controller 的代码, 代码量不多, 可能由于没上 GA 的原因, 整个 controllor 代码的设计也比较过程式, 不会像其他组件用到一些比如 Informer, refractor之类的组件读起来相对晦涩
下面开始解读下 release1.17 分支的 k8s cronjob controller 代码
 Controller struct  1 2 3 4 5 6 7  type Controller struct { kubeClient clientset.]]></description>
</item><item>
    <title>记一次升级 kube-proxy ipvs 引发的线上故障</title>
    <link>https://example.com/kubeproxy-ipvs-accident/</link>
    <pubDate>Mon, 29 Apr 2019 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/kubeproxy-ipvs-accident/</guid>
    <description><![CDATA[背景 最近在升级集群的 kube-prxoy 并开启 ipvs mode, 引发了一些线上故障
替换原因 由于豆瓣的集群使用 calico + kube-proxy iptables mode + puppet iptable 脚本管理
三个组件共同操作同一份 iptables, 容易出现 race condition 问题, 并且还会互相抢占 iptables 锁, 是个 Mutex unsafe 的操作, 不易于维护.
故打算尽量减少操作 iptables 的部分, 替换成 ipvs
事故回溯 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  @400000005cbea9a81eaf9164 W0423 13:58:54.514773 14016 server.go:195] WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated.]]></description>
</item><item>
    <title>浅谈 k8s service&amp;kube-proxy</title>
    <link>https://example.com/k8s-service-endpoints/</link>
    <pubDate>Fri, 14 Sep 2018 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/k8s-service-endpoints/</guid>
    <description><![CDATA[<div style="text-align: right;">Lexus Lee</div>
<h3 id="背景">背景</h3>
<p>最开始听到同事 k8s 分享时比较困惑我的一个问题是 k8s 怎么实现一个私有 ip(虚拟 ip，以下简称 vip)到另一个私有ip收发包的。</p>
<p>不过其实我想知道的应该是 k8s 通信机制，它是怎么实现服务发现的，新建的 pod 是怎么感知到的，万一有些 pod 节点变更 vip 变了 k8s 是如何感知的。</p>
<p>基于这个问题，做一下关于 k8s service&amp;kube-proxy 的分享。</p>]]></description>
</item></channel>
</rss>
