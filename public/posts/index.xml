<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>All Posts - LexusLee&#39;s blog</title>
        <link>https://example.com/posts/</link>
        <description>All Posts | LexusLee&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>lexuscyborg103@gmail.com (LexusLee)</managingEditor>
            <webMaster>lexuscyborg103@gmail.com (LexusLee)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Fri, 05 Feb 2021 10:29:05 &#43;0800</lastBuildDate><atom:link href="https://example.com/posts/" rel="self" type="application/rss+xml" /><item>
    <title>译《Scuba: Diving into data at Facebook》</title>
    <link>https://example.com/scuba-diving-into-data-at-facebook/</link>
    <pubDate>Fri, 05 Feb 2021 10:29:05 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/scuba-diving-into-data-at-facebook/</guid>
    <description><![CDATA[Scuba: Diving into Data at Facebook 原文链接: https://www.semanticscholar.org/paper/Scuba%3A-Diving-into-Data-at-Facebook-Abraham-Allen/0bce0735ef3ea01e02b73c98338727d519a71be4
概要 Facebook很重视性能监控。一些性能问题可能会影响超过10亿用户，所以我们跟踪了成千上万的服务器，每天上百PB的网络流量，每天上百个代码变更，以及许多其他指标。我们对时延性的要求是，在事件发生后的一分钟内，如一个客户电话请求，一个提交了bug报告，一份新的代码变更, 能及时更新到图表，并在监控系统中展示出来。
Scuba 是Facebook的数据管理系统，用于大部分的实时分析。Scuba 是一个快速的、可扩展的、分布式的、内存式的 Facebook 自研数据库。它目前吞吐率为每秒数百万行(事件)流入，并以同样的速度过期数据进行流出。Scuba 存储后端在数百台服务器上，每台服务器上的数据完全在内存中，每台服务器内存规格为 144 GB RAM。为了处理每个查询，Scuba 会从所有的数据中聚合服务器以处理每天处理近百万次查询。
在 Facebook, Scuba 广泛应用于交互式、临时性的分析查询，这些查询运行在在一秒内完成对实时数据的处理。
1. 介绍 在Facebook，无论是诊断性能回归还是衡量基础设施变化带来的影响，我们都希望用有效数据来衡量。Facebook基础设施团队依靠的是在实时数据监控以确保网站始终运行顺利。我们的需求包括能在非常短的延迟（通常在一分钟以内）从运行Facebook的网络服务器上查询到发生的事件。查询数据的灵活性和速度对于诊断出问题根因带来不少收益。识别问题的根本原因往往是由于各子系统之间复杂的依赖关系，很难在短时间内完成。然而，一旦出现了没有在几分钟或几小时未解决的问题。那 Facebook 的10亿用户就会变得不快乐，从而对 Facebook 整体发展不利。
为了解决上述问题， 起初，我们依靠的是预先汇总的图表和一套精心管理、手工编码的脚本，通过MySQL数据库的形式管理数据。到了2011年，这个解决方案变得过于僵化和缓慢。它无法跟上不断增长的数据吞吐率和查询效率。而 Facebook内部的其他查询系统，比如Hive[20]和Peregrine[13]，在数据被提供给查询之前，查询数据被写入HDFS，有很长的（典型的一天）延迟，而查询本身需要几分钟的时间来运行。
因此，我们建立了 Scuba，一个快速、可扩展、内存数据库。Scuba 是我们收集和分析来自各种系统的数据的方式的一个重大演变，正是这些系统保证了网站每天的正常运行。我们现在使用 Scuba 能对大多数实时的、结构化的人工数据进行分析。我们将在本文后面将 Scuba 与其他数据管理系统进行比较，但据我们了解，没有任何其他系统能像 Scuba 一样快速地吞吐数据并进行复杂的查询。
如今，Scuba 运行在数百台服务器上，每台服务器有144 GB内存，在一个无共享架构的集群中。它在内存中为1000多个表存储了大约70TB的压缩数据，通过在所有服务器上随机分配每个表来分配内存。Scuba 每秒可处理数百万行。由于 Scuba 是内存绑定的，它能以同样的速度过期掉老数据。为了限制数据量，Scuba 允许行指定一个可选的采样率，这表明 Scuba 只包含原始事件的一小部分（通常是100分之一到100万分之一）。这种采样对于像 Facebook 客户端请求这样每秒发生数百万次的事件是十分必要的。采样可以是统一的，也可以是基于一些关键数据，比如用户ID等。Scuba在计算汇总时，会对采样率进行聚合补偿。
除了提供一个 SQL 查询接口（对于一个 SQL 子集，包括分组和聚合，但不包括连接表），Scuba 提供了一个 GUI，可以生成时间序列图、饼图、列值分布，以及除文本选项之外的其他十几种数据的可视化。图1显示了一个时间序列图，在Scuba GUI中对页面流量进行了一周的比较。在后台，一个聚合树将每个查询分发到每个服务器，然后收集结果发回客户端。
虽然 Scuba 是为了支持性能分析而建立的，但它很快就成为了在其他时间敏感数据上执行探索性查询的首选系统。Facebook的许多团队都在使用Scuba。
 移动开发团队使用 Scuba 来跟踪哪些用户在运行不同的移动设备、操作系统和Facebook应用的版本。 广告开发团队利用 Scuba 来监控人们对广告的印象、点击和收入的变化。当出现下降时，他们可以迅速将其缩小到特定的国家、广告类型或服务器集群，并确定根本问题。 SRE 团队过使用 Scuba观察服务器错误。当发生峰值时，他们可以精确地指出是否是由于特定端点中的错误、特定数据中心或服务器集群中的服务，或数据中心的部分物理问题。 错误报告监测每小时运行数千次查询，以寻找Facebook用户报告的错误数量高峰，按几十个人口统计维度（位置、年龄、好友数等）进行分组。  总的来说，用户会先提出高级别的汇总查询，以识别数据中的有趣现象，然后再深入挖掘（因此被称为Scuba），以找到感兴趣的基础数据点。在上述所有情况下，能够沿着多个维度对数据进行临时分解是至关重要的。]]></description>
</item><item>
    <title>云幕电影放映机</title>
    <link>https://example.com/cloud-movie-projector/</link>
    <pubDate>Mon, 01 Feb 2021 13:30:46 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/cloud-movie-projector/</guid>
    <description><![CDATA[难得出太阳的一天。
北京的天总是阴沉沉的，有时候好像是要开启一场庭审，让我紧张得四肢发麻。
二月的第一天，终于有了一些不同。
敞亮的日光，透过大片鲜绿的树叶凶猛地穿透进来。
一抬头，世界在光明刺眼的色调里猛涨。
浑身暖洋洋。
天上的云彩白的好像一个个凸出来的拳头。
我希望它们沉下来，重重砸在我脸上。
将我击碎打散，
那下午便也不用上班。
我给自己定了个目标，这一年里，要看够 1750 部电影。
没想一个月过去了，只看到了 1518 部。
要是电影可以投射在云层里，那我便天天都可以看电影了。]]></description>
</item><item>
    <title>I hate people</title>
    <link>https://example.com/i-hate-people/</link>
    <pubDate>Fri, 29 Jan 2021 18:47:00 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/i-hate-people/</guid>
    <description><![CDATA[「I hate people !」
早上摸鱼时从友邻广播里看到这句话.
广播里讲了个故事，一位很健谈的朋友，一边讨厌人类，一边等个红灯都能跟人聊五块钱的，从椰子鸡到加拿大冰球，什么都聊。
如此言行不一致, 遭到了质疑。
朋友解释道，people in general 才是让他厌烦之处。When it gets personal, 那世界就变得有趣起来。
我开始反思。
是啊, people 是个非常 general 的概念, 是无具象的.
但当讨论到 person. 那这个人开始有了名字，有了声调. 这个人从&quot;people&quot;的群体里脱离出来，变得具象起来。
every single person 各有各自值得喜欢的地方。
只要一想到一个人类，无论他或她在哪里，在做什么想什么，都在发出悠长的呼吸;
睡梦时眼皮微微滑动，在做梦
睡醒时眼皮被太阳直登登地打亮
这样鲜活的人，多浪漫呀.
哪儿会有人不喜欢具象的人呢。
反观下来，自己似乎常常追逐一些抽象的事物。
越是虚妄，越着迷。
像是对超级英雄有着的不切实际的幻想, 渴望超能力的降临.
而抽离回生活中，却只能靠具象之物来校正这份抽象，而无法放弃抽象本身。
即使那些肌肤相亲的实景实物让人感到愉悦，但也会怀疑是否是自己赋予了某种抽象的意义.
王小波说，人是轻易不能知道自己的。
因为我们的感官是向外的，能察觉到他人细微的变化。但对于自己却不十分了然。
我不太信。
我爱游离在幻想与现实的边界。
有时，我想我爱自己多过爱他人。
我希望我的自我永远滋滋作响，翻腾不休，就像火炭上的一滴糖。
只是转念一想，
椰子鸡也好吃，加拿大冰球，也挺酷。
还有铲冰激凌，炸得焦脆的牛奶块。
打开冰箱门满满的饮料。
冬天里猫咪柔软的肚子，以及情人温软的胸膛。
美好得令人惭愧。
你说，从今天起,
我也要开始学习爱人，爱他人，爱具象的人。
像王小波一样, 像友邻的朋友一样，
I hate people,
But I love person.]]></description>
</item><item>
    <title>译《Autopilot: workload autoscaling at Google》</title>
    <link>https://example.com/google-autopilot-scaling/</link>
    <pubDate>Sun, 10 Jan 2021 13:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/google-autopilot-scaling/</guid>
    <description><![CDATA[摘要 原文链接：https://dl.acm.org/doi/pdf/10.1145/3342195.3387524
在许多公共和私有云系统中，用户需要指定资源量（CPU内核和RAM）的限制以为其工作负荷提供资源。 超出其限制的作业可能会受到限制或终止，从而导致最终用户请求的延迟或丢弃，因此人工操作人员针对这种问题出于谨慎考虑，会申请高于任务自身需要的配置。 从规模上讲，这将导致大量的资源浪费。
为了解决这个问题，Google使用Autopilot自动配置资源，同时调整作业中的并发任务数（水平缩放）和单个任务的CPU /内存限制（垂直缩放）。 Autopilot与人工操作员遵循相同的原则：Autopilot的主要目标是减少松弛（slack）（申请资源和实际资源使用之间的差异），同时最大程度地降低因内存不足（OOM）错误或 由于CPU节流，其性能下降。 Autopilot将机器学习算法应用到有关作业先前执行情况的历史数据上，再加上一组经过微调的启发式方法来实现这一目标。 在实践中，Autopilot工作只有23％的松弛（slack），而手动管理工作只有46％的松弛（slack）。 此外，Autopilot将受OOM严重影响的工作数量减少了10倍。
尽管有其优点，要确保Autopilot被广泛采用仍需付出巨大的努力，包括使尚未加入的客户容易看到潜在的建议，自动迁移某些类别的任务以及增加对自定义推荐器的支持。 在撰写本文时，Autopilot任务占Google资源使用的48％以上。
ACM参考格式：
Krzysztof Rzadca，Pawel Findeisen，Jacek Swiderski，Przemyslaw Zych，Przemyslaw Broniek，Jarek Kusmierek，Pawel Nowak，Beata Strack，Piotr Witusowski，Steven Hand和John Wilkes。 2020年。
Autopilot：Google的工作负载自动缩放。 在第十五欧洲2020年4月27日至30日，计算机系统会议（EuroSys'20），希腊伊拉克利翁。 ACM，美国纽约，纽约，共16页。 https://doi. org/10.1145/3342195.3387524
1 介绍 许多类型的公共云和私有云系统要求其用户声明在执行期间其工作负载将需要多少个实例以及每个实例所需的资源：在公共云平台中，用户需要选择他们需要租用虚拟机（VM）的类型和数量； 在Kubernetes集群中，用户可以设置Pod副本的数量和单个Pod的资源限制; 在Google中，我们要求用户指定所需的容器数量以及每个容器的资源限制。 这些限制使云基础架构能够提供足够的性能隔离，从而使云计算成为可能。
但是限制（主要是）对用户造成了麻烦。 很难估计一个作业需要多少资源才能最佳运行：CPU功率，内存和同时运行的副本数的正确组合。 负载测试可以帮助找到初始估计值，但是随着资源需求随时间变化，这些建议将过时，因为许多最终用户服务工作具有每日或每周的负载模式，并且随着服务变得越来越流行，流量在更长的时间内发生变化 。 最后，处理给定负载所需的资源会随着基础软件或硬件堆栈的新功能，优化和更新而变化。如果CPU容量不足，超出请求的资源可能会导致性能下降，或者导致任务被杀死 内存不足（OOM）。 都不是好事。
从调研结果看，理性的用户将故意高估其工作所需的资源，从而导致对物理资源的不良利用。 一项分析[26]对在一个Google集群[27]上执行的为期一个月的作业跟踪显示，平均内存利用率为50％； 对阿里巴巴YARN集群的另一项分析[23]显示任务的峰值内存利用率从未超过80％。
针对配置资源的困难，一种常见的模式是采用水平自动缩放器，该缩放器通过监控终端用户流量或平均CPU利用率的变化添加或删除副本来缩放任务。 所有主要的云提供商（AWS，Azure和GCP）都提供水平自动扩展功能； 它在某些云中间件（如Kubernetes）中也可用。 较不常见的模式是使用垂直自动缩放来调整每个副本可用的资源量。 两种技术也可以组合。
Autopilot是Google在其内部云上使用的主要自动缩放器。 它提供水平和垂直自动缩放。 本文重点介绍Autopilot的内存垂直扩展，因为这种情况鲜为人知。 论文：
  描述下Autopilot，以及它用于垂直自动缩放的两个主要算法：第一个算法依赖于历史用量的指数平滑滑动窗口； 另一个是基于从强化学习中借用的思想的元学习，该算法运行滑动窗口算法的许多变体，并为每个任务选择历史数据表现最佳的算法。（译注：强化学习：依赖海量的训练，并且需要精准的奖励。成本较高且比较复杂。元学习：具备自学能力，能够充分利用过去的经验来指导未来的任务。被认为是实现通用人工智能的关键。）
  通过Google的工作负载采样评估Autopilot算法的有效性；
  讨论为使我们的集群广泛采用Autopilot而采取的步骤。
  2 通过Borg管理云资源 Autopilot的目标和制约因素来自Google的Borg基础架构，并且针对Google的工作负载进行了调整。 我们在此处提供了两者的简要概述：有关Borg的更多详细信息，请参见[34]，有关工作负载的更多详细信息，请参见[26、27、31、35]。]]></description>
</item><item>
    <title>Awesome macOS apps list for developer</title>
    <link>https://example.com/apps-liat-for-macbook/</link>
    <pubDate>Fri, 25 Dec 2020 14:02:16 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/apps-liat-for-macbook/</guid>
    <description><![CDATA[Enumerate the list of apps that need to be installed on my migrated work computer at the moment. I&rsquo;ll consider adding it to the sync tool later.
Develop  Vscode Sublime Sourcetree Github Desktop Iterm2 Postman Cyberduck Docker  Command-line  oh-my-zsh homebrew homebrew cask tmux ack ag bat autojump job fselect  Prototype  draw.io desktop XMind Sketch  Database  Sequel Pro (for MySql) Navicat (for PostgreSql) Redis Manager  IM  Wechat Qiye Wechat Slack  浏览器  Chrome  Browser  Typora Notion  RSS  Leaf  Wallpaper  Iruve  System tools  CleanMyMac Dozer  Password  1Password  Music  NetEase Music Spotify  Video Player  IINA  Util Tool  Paste Yoink Beyond Compare Handshake Quicklook  GTD  Fantastical 2   ]]></description>
</item><item>
    <title>升级 k8s 集群 docker</title>
    <link>https://example.com/upgrade-dockerd-in-k8s/</link>
    <pubDate>Fri, 10 Jul 2020 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/upgrade-dockerd-in-k8s/</guid>
    <description><![CDATA[背景 最近需要给 k8s 集群升级 docker, 预期升到 19.03.x.
遇到一些问题, 记录下
调试期间遇到的问题: 集群中的 ingress 是以 daemonsets 方式部署, 通过 node-selector 选择节点定死.
而 kubectl drain node 并不 evict daemonset pods. 故在升级/重启 dockerd 期间会造成 ingress 短暂不可用. 而现有的 lb 的 health check 不能 cover 这一点, 仍会有流量打入.会导致升级期间 ingress 流量黑洞问题.
并且 dockerd 拉起来后, 有些 daemonsets 由于 ingress 自身 livenessProbe 等原因在 dockerd 升级期间持续 crashLoopbackoff 了, 一个原因是仍然 mount 一份旧的 docker overlay. 在 delete pod 重启后恢复. 故需要一个手段在升级后重启 ingress pods.
Ref https://github.com/kubernetes/kubernetes/issues/75482#issuecomment-511476698]]></description>
</item><item>
    <title>告警收敛设计</title>
    <link>https://example.com/alert-convergence/</link>
    <pubDate>Tue, 07 Jul 2020 22:30:07 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/alert-convergence/</guid>
    <description><![CDATA[背景 最近在豆瓣做面向开发者的 op 报警系统, 最头疼的是下列几种情况
 脉冲型报警 单个原因引起的报警发散 多个报警为同个诱因  由于目前现有的一些告警系统都强依赖后面的监控 agent/cmdb , 比如 open-falcon/prometheus 虽然都支持了告警收敛这一套, 但要从豆瓣的 statsd+icinga 迁移到 alertManager 需要折腾一阵。而我实际只想要一个中间轻量的 alert exporter 去做告警聚合/收敛.
所以我期望实现下列 feature:
 对于告警可配置梯度, 比如 apperr &gt; 50, 则立马报警不做 retry, 并且 notify_interval 为 1 分钟; 对于 50 &gt; apperr &gt; 20 则有3次 retry, retry 周期为 1 分钟, 报警周期为2分钟. 对于 apperr &lt; 1 则 retry 5 次, retry 周期 3 分钟, 报警周期 5 分钟. 告警配置中可用 与|或|非 的方式进行组合告警,告警收敛. 如  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  [rules] apperr - script: xxx - label: http, service, statsd, codeerr - warning_threshold: 0.]]></description>
</item><item>
    <title>eBPF learning 01</title>
    <link>https://example.com/ebpf-leanring1/</link>
    <pubDate>Mon, 13 Jan 2020 18:26:09 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/ebpf-leanring1/</guid>
    <description><![CDATA[(译) eBPF Tracing 简明教程与示例 背景    原文链接 http://www.brendangregg.com/blog/2019-01-01/learn-ebpf-tracing.html     原文作者 Brendan Gregg   出版时间 01 Jan 2019   翻译时间 11 Jan 2020    之前开 2019 ShangHai KubeConf 听了几场 eBPF 的分享, 最近才开始深入研究, 故打算把研究 Linux performance 的大佬 Brendan Gregg 的文章《Learn eBPF Tracing: Tutorial and Examples》 作为 eBPF 入门翻译一遍, 希望对其他非英语母语的开发者有帮助。
译文 在 2019 年的 Linux Plumber&rsquo;s 大会上至少有 24 场关于 eBPF 的讲座, eBPF 迅速地成为了炙手可热的技术。所以也许你也计划在新的一年里开始学习 eBPF!]]></description>
</item><item>
    <title>阿芙罗狄忒你不要哭泣</title>
    <link>https://example.com/dont-cry-aphrodite/</link>
    <pubDate>Thu, 09 Jan 2020 20:20:28 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/dont-cry-aphrodite/</guid>
    <description><![CDATA[这些日子隐隐觉得你并不开心, 我十分自责, 总觉得是那天妖精们藏起了星星。
北京的天总是阴沉沉的，有时候好像是要开启一场庭审，让我紧张得四肢发麻，不知道你会不会也因此闷闷不乐。这样的日子，可真叫人腻味，我沮丧得想睡觉。唯独那天看着你把脸藏在被子里沉沉地入睡，我欢喜得不行，这一天何其矜贵，希望你做个好梦。
所以我特别期望能有个间隙去看看被藏起来的东西，看星星也是我生活中为数不多的理想主义。
那时你说我喋喋不休，要知道我是个时刻沸腾的人，能滋滋作响一整天，总有一些莫名其妙的巨大能量，但我也要悄悄地藏起来，可要把我所有的能量分成一天天的给你。这样我就能沉静许多，不过其实一见到你，那些跳脱的念头总能平静下来。
就像一只海豚，终于跳进了阿芙罗狄忒的海湾，有海鸥有落日，不免沉浸其中。
这样的日子，我也想藏起来，担心有一天我不会记得，只期望有那么几个瞬间，偶然念及你的名字，心底像冒出海葵与青荇，世界变量明亮开朗起来。
所以，我还想看着你笑，看着你看电影时甜甜地睡着，你说，我哪能让你一个人生闷气呢。
你说未来会有多少无生趣的毫不起眼的夜晚，我就跋涉到灯塔上，威胁妖精们把藏起来的光，投射下来。
加油啊，阿芙罗狄忒。]]></description>
</item><item>
    <title>Talk about Kubernetes cronJob controller</title>
    <link>https://example.com/talk-about-k8s-cronjob/</link>
    <pubDate>Sat, 14 Dec 2019 13:57:32 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/talk-about-k8s-cronjob/</guid>
    <description><![CDATA[背景 之前一段时间正好接触到 kubernetes cronjob, 在接入时遇上了在一定量级下 cronjob schedule delay 的问题, 故开始读了下代码, 发现了一些问题并试着调优了下
存在的问题 按生产环境实际测试来看约 250-375 个 */1 * * * * 每分钟 interval 的 cronjob 就会产生 delay, cronjob 和 controller manager 没有异常 event 但新产生的 job 出现了延迟, 由于我们设置了 startingDeadlineSeconds 故累加起来的 delay 最终导致了 cron 任务严重滞后
代码解读 出于分析上述问题的目的, 读了下 cronjob controller 的代码, 代码量不多, 可能由于没上 GA 的原因, 整个 controllor 代码的设计也比较过程式, 不会像其他组件用到一些比如 Informer, refractor之类的组件读起来相对晦涩
下面开始解读下 release1.17 分支的 k8s cronjob controller 代码
 Controller struct  1 2 3 4 5 6 7  type Controller struct { kubeClient clientset.]]></description>
</item></channel>
</rss>
