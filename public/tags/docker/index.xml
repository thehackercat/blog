<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Docker - Tag - LexusLee&#39;s blog</title>
        <link>https://example.com/tags/docker/</link>
        <description>Docker - Tag - LexusLee&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>lexuscyborg103@gmail.com (LexusLee)</managingEditor>
            <webMaster>lexuscyborg103@gmail.com (LexusLee)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sat, 02 Oct 2021 11:04:16 &#43;0800</lastBuildDate><atom:link href="https://example.com/tags/docker/" rel="self" type="application/rss+xml" /><item>
    <title>云原生下发布平台建设思考</title>
    <link>https://example.com/talk-about-cloud-based-release-platform/</link>
    <pubDate>Sat, 02 Oct 2021 11:04:16 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/talk-about-cloud-based-release-platform/</guid>
    <description><![CDATA[<div style="text-align: right;">Lexus Lee</div>
<h2 id="背景">背景</h2>
<p>从过去几个月, 我的工作精力主要集中在企业级发布平台的建设上. 同时也聚焦在容器云 <strong>PaaS</strong> 层建设。</p>
<p>在当下云原生时代, 发布平台通常起到一个联结 PaaS 和 IaaS 的作用. 通过在 <strong>PaaS</strong> 层抽象，将 IaaS 层基建细节屏蔽，同时通过 api 和各个服务平台(CI/CD 服务, 监控告警服务, 网关服务, 工单系统, 服务治理系统等) 交互, 在云平台提供了一个统一的快捷入口，从逻辑上进行汇聚，便于开发者进行<strong>一站式</strong>应用发布、服务生命周期管理、服务治理、服务运维等操作.</p>]]></description>
</item><item>
    <title>基于服务画像的垂直伸缩系统建设思考</title>
    <link>https://example.com/service-portrait-vpa-design/</link>
    <pubDate>Tue, 28 Sep 2021 19:51:32 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/service-portrait-vpa-design/</guid>
    <description><![CDATA[<div style="text-align: right;">Lexus Lee</div>
<h2 id="背景">背景</h2>
<p>近期公司开始做成本治理, 降本增效. 通过观察以往各业务线的资源规格配置, 不难发现存在大量服务资源超配情况. 即服务实际资源开销远小于其声明的资源开销. 而这部分的资源超配直接导致了集群的资源使用率较低. 通常形如下图</p>]]></description>
</item><item>
    <title>Talk about Containerd</title>
    <link>https://example.com/talk-about-containerd/</link>
    <pubDate>Sat, 10 Apr 2021 15:04:09 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/talk-about-containerd/</guid>
    <description><![CDATA[背景 最近看一些社区 issue 中正好接触到 containerd 源码, 早年大家基本都使用 docker/podman 这类容器上层包装, 包括我也是, 故对真正容器运行时的结构部分了解得不够深入, 最近 k8s 社区在 1.21 规划上计划从 kubelet 中移除 docker-shim 交互的逻辑, 大势上组件走上 containerd-shim, 故需要着手对容器运行时更深入了解.
带着问题作为引入,
容器底层还是对 Linux LXC 的交互, 那究竟 docker daemon 中是哪个组件来完成这一步的呢？
1、背景知识 首先我们需要了解 Docker Daemon 生产出容器的过程.
当前整个 Docker 调用链架构可以用下图来简单概括
从 Docker 1.11 之后，Docker Daemon 被分成了多个模块以适应 OCI 标准。拆分之后，结构分成了以下几个部分：
16年12月 Docker公司宣布将 containerd 从Docker Engine中分离，并捐赠到开源社区独立发展和运营。
一个工业标准的容器运行时，注重简单、 健壮性、可移植性
Docker本身其实已经被剥离干净了，只剩下 Docker 自身作为 cli 的一些特色功能了，真正容器的管控都在 containerd 里实现。
在 17 年 docker重命名为moby, 从命名上逐渐脱离和 container 的关系, 而 Moby 更像是一个“乐高积木”，它包含了容器化的组件库 底层的构建、日志处理、卷管理、网络、镜像管理、containerd、SwarmKit等模块, 是个大杂烩.]]></description>
</item><item>
    <title>升级 k8s 集群 docker</title>
    <link>https://example.com/upgrade-dockerd-in-k8s/</link>
    <pubDate>Fri, 10 Jul 2020 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/upgrade-dockerd-in-k8s/</guid>
    <description><![CDATA[背景 最近需要给 k8s 集群升级 docker, 预期升到 19.03.x.
遇到一些问题, 记录下
调试期间遇到的问题: 集群中的 ingress 是以 daemonsets 方式部署, 通过 node-selector 选择节点定死.
而 kubectl drain node 并不 evict daemonset pods. 故在升级/重启 dockerd 期间会造成 ingress 短暂不可用. 而现有的 lb 的 health check 不能 cover 这一点, 仍会有流量打入.会导致升级期间 ingress 流量黑洞问题.
并且 dockerd 拉起来后, 有些 daemonsets 由于 ingress 自身 livenessProbe 等原因在 dockerd 升级期间持续 crashLoopbackoff 了, 一个原因是仍然 mount 一份旧的 docker overlay. 在 delete pod 重启后恢复. 故需要一个手段在升级后重启 ingress pods.
Ref https://github.com/kubernetes/kubernetes/issues/75482#issuecomment-511476698]]></description>
</item></channel>
</rss>
