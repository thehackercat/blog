<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Linux - Tag - LexusLee&#39;s blog</title>
        <link>https://example.com/tags/linux/</link>
        <description>Linux - Tag - LexusLee&#39;s blog</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>lexuscyborg103@gmail.com (LexusLee)</managingEditor>
            <webMaster>lexuscyborg103@gmail.com (LexusLee)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 13 Feb 2022 21:55:54 &#43;0800</lastBuildDate><atom:link href="https://example.com/tags/linux/" rel="self" type="application/rss+xml" /><item>
    <title>记一次 k8s apiserver watch hang 问题排查</title>
    <link>https://example.com/k8s-api-watch-hang-dignosis/</link>
    <pubDate>Sun, 13 Feb 2022 21:55:54 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/k8s-api-watch-hang-dignosis/</guid>
    <description><![CDATA[Lexus Lee 记一次 k8s apiserver watch hang 问题排查 问题背景 注📢: 本文中涉及 apiserver 地址和 ingressgateway 地址, 为脱敏处理, 将会做马赛克处理！！！
传统的 kubernetes apiserver 请求访问链路为客户端直连 apiserver，为了做 apiserver 高可用，通常我们会给 apiserver 前端再套一层4层或7层代理做多个 apiserver 实例的负载均衡。
在我们的场景下，使用了 istio 的 ingressgateway 作为 client -&gt; apiserver 这条链路中的7层代理。链路变成了 client -&gt; ingressgateway -&gt; apiserver ，gateway 暴露 80 端口供客户端访问, 同时通过 istio virtualService + destinationRule 规则配置 gateway 能通过域名访问到 apiserver 6443 端口，从而实现流量路由。
具体链路如下图所示，
在这样的链路下，我们遇到了如下问题，
在 k8s apiserver 1.18 版本的集群在滚动重启过出现部分组件无法 watch 到事件的情况，客户端 watch 请求偶发 503。需要重启组件，重建 watch 连接才能恢复。]]></description>
</item><item>
    <title>《容器高手实战》笔记</title>
    <link>https://example.com/container-travel-lesson-01/</link>
    <pubDate>Sun, 26 Dec 2021 13:40:08 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/container-travel-lesson-01/</guid>
    <description><![CDATA[背景 最近在接触《容器高手实战》这门课，作为有一定容器经验的工程师，从这个视角下，我会记录一些非基础的，但是不是需要回看一眼的课程笔记，用于分享。
课程相关源码及笔记我放在这个这个链接中, 欢迎 fork &amp; comment~
《容器高手实战课程》 源码及笔记, 共分为以下几个模块，下面将逐一介绍，
 Cpu 内存 文件系统 一号进程 容器网络 容器 debug 工具 容器安全  容器 load Load Average 等于单位时间内正在运行的进程加上可运行队列的进程
第一，不论计算机 CPU 是空闲还是满负载，Load Average 都是 Linux 进程调度器中可运行队列（Running Queue）里的一段时间的平均进程数目。
第二，计算机上的 CPU 还有空闲的情况下，CPU Usage 可以直接反映到&quot;load average&quot;上，什么是 CPU 还有空闲呢？具体来说就是可运行队列中的进程数目小于 CPU 个数，这种情况下，单位时间进程 CPU Usage 相加的平均值应该就是&quot;load average&quot;的值。
第三，计算机上的 CPU 满负载的情况下，计算机上的 CPU 已经是满负载了，同时还有更多的进程在排队需要 CPU 资源。这时&quot;load average&quot;就不能和 CPU Usage 等同了。
比如对于单个 CPU 的系统，CPU Usage 最大只是有 100%，也就 1 个 CPU；而&quot;load average&quot;的值可以远远大于 1，因为&quot;load average&quot;看的是操作系统中可运行队列中进程的个数。]]></description>
</item><item>
    <title>从 Facebook 故障开始探索 BGP 工具使用</title>
    <link>https://example.com/tools-to-explore-bgp/</link>
    <pubDate>Tue, 16 Nov 2021 11:46:09 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/tools-to-explore-bgp/</guid>
    <description><![CDATA[背景 上个月，Facebook 发生了一次由 BGP 引起的大型故障，从故障报告 中我确实看得云里雾里。所以近期阅读一些文章，期望深入学习。
这篇博文将会分享一些我如何学习 BGP 的过程及用来查询 BGP 信息的相关工具使用。因为我对 BGP 不是那么了解，故文章难免有不精确的部分，希望多多交流。
BGP 探索之路 如何发布BGP路由 之前我从没有开始接触 BGP 的原因之一是&ndash;我不知道如何在我的服务器上通过 BGP 发布路由。
对于大多数网络协议，如果你愿意，都可以折腾，自己实现协议。例如，你可以。
  发行你自己的TLS证书
  编写你自己的HTTP服务器
  编写你自己的TCP实现
  为你的域名编写你自己的权威DNS服务器
  建立你自己的证书颁发机构
  但对于BGP，我认为除非你拥有自己的 ASN，否则不能自己发布路由，尽管可能可以在你的家庭网络上实现BGP，但我希望它们运行在真正的互联网上。
于是我开始着手于如何在我的服务器上发布 BGP 路由。
首先我们来谈谈 BGP 的一些术语。这部分我会简单的介绍，因为我对工具更感兴趣，而且网上有很多关于 BGP 的高水平资料（比如 cloudflare 的这篇帖子）。
什么是 AS（autonomous system）？ 我们需要了解的第一件术语是 AS。每一个 AS 都是什么呢？
参考这篇维基百科, 他们有以下特征，
  由一个组织拥有的（通常是一个大型组织，如你的ISP，政府，大学，Facebook，等等）
  他们能控制一组特定的 IP 地址（例如，我的 ISP 的 AS 包括 247,808 个IP地址）。]]></description>
</item><item>
    <title>Kube-scheduler 调度模型与美团本地化改造</title>
    <link>https://example.com/kubernetes-scheduler-modified-in-meituan/</link>
    <pubDate>Sat, 09 Oct 2021 09:57:53 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/kubernetes-scheduler-modified-in-meituan/</guid>
    <description><![CDATA[背景 今天主要分享下我在美团 Hulk 团队调度系统组期间学习到的 k8s scheduler 调度模型及美团对于调度器相关改造
Hulk 团队介绍 Hulk 是美团负责容器云平台的团队, 我所在的组是调度系统组下面的 k8s 小组. 主要负责所有 k8s 相关组件开发维护.
Kube-scheduler 调度模型介绍 (本次介绍基于 k8s 1.16 版本)
pod 是 k8s 工作负载的最小实体，也是 scheduler 调度的主要对象。
k8s 调度器所做的工作，即将 pod 绑定到指定节点上，为 pod 选择合适的节点。
Scheduler 会先 watch apiserver 获取其中待调度的 pod, 即 pod.spec.nodeName 字段为空的 pod 对象, 通过调用 pod informer 的 handler 并将该 pod 加入调度队列中。
默认情况下, k8s 调度队列是一个 PriorityQueue, 并且当某些集群信息发生改变时, 调度器会对调度队列的内容进行一些特殊操作. 在这儿不论述.
一次调度过程在判断一个 Node是否可作为目标机器时，主要分为三个阶段：
 Predicates 预选阶段：硬性条件，过滤掉不满足条件的节点，这个过程称为 Predicates。这是固定先后顺序的一系列过滤条件，任何一个 Predicate 不符合则放弃该 Node。 Prioritites 优选阶段：软性条件，对通过的节点按照优先级排序，称之为 Priorities。每一个 Priority 都是一个影响因素，都有一定的权重。 Select 选定阶段：从优选列表中选择优先级最高的节点，称为 Select。选择的 Node 即为最终部署 Pod 的机器。  预选阶段 Scheduler 起 n 个 goroutine 并发对整个集群的所有 nodes 进行 predicates 串行运算过滤规则，直到最终过滤出符合条件的 nodes 列表]]></description>
</item><item>
    <title>基于服务画像的垂直伸缩系统建设思考</title>
    <link>https://example.com/service-portrait-vpa-design/</link>
    <pubDate>Tue, 28 Sep 2021 19:51:32 &#43;0800</pubDate>
    <author>Author</author>
    <guid>https://example.com/service-portrait-vpa-design/</guid>
    <description><![CDATA[<div style="text-align: right;">Lexus Lee</div>
<h2 id="背景">背景</h2>
<p>近期公司开始做成本治理, 降本增效. 通过观察以往各业务线的资源规格配置, 不难发现存在大量服务资源超配情况. 即服务实际资源开销远小于其声明的资源开销. 而这部分的资源超配直接导致了集群的资源使用率较低. 通常形如下图</p>]]></description>
</item><item>
    <title>eBPF learning 01</title>
    <link>https://example.com/ebpf-leanring1/</link>
    <pubDate>Mon, 13 Jan 2020 18:26:09 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/ebpf-leanring1/</guid>
    <description><![CDATA[(译) eBPF Tracing 简明教程与示例 背景    原文链接 http://www.brendangregg.com/blog/2019-01-01/learn-ebpf-tracing.html     原文作者 Brendan Gregg   出版时间 01 Jan 2019   翻译时间 11 Jan 2020    之前开 2019 ShangHai KubeConf 听了几场 eBPF 的分享, 最近才开始深入研究, 故打算把研究 Linux performance 的大佬 Brendan Gregg 的文章《Learn eBPF Tracing: Tutorial and Examples》 作为 eBPF 入门翻译一遍, 希望对其他非英语母语的开发者有帮助。
译文 在 2019 年的 Linux Plumber&rsquo;s 大会上至少有 24 场关于 eBPF 的讲座, eBPF 迅速地成为了炙手可热的技术。所以也许你也计划在新的一年里开始学习 eBPF!]]></description>
</item><item>
    <title>Talk about Kubernetes cronJob controller</title>
    <link>https://example.com/talk-about-k8s-cronjob/</link>
    <pubDate>Sat, 14 Dec 2019 13:57:32 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/talk-about-k8s-cronjob/</guid>
    <description><![CDATA[背景 之前一段时间正好接触到 kubernetes cronjob, 在接入时遇上了在一定量级下 cronjob schedule delay 的问题, 故开始读了下代码, 发现了一些问题并试着调优了下
存在的问题 按生产环境实际测试来看约 250-375 个 */1 * * * * 每分钟 interval 的 cronjob 就会产生 delay, cronjob 和 controller manager 没有异常 event 但新产生的 job 出现了延迟, 由于我们设置了 startingDeadlineSeconds 故累加起来的 delay 最终导致了 cron 任务严重滞后
代码解读 出于分析上述问题的目的, 读了下 cronjob controller 的代码, 代码量不多, 可能由于没上 GA 的原因, 整个 controllor 代码的设计也比较过程式, 不会像其他组件用到一些比如 Informer, refractor之类的组件读起来相对晦涩
下面开始解读下 release1.17 分支的 k8s cronjob controller 代码
 Controller struct  1 2 3 4 5 6 7  type Controller struct { kubeClient clientset.]]></description>
</item><item>
    <title>记一次升级 kube-proxy ipvs 引发的线上故障</title>
    <link>https://example.com/kubeproxy-ipvs-accident/</link>
    <pubDate>Mon, 29 Apr 2019 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/kubeproxy-ipvs-accident/</guid>
    <description><![CDATA[背景 最近在升级集群的 kube-prxoy 并开启 ipvs mode, 引发了一些线上故障
替换原因 由于豆瓣的集群使用 calico + kube-proxy iptables mode + puppet iptable 脚本管理
三个组件共同操作同一份 iptables, 容易出现 race condition 问题, 并且还会互相抢占 iptables 锁, 是个 Mutex unsafe 的操作, 不易于维护.
故打算尽量减少操作 iptables 的部分, 替换成 ipvs
事故回溯 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24  @400000005cbea9a81eaf9164 W0423 13:58:54.514773 14016 server.go:195] WARNING: all flags other than --config, --write-config-to, and --cleanup are deprecated.]]></description>
</item><item>
    <title>浅谈 k8s service&amp;kube-proxy</title>
    <link>https://example.com/k8s-service-endpoints/</link>
    <pubDate>Fri, 14 Sep 2018 19:10:46 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/k8s-service-endpoints/</guid>
    <description><![CDATA[<div style="text-align: right;">Lexus Lee</div>
<h3 id="背景">背景</h3>
<p>最开始听到同事 k8s 分享时比较困惑我的一个问题是 k8s 怎么实现一个私有 ip(虚拟 ip，以下简称 vip)到另一个私有ip收发包的。</p>
<p>不过其实我想知道的应该是 k8s 通信机制，它是怎么实现服务发现的，新建的 pod 是怎么感知到的，万一有些 pod 节点变更 vip 变了 k8s 是如何感知的。</p>
<p>基于这个问题，做一下关于 k8s service&amp;kube-proxy 的分享。</p>]]></description>
</item><item>
    <title>记一次 postgresql 斯嘉丽约翰逊攻击的排查</title>
    <link>https://example.com/scarllet-sql-attack/</link>
    <pubDate>Sat, 11 Aug 2018 22:49:52 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://example.com/scarllet-sql-attack/</guid>
    <description><![CDATA[<h3 id="背景">背景</h3>
<p>今天下午连续收到了腾讯云 CPU overload 报警</p>
<p></p>
<p>登服务器一看, 有个 postgres 账户跑的进程把 CPU 占满了，进程名特别奇怪。</p>
<p></p>]]></description>
</item></channel>
</rss>
